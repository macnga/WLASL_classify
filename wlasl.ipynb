{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3153248,"sourceType":"datasetVersion","datasetId":1903686},{"sourceId":12059660,"sourceType":"datasetVersion","datasetId":7590419},{"sourceId":12072209,"sourceType":"datasetVersion","datasetId":7599152}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom collections import defaultdict\n\ninput_txt = '/kaggle/input/keypoint/videoid_label.txt'\nout_train = '/kaggle/working/train.txt'\nout_val = '/kaggle/working/val.txt'\nout_test = '/kaggle/working/test.txt'\n\n#os.makedirs(out_train, exist_ok=True)\n#os.makedirs(out_val, exist_ok=True)\n#os.makedirs(out_test, exist_ok=True)\n# Đọc file và gom theo nhãn\nlabel_dict = defaultdict(list)\nwith open(input_txt, 'r') as f:\n    for line in f:\n        line = line.strip()\n        if not line: continue\n        video_id, label = line.split()\n        label_dict[label].append(video_id)\n\n# Chia tỉ lệ 6:4:4 cho từng nhãn\ntrain_lines, val_lines, test_lines = [], [], []\nfor label, vids in label_dict.items():\n    vids = list(vids)\n    random.shuffle(vids)\n    n = len(vids)\n    n_train = round(n * 0.6)\n    n_val = round(n * 0.2)\n    n_test = n - n_train - n_val\n    train = vids[:n_train]\n    val = vids[n_train:n_train+n_val]\n    test = vids[n_train+n_val:]\n    train_lines.extend([f\"{vid} {label}\\n\" for vid in train])\n    val_lines.extend([f\"{vid} {label}\\n\" for vid in val])\n    test_lines.extend([f\"{vid} {label}\\n\" for vid in test])\n\n# Shuffle lại từng tập để tránh cùng nhãn đứng liền nhau\nrandom.shuffle(train_lines)\nrandom.shuffle(val_lines)\nrandom.shuffle(test_lines)\n\n# Lưu file\nwith open(out_train, 'w') as f: f.writelines(train_lines)\nwith open(out_val, 'w') as f: f.writelines(val_lines)\nwith open(out_test, 'w') as f: f.writelines(test_lines)\n\nprint(\"Đã chia xong. Train:\", len(train_lines), \"Val:\", len(val_lines), \"Test:\", len(test_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:15.420472Z","iopub.execute_input":"2025-06-10T15:51:15.420743Z","iopub.status.idle":"2025-06-10T15:51:15.456608Z","shell.execute_reply.started":"2025-06-10T15:51:15.420718Z","shell.execute_reply":"2025-06-10T15:51:15.455913Z"}},"outputs":[{"name":"stdout","text":"Đã chia xong. Train: 456 Val: 145 Test: 150\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1. Pretrain Vision encoder","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport torchvision\nfrom tqdm import tqdm\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:15.458224Z","iopub.execute_input":"2025-06-10T15:51:15.458754Z","iopub.status.idle":"2025-06-10T15:51:18.440559Z","shell.execute_reply.started":"2025-06-10T15:51:15.458728Z","shell.execute_reply":"2025-06-10T15:51:18.439944Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_label_map(txt_files):\n    labels = set()\n    for txt in txt_files:\n        with open(txt, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) >= 2:\n                    label = ' '.join(parts[1:]).strip()\n                    labels.add(label)\n    labels = sorted(labels)\n    label_map = {lbl: idx for idx, lbl in enumerate(labels)}\n    return label_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:18.441164Z","iopub.execute_input":"2025-06-10T15:51:18.441408Z","iopub.status.idle":"2025-06-10T15:51:18.446369Z","shell.execute_reply.started":"2025-06-10T15:51:18.441392Z","shell.execute_reply":"2025-06-10T15:51:18.445544Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_root = \"/kaggle/input/image-mask/cropped_hands\"\ntrain_txt = \"/kaggle/working/train.txt\"\nvalid_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 6\nlr = 1e-3\nout_ckpt = \"/kaggle/working/pretrained_hand_rgb.pth\"\nout_labelmap = \"/kaggle/working/label_map.json\"\n\nlabel_map = build_label_map([train_txt, valid_txt, test_txt])\nnum_classes = len(label_map)\nprint(\"Số lớp:\", num_classes)\nprint(\"Sample label_map:\", dict(list(label_map.items())[:5]))\n\nwith open(out_labelmap, \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:18.447008Z","iopub.execute_input":"2025-06-10T15:51:18.447379Z","iopub.status.idle":"2025-06-10T15:51:18.469008Z","shell.execute_reply.started":"2025-06-10T15:51:18.447358Z","shell.execute_reply":"2025-06-10T15:51:18.468433Z"}},"outputs":[{"name":"stdout","text":"Số lớp: 32\nSample label_map: {'all': 0, 'before': 1, 'black': 2, 'book': 3, 'candy': 4}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==== 2. Dataset for cropped hand images ====\nclass HandImageDataset(Dataset):\n    def __init__(self, data_root, list_file, label_map, transform=None):\n        self.samples = []\n        self.transform = transform\n        with open(list_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    video_id, label = parts\n                    label_idx = label_map[label]\n                    #img_dir = os.path.join(data_root, video_id)\n                    # All *_left.jpg and *_right.jpg (can add filter for frame sampling if needed)\n                    imgs = sorted(glob(os.path.join(data_root, f\"{video_id}_*_left.jpg\"))) + \\\n                           sorted(glob(os.path.join(data_root, f\"{video_id}_*_right.jpg\")))\n                    for img_path in imgs:\n                        if os.path.isfile(img_path):\n                            self.samples.append((img_path, label_idx))\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:18.469776Z","iopub.execute_input":"2025-06-10T15:51:18.470013Z","iopub.status.idle":"2025-06-10T15:51:18.483435Z","shell.execute_reply.started":"2025-06-10T15:51:18.469991Z","shell.execute_reply":"2025-06-10T15:51:18.482667Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\ntrain_ds = HandImageDataset(data_root, train_txt, label_map, transform=transform)\nval_ds = HandImageDataset(data_root, valid_txt, label_map, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\nprint(f\"Số ảnh train: {len(train_ds)}, Số ảnh val: {len(val_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:18.484092Z","iopub.execute_input":"2025-06-10T15:51:18.484330Z","iopub.status.idle":"2025-06-10T15:51:31.252179Z","shell.execute_reply.started":"2025-06-10T15:51:18.484299Z","shell.execute_reply":"2025-06-10T15:51:31.251541Z"}},"outputs":[{"name":"stdout","text":"Số ảnh train: 2298, Số ảnh val: 721\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\n\nclass VisionClassifier(nn.Module):\n    def __init__(self, num_classes, out_channels=256):\n        super().__init__()\n        backbone = torchvision.models.efficientnet_b0(pretrained=True)\n        # Reduce feature channels to out_channels (C=64) with a 1x1 conv\n        self.features = nn.Sequential(\n            backbone.features,                     # (B, 1280, H, W)\n            nn.Conv2d(1280, out_channels, 1),      # (B, 256, H, W)\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(out_channels, num_classes)\n\n    def forward(self, x, return_feature=False):\n        feat = self.features(x)                              # (B, 256, H, W)\n        pooled = self.pool(feat).view(x.size(0), -1)         # (B, 256)\n        logits = self.classifier(pooled)                     # (B, num_classes)\n        if return_feature:\n            return logits, feat                              # (B, num_classes), (B, 256, H, W)\n        else:\n            return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:31.253008Z","iopub.execute_input":"2025-06-10T15:51:31.253507Z","iopub.status.idle":"2025-06-10T15:51:31.259457Z","shell.execute_reply.started":"2025-06-10T15:51:31.253485Z","shell.execute_reply":"2025-06-10T15:51:31.258573Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = VisionClassifier(num_classes)\nif torch.cuda.device_count() > 1:\n    print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n    model = nn.DataParallel(model)\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:31.262026Z","iopub.execute_input":"2025-06-10T15:51:31.262256Z","iopub.status.idle":"2025-06-10T15:51:31.620346Z","shell.execute_reply.started":"2025-06-10T15:51:31.262237Z","shell.execute_reply":"2025-06-10T15:51:31.619585Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Using DataParallel with 2 GPUs\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"best_val_acc = 0\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for imgs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\", leave=False):\n        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n    train_acc = correct / total\n    train_loss = total_loss / total\n\n    # Validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=\"Valid\", leave=False):\n            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += imgs.size(0)\n    val_acc = val_correct / val_total\n    val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n    # Save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model\": model.state_dict(),\n            \"label_map\": label_map\n        }, out_ckpt)\n        print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\nprint(\"Done. Best val acc:\", best_val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:51:31.621110Z","iopub.execute_input":"2025-06-10T15:51:31.621378Z","iopub.status.idle":"2025-06-10T15:52:55.113072Z","shell.execute_reply.started":"2025-06-10T15:51:31.621360Z","shell.execute_reply":"2025-06-10T15:52:55.112357Z"}},"outputs":[{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 3.0880 Acc: 0.1819 | Val Loss: 2.9932 Acc: 0.2205\nBest model saved at epoch 1, val_acc=0.2205\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/6 | Train Loss: 2.5333 Acc: 0.3216 | Val Loss: 2.9418 Acc: 0.2677\nBest model saved at epoch 2, val_acc=0.2677\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/6 | Train Loss: 2.1871 Acc: 0.4038 | Val Loss: 2.7004 Acc: 0.3010\nBest model saved at epoch 3, val_acc=0.3010\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/6 | Train Loss: 1.8409 Acc: 0.5009 | Val Loss: 2.9763 Acc: 0.2732\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/6 | Train Loss: 1.5737 Acc: 0.5688 | Val Loss: 3.0074 Acc: 0.3065\nBest model saved at epoch 5, val_acc=0.3065\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/6 | Train Loss: 1.4008 Acc: 0.6097 | Val Loss: 3.0779 Acc: 0.3315\nBest model saved at epoch 6, val_acc=0.3315\nDone. Best val acc: 0.3314840499306519\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 2. pretrain STGCN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass PoseSpatialPartDataset(Dataset):\n    def __init__(self, data_root, txt_file, label_map, part='body'):\n        self.samples = []\n        self.label_map = label_map\n        self.part = part\n        with open(txt_file) as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    print(f\"WARNING: dòng bị lỗi format: {line}\")\n                    continue\n                npy_id = parts[0]\n                label = ' '.join(parts[1:]).strip()\n                if label not in label_map:\n                    print(f\"WARNING: label '{label}' chưa có trong label_map!\")\n                    continue\n                npy_path = f\"{data_root}/{npy_id}_keypoint.npy\"\n                self.samples.append((npy_path, int(label_map[label])))\n        # Define keypoint slices for each part\n        if part == 'body':\n            self.idx_start, self.idx_end = 0, 25\n        elif part == 'left':\n            self.idx_start, self.idx_end = 25, 46\n        elif part == 'right':\n            self.idx_start, self.idx_end = 46, 67\n        else:\n            raise ValueError(\"Unknown part: \" + part)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        npy_path, label = self.samples[idx]\n        keypoints = np.load(npy_path)  # (T, 67, 3) expected\n        # Auto fix shape if needed\n        if keypoints.shape[-2:] == (67, 3):\n            pass\n        elif keypoints.shape[0] == 67 and keypoints.shape[1] == 3:\n            keypoints = np.transpose(keypoints, (2, 0, 1))\n        elif keypoints.shape[1] == 3 and keypoints.shape[2] == 67:\n            keypoints = np.transpose(keypoints, (0, 2, 1))\n        else:\n            raise RuntimeError(f\"Unrecognized keypoints shape: {keypoints.shape}\")\n        part_kp = keypoints[:, self.idx_start:self.idx_end, :]  # (T, N, 3)\n        # Lấy frame giữa\n        t = part_kp.shape[0] // 2\n        part_kp = part_kp[t]  # (N, 3)\n        if idx == 0:\n            print(f\"Dataset part_kp.shape: {part_kp.shape}\")\n        return torch.tensor(part_kp, dtype=torch.float32), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.114141Z","iopub.execute_input":"2025-06-10T15:52:55.114411Z","iopub.status.idle":"2025-06-10T15:52:55.124194Z","shell.execute_reply.started":"2025-06-10T15:52:55.114385Z","shell.execute_reply":"2025-06-10T15:52:55.123373Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SpatialGCNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, A):\n        super().__init__()\n        self.register_buffer('A', A)\n        self.fc = nn.Linear(in_channels, out_channels)\n        self.bn = nn.BatchNorm1d(out_channels)\n        if in_channels != out_channels:\n            self.residual = nn.Linear(in_channels, out_channels)\n        else:\n            self.residual = nn.Identity()\n\n    def forward(self, x):  # x: (B, N, in_channels)\n        # Đảm bảo self.A cùng device với x\n        A = self.A\n        if A.device != x.device:\n            A = A.to(x.device)\n\n        res = self.residual(x)                   # (B, N, out_channels)\n        h = self.fc(x)                           # (B, N, out_channels)\n        h = h.permute(0, 2, 1)                   # (B, out_channels, N)\n        h = torch.matmul(h, A)                   # (B, out_channels, N)\n        # BatchNorm1d expects (B, C, N) => reshape lại cho an toàn\n        h = self.bn(h)                           # (B, out_channels, N)\n        h = h.permute(0, 2, 1)                   # (B, N, out_channels)\n        out = torch.relu(h + res)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.124997Z","iopub.execute_input":"2025-06-10T15:52:55.125241Z","iopub.status.idle":"2025-06-10T15:52:55.137441Z","shell.execute_reply.started":"2025-06-10T15:52:55.125225Z","shell.execute_reply":"2025-06-10T15:52:55.136806Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SpatialPoseEncoder(nn.Module):\n    def __init__(self, in_channels, num_joints, num_classes, A, hid_dim=128, out_dim=256, input_dim=3):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, in_channels)  # (B, N, 3) -> (B, N, in_channels)\n        self.gcn1 = SpatialGCNLayer(in_channels, hid_dim, A)\n        self.gcn2 = SpatialGCNLayer(hid_dim, out_dim, A)\n        self.classifier = nn.Linear(out_dim * num_joints, num_classes)\n\n    def forward(self, x):  # x: (B, N, 3)\n        B, N, C = x.shape\n        assert N == self.gcn1.A.shape[0], f\"x.shape={x.shape}, A.shape={self.gcn1.A.shape}\"\n        x = self.input_proj(x)         # (B, N, in_channels)\n        h = self.gcn1(x)               # (B, N, hid_dim)\n        h = self.gcn2(h)               # (B, N, out_dim)\n        h = h.permute(0, 2, 1)         # (B, out_dim, N)\n        logits = self.classifier(h.flatten(1))  # (B, num_classes)\n        return logits, h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.138263Z","iopub.execute_input":"2025-06-10T15:52:55.138550Z","iopub.status.idle":"2025-06-10T15:52:55.154908Z","shell.execute_reply.started":"2025-06-10T15:52:55.138527Z","shell.execute_reply":"2025-06-10T15:52:55.154308Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_spatial_adjacency(num_node, edge):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[i, j] = 1\n        A[j, i] = 1\n    # Normalize\n    Dl = np.sum(A, 0)\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i] ** (-1)\n    A_normalized = np.dot(A, Dn)\n    return torch.tensor(A_normalized, dtype=torch.float32)\n\ndef get_body_spatial_graph():\n    # 25 body keypoints (Mediapipe hoặc OpenPose định nghĩa)\n    num_node = 25\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1), (1, 2), (2, 3), (3, 7),\n        (0, 4), (4, 5), (5, 6), (6, 8),\n        (9, 10), (11, 12), (11, 13), (13, 15), (15, 21), (15, 19), (15, 17),\n        (17, 19), (11, 23), (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),\n        (18, 20), (12, 24), (23, 24)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\n\ndef get_left_hand_spatial_graph():\n    # 21 left hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\ndef get_right_hand_spatial_graph():\n    # 21 right hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.155585Z","iopub.execute_input":"2025-06-10T15:52:55.155808Z","iopub.status.idle":"2025-06-10T15:52:55.169728Z","shell.execute_reply.started":"2025-06-10T15:52:55.155782Z","shell.execute_reply":"2025-06-10T15:52:55.169066Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"PART_INFO = {\n    'body':  (0, 25),\n    'left':  (25, 46),\n    'right': (46, 67)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.170390Z","iopub.execute_input":"2025-06-10T15:52:55.170630Z","iopub.status.idle":"2025-06-10T15:52:55.181876Z","shell.execute_reply.started":"2025-06-10T15:52:55.170615Z","shell.execute_reply":"2025-06-10T15:52:55.181172Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef train_pose_spatial_part(part, num_joints, get_A_func, best_ckpt_file):\n    print(f\"\\n--- Pretraining {part} ---\")\n    train_ds = PoseSpatialPartDataset(data_root, train_txt, label_map, part=part)\n    val_ds = PoseSpatialPartDataset(data_root, val_txt, label_map, part=part)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    print(f\"Số mẫu train: {len(train_ds)}, val: {len(val_ds)}\")\n    print(f\"Số batch train: {len(train_loader)}, val: {len(val_loader)}\")\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    A = get_A_func().to(device)\n    model = SpatialPoseEncoder(in_channels=3, num_joints=num_joints, num_classes=num_classes, A=A)\n    if torch.cuda.device_count() > 1:\n        print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    best_val_acc = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for x, labels in tqdm(train_loader, desc=f\"Train {part} Epoch {epoch+1}\", leave=False):\n            #print(\"Batch x.shape:\", x.shape)\n            x, labels = x.to(device), labels.to(device)\n            logits, _ = model(x)\n            loss = criterion(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * x.size(0)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += x.size(0)\n        if total > 0:\n            train_acc = correct / total\n            train_loss = total_loss / total\n        else:\n            train_acc = 0\n            train_loss = 0\n            print(\"WARNING: Không có sample nào trong batch train!\")\n        \n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for x, labels in tqdm(val_loader, desc=f\"Val {part} Epoch {epoch+1}\", leave=False):\n                x, labels = x.to(device), labels.to(device)\n                logits, _ = model(x)\n                loss = criterion(logits, labels)\n                val_loss += loss.item() * x.size(0)\n                preds = logits.argmax(1)\n                val_correct += (preds == labels).sum().item()\n                val_total += x.size(0)\n        if val_total > 0:\n            val_acc = val_correct / val_total\n            val_loss = val_loss / val_total\n        else:\n            val_acc = 0\n            val_loss = 0\n            print(\"WARNING: Không có sample nào trong batch val!\")\n\n        print(f\"[{part}] Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\"model\": model.state_dict(), \"label_map\": label_map}, best_ckpt_file)\n            print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\n    print(f\"Done {part}. Best val acc: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.183070Z","iopub.execute_input":"2025-06-10T15:52:55.183402Z","iopub.status.idle":"2025-06-10T15:52:55.196061Z","shell.execute_reply.started":"2025-06-10T15:52:55.183375Z","shell.execute_reply":"2025-06-10T15:52:55.195454Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"data_root = \"/kaggle/input/keypoint/keypoints\"\ntrain_txt = \"/kaggle/working/train.txt\"\nval_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 10\nlr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)\nwith open(\"label_map_pose.json\", \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.196849Z","iopub.execute_input":"2025-06-10T15:52:55.197096Z","iopub.status.idle":"2025-06-10T15:52:55.212016Z","shell.execute_reply.started":"2025-06-10T15:52:55.197073Z","shell.execute_reply":"2025-06-10T15:52:55.211147Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(get_body_spatial_graph().shape)       # (25, 25)\nprint(get_left_hand_spatial_graph().shape)  # (21, 21)\nprint(get_right_hand_spatial_graph().shape) # (21, 21)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.212932Z","iopub.execute_input":"2025-06-10T15:52:55.213211Z","iopub.status.idle":"2025-06-10T15:52:55.223853Z","shell.execute_reply.started":"2025-06-10T15:52:55.213189Z","shell.execute_reply":"2025-06-10T15:52:55.223195Z"}},"outputs":[{"name":"stdout","text":"torch.Size([25, 25])\ntorch.Size([21, 21])\ntorch.Size([21, 21])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_pose_spatial_part('body',  num_joints=25, get_A_func=get_body_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_body_best.pth\")\ntrain_pose_spatial_part('left',  num_joints=21, get_A_func=get_left_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_left_best.pth\")\ntrain_pose_spatial_part('right', num_joints=21, get_A_func=get_right_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_right_best.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:52:55.224884Z","iopub.execute_input":"2025-06-10T15:52:55.225165Z","iopub.status.idle":"2025-06-10T15:53:13.357030Z","shell.execute_reply.started":"2025-06-10T15:52:55.225121Z","shell.execute_reply":"2025-06-10T15:53:13.356125Z"}},"outputs":[{"name":"stdout","text":"\n--- Pretraining body ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n  return F.linear(input, self.weight, self.bias)\nTrain body Epoch 1:   7%|▋         | 1/15 [00:00<00:04,  3.40it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 1/10 | Train Loss: 3.9770 Acc: 0.0702 | Val Loss: 3.4690 Acc: 0.0552\nBest model saved at epoch 1, val_acc=0.0552\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 2/10 | Train Loss: 2.9187 Acc: 0.1996 | Val Loss: 3.4556 Acc: 0.0759\nBest model saved at epoch 2, val_acc=0.0759\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 3:   7%|▋         | 1/15 [00:00<00:02,  6.16it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 3/10 | Train Loss: 2.5772 Acc: 0.2171 | Val Loss: 3.7884 Acc: 0.0897\nBest model saved at epoch 3, val_acc=0.0897\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 4/10 | Train Loss: 2.3808 Acc: 0.2566 | Val Loss: 3.6181 Acc: 0.1034\nBest model saved at epoch 4, val_acc=0.1034\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 5/10 | Train Loss: 2.2740 Acc: 0.2982 | Val Loss: 3.2567 Acc: 0.1517\nBest model saved at epoch 5, val_acc=0.1517\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 6:   7%|▋         | 1/15 [00:00<00:02,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 6/10 | Train Loss: 2.1701 Acc: 0.3333 | Val Loss: 2.9016 Acc: 0.2207\nBest model saved at epoch 6, val_acc=0.2207\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 7/10 | Train Loss: 2.0709 Acc: 0.3553 | Val Loss: 3.2621 Acc: 0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 8/10 | Train Loss: 1.9551 Acc: 0.3794 | Val Loss: 3.2639 Acc: 0.2000\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 9:   7%|▋         | 1/15 [00:00<00:02,  6.58it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 9/10 | Train Loss: 1.9683 Acc: 0.3860 | Val Loss: 3.0793 Acc: 0.2138\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 10/10 | Train Loss: 1.8564 Acc: 0.4079 | Val Loss: 3.0937 Acc: 0.2552\nBest model saved at epoch 10, val_acc=0.2552\nDone body. Best val acc: 0.2552\n\n--- Pretraining left ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 1/10 | Train Loss: 3.9994 Acc: 0.0636 | Val Loss: 3.3982 Acc: 0.0414\nBest model saved at epoch 1, val_acc=0.0414\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 2/10 | Train Loss: 2.9818 Acc: 0.1338 | Val Loss: 3.3133 Acc: 0.0276\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 3/10 | Train Loss: 2.8101 Acc: 0.1557 | Val Loss: 3.2798 Acc: 0.0828\nBest model saved at epoch 3, val_acc=0.0828\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 4/10 | Train Loss: 2.6940 Acc: 0.2039 | Val Loss: 3.0431 Acc: 0.1655\nBest model saved at epoch 4, val_acc=0.1655\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 5/10 | Train Loss: 2.6751 Acc: 0.1996 | Val Loss: 3.0014 Acc: 0.1379\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 6:   7%|▋         | 1/15 [00:00<00:02,  6.59it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 6/10 | Train Loss: 2.6486 Acc: 0.1930 | Val Loss: 2.9682 Acc: 0.1793\nBest model saved at epoch 6, val_acc=0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 7/10 | Train Loss: 2.5875 Acc: 0.2061 | Val Loss: 3.0088 Acc: 0.1862\nBest model saved at epoch 7, val_acc=0.1862\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 8:   7%|▋         | 1/15 [00:00<00:02,  6.65it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 8/10 | Train Loss: 2.5227 Acc: 0.2368 | Val Loss: 2.9322 Acc: 0.2276\nBest model saved at epoch 8, val_acc=0.2276\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 9/10 | Train Loss: 2.5406 Acc: 0.2675 | Val Loss: 3.0394 Acc: 0.1586\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 10/10 | Train Loss: 2.4836 Acc: 0.2303 | Val Loss: 2.9927 Acc: 0.2276\nDone left. Best val acc: 0.2276\n\n--- Pretraining right ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 1/10 | Train Loss: 3.9868 Acc: 0.0768 | Val Loss: 3.4628 Acc: 0.0276\nBest model saved at epoch 1, val_acc=0.0276\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 2/10 | Train Loss: 3.0264 Acc: 0.1425 | Val Loss: 3.4256 Acc: 0.0414\nBest model saved at epoch 2, val_acc=0.0414\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 3/10 | Train Loss: 2.7901 Acc: 0.1645 | Val Loss: 3.2728 Acc: 0.1034\nBest model saved at epoch 3, val_acc=0.1034\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 4/10 | Train Loss: 2.5561 Acc: 0.2917 | Val Loss: 3.1556 Acc: 0.1241\nBest model saved at epoch 4, val_acc=0.1241\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 5:   7%|▋         | 1/15 [00:00<00:02,  6.65it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 5/10 | Train Loss: 2.5337 Acc: 0.2675 | Val Loss: 2.9142 Acc: 0.2345\nBest model saved at epoch 5, val_acc=0.2345\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 6/10 | Train Loss: 2.4139 Acc: 0.2961 | Val Loss: 2.8280 Acc: 0.2759\nBest model saved at epoch 6, val_acc=0.2759\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 7/10 | Train Loss: 2.3448 Acc: 0.2961 | Val Loss: 2.9001 Acc: 0.2897\nBest model saved at epoch 7, val_acc=0.2897\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 8:   7%|▋         | 1/15 [00:00<00:02,  6.24it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 8/10 | Train Loss: 2.1588 Acc: 0.3794 | Val Loss: 3.0839 Acc: 0.2483\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 9:   7%|▋         | 1/15 [00:00<00:02,  6.15it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 9/10 | Train Loss: 2.0740 Acc: 0.3640 | Val Loss: 2.8324 Acc: 0.2828\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 10:   7%|▋         | 1/15 [00:00<00:02,  6.29it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 ","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 10/10 | Train Loss: 2.1001 Acc: 0.3904 | Val Loss: 2.9052 Acc: 0.2483\nDone right. Best val acc: 0.2897\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# 3. WLASL Module","metadata":{}},{"cell_type":"markdown","source":"# 3.1. Fusion Module","metadata":{}},{"cell_type":"markdown","source":"# sửa lại mask","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport glob\nimport cv2\n\ndef get_n_frames(video_path):\n    cap = cv2.VideoCapture(video_path)\n    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return n_frames\n    \ndef get_sorted_indexes(crop_dir, video_id, hand):\n    # hand: 'left' hoặc 'right'\n    pattern = os.path.join(crop_dir, f\"{video_id}_frame*_{hand}.jpg\")\n    files = glob.glob(pattern)\n    idxs = []\n    for f in files:\n        # Lấy index từ tên file: ..._frame<idx>_left.jpg hoặc ..._frame<idx>_right.jpg\n        basename = os.path.basename(f)\n        idx = int(basename.split(\"_frame\")[1].split(f\"_{hand}\")[0])\n        idxs.append(idx)\n    idxs = sorted(list(set(idxs)))\n    return idxs\n\ndef make_mask_for_hand(n_frames, crop_idxs, out_len=64):\n    \"\"\"\n    n_frames: số frame gốc của video\n    crop_idxs: list index frame gốc có ảnh crop tay\n    out_len: số frame muốn nội suy (thường = 64)\n    Đảm bảo số mask[i]=1 đúng bằng số crop_idxs (số ảnh crop)\n    \"\"\"\n    idxs_interp = np.linspace(0, n_frames - 1, out_len)\n    mask = np.zeros(out_len, dtype=np.uint8)\n    used_positions = set()\n    for crop_idx in crop_idxs:\n        # Tìm vị trí chưa được gán nào gần nhất crop_idx\n        distances = np.abs(idxs_interp - crop_idx)\n        for i in np.argsort(distances):\n            if i not in used_positions:\n                mask[i] = 1\n                used_positions.add(i)\n                break\n    return mask\n\ndef save_masks_for_video(video_id, crop_dir, video_dir, out_dir, out_len=64):\n    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n    n_frames = get_n_frames(video_path)\n    idxs_left = get_sorted_indexes(crop_dir, video_id, 'left')\n    idxs_right = get_sorted_indexes(crop_dir, video_id, 'right')\n\n    mask_left = make_mask_for_hand(n_frames, idxs_left, out_len)\n    mask_right = make_mask_for_hand(n_frames, idxs_right, out_len)\n\n    np.save(os.path.join(out_dir, f\"{video_id}_mask_left.npy\"), mask_left)\n    np.save(os.path.join(out_dir, f\"{video_id}_mask_right.npy\"), mask_right)\n    print(f\"Saved masks for {video_id}: left sum={mask_left.sum()}, right sum={mask_right.sum()} (left imgs={len(idxs_left)}, right imgs={len(idxs_right)})\")\n\nCROPPED_HANDS_DIR = \"/kaggle/input/image-mask/cropped_hands\"\nVIDEO_DIR = \"/kaggle/input/wlasl-processed/WLASL/videos\"\nOUT_DIR = '/kaggle/working/mask'\n\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef get_video_ids_from_txt(txt_path):\n    video_ids = []\n    with open(txt_path, \"r\") as f:\n        for line in f:\n            video_id = line.strip().split()[0]\n            video_ids.append(video_id)\n    return video_ids\n\n# Ví dụ xử lý cho train\ntxt_path = \"/kaggle/input/keypoint/videoid_label.txt\"\nvideo_ids = get_video_ids_from_txt(txt_path)\n\nfor video_id in video_ids:\n    save_masks_for_video(video_id, CROPPED_HANDS_DIR, VIDEO_DIR, OUT_DIR, out_len=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:13.358724Z","iopub.execute_input":"2025-06-10T15:53:13.358948Z","iopub.status.idle":"2025-06-10T15:53:25.417943Z","shell.execute_reply.started":"2025-06-10T15:53:13.358924Z","shell.execute_reply":"2025-06-10T15:53:25.417289Z"}},"outputs":[{"name":"stdout","text":"Saved masks for 69241: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 65225: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 68011: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 68208: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 68012: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 70212: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 70266: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 07085: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 07086: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 07087: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07069: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 07088: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 07089: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 07090: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07091: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07092: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 07093: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 07068: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 07094: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07095: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 07096: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 07097: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 07070: left sum=1, right sum=0 (left imgs=1, right imgs=0)\nSaved masks for 07098: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 07099: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07071: left sum=5, right sum=3 (left imgs=5, right imgs=3)\nSaved masks for 07072: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 07073: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 67424: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 07074: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 07075: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07076: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 07077: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07078: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07079: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07080: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 07081: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07082: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 07083: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 07084: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 69302: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 65539: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 70173: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 68538: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 68042: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 68660: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68041: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 17725: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17726: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 17727: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 17728: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17710: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 17729: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17730: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17731: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17732: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17733: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65540: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17734: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17711: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17712: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17713: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17714: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 17715: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 17716: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 17717: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17718: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 17709: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 67594: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 17719: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 17720: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 17721: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 17722: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 17723: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 17724: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 12306: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 68028: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 69054: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 12328: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 12329: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12330: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12312: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12331: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12332: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 12333: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12335: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 12336: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 12311: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 12337: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12338: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 12313: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 12314: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 12315: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 12316: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 12317: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 12318: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 12319: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 12320: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 67519: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 12321: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 12322: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 12323: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 12324: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 12326: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 12327: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 05724: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 70348: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 68007: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 05744: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 05746: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 05728: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 05747: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 05748: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 05749: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 05750: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 05729: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 05730: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 65167: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 05731: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 05732: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 05733: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 05734: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 05735: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 05736: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 05727: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 05737: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 05739: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 05740: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 05741: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 05742: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 05743: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 09847: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 70230: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 68580: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 70263: left sum=4, right sum=0 (left imgs=4, right imgs=0)\nSaved masks for 68019: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 09865: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09866: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 09848: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09867: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09869: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09849: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 09850: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 09851: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 65328: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 09853: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 09854: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 09855: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09856: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 67483: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 09857: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09858: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09859: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09860: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09861: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09862: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 09863: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 24857: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69345: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 68292: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 24955: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 24956: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 24941: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 24960: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 24961: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 24962: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 24965: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 24969: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 24970: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 24971: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 24972: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65824: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 24973: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 24943: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 24946: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 24947: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 24948: left sum=5, right sum=2 (left imgs=5, right imgs=2)\nSaved masks for 24940: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 67715: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 24950: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 24951: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 24952: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 24954: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 11305: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 68870: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 68024: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 11327: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11328: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11310: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11329: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 11330: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 11311: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 11312: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 11313: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 11314: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 11315: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 11309: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11316: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11317: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 11318: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 11319: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 11320: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 11321: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 11322: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 11323: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11324: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11325: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 11326: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 63219: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69534: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 68890: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 68183: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63239: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63240: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63241: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63242: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 63226: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63227: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63228: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 63229: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63230: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 63231: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 66778: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63232: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63233: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 63234: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 66779: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 66780: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63225: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67066: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63236: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 63237: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 63238: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 08909: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 68018: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 70326: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 68790: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08928: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08929: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08916: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08917: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08918: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08919: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08920: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 08921: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08922: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 08923: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65298: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 08924: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 65299: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65300: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 08915: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67468: left sum=1, right sum=5 (left imgs=1, right imgs=5)\nSaved masks for 67470: left sum=2, right sum=5 (left imgs=2, right imgs=5)\nSaved masks for 08925: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 08926: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 08927: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65415: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 70332: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 68592: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13647: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 13648: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 13631: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13632: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13633: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 13634: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 13635: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 13636: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13637: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13638: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13630: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 13639: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 67535: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13640: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13641: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13642: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13643: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13644: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13645: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 13646: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 14855: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 70015: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 68586: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 68033: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 14899: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 14900: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14882: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 14901: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14903: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 14883: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14884: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14885: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14886: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65445: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14887: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 14888: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 14889: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 14891: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14893: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 14894: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 14895: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 14896: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 14898: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 65717: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 70234: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 68048: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 21885: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 21886: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 21887: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 21888: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 21870: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 21889: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 21890: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 21891: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 21871: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 21872: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 21869: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 21874: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 21875: left sum=5, right sum=3 (left imgs=5, right imgs=3)\nSaved masks for 21876: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 21878: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 67660: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 67659: left sum=2, right sum=5 (left imgs=2, right imgs=5)\nSaved masks for 21883: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 21884: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 27194: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69364: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 27219: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 27220: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 27221: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 27206: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 27207: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 27208: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 27209: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 27210: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 27211: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 27212: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 65890: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 27213: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 27214: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 65891: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 65889: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 67757: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 27215: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 27216: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 27217: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 27218: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 38482: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69411: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68110: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68406: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 38539: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 38540: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 38541: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 38542: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 38525: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 38544: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 38527: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 38529: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 38530: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 38531: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 38532: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 38533: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 66183: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 38524: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67934: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 38534: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 38536: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 38538: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57919: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 57948: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57949: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57950: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57952: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57934: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 57953: left sum=4, right sum=2 (left imgs=4, right imgs=2)\nSaved masks for 57935: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 57936: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57937: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 66606: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57939: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 57940: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57941: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57942: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 66607: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 57933: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 67299: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 57943: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 57944: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57945: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57947: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 62152: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 68852: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 68918: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 68177: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 62173: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 62174: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62175: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 62159: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 62160: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 62163: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62164: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 62165: left sum=1, right sum=5 (left imgs=1, right imgs=5)\nSaved masks for 66743: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62166: left sum=3, right sum=5 (left imgs=3, right imgs=5)\nSaved masks for 66742: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62158: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 67036: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62168: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 62169: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 62170: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 62171: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 62172: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 64201: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 64211: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 68416: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 68190: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 70306: left sum=4, right sum=2 (left imgs=4, right imgs=2)\nSaved masks for 68992: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 64223: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 64224: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 64210: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 64212: left sum=4, right sum=2 (left imgs=4, right imgs=2)\nSaved masks for 66818: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 64213: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 66816: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 64209: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 64215: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 64216: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 67012: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 64217: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 64219: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 64218: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 64221: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 64222: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 64275: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69546: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 64287: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68192: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 64290: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 64284: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 64291: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 64292: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 64293: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 64294: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 64295: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 64296: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 66820: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 64297: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 64280: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 64298: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 64299: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 64300: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 64281: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 64288: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 64289: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 64283: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 01912: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69206: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 68001: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68720: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 02002: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 02003: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 01987: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 01988: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 01989: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 01990: left sum=1, right sum=5 (left imgs=1, right imgs=5)\nSaved masks for 01991: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 01992: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 65043: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 01995: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 01986: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 67356: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 01996: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 01997: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 01998: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 01999: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 02000: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 06455: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 69236: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68230: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 70244: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 06472: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 06473: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06474: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06475: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06476: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 06477: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 06478: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 06480: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 65200: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06471: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67414: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06481: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 06482: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 06483: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 06484: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 06485: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 06486: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 69281: left sum=4, right sum=0 (left imgs=4, right imgs=0)\nSaved masks for 65403: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 70271: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 13213: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 13214: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13197: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13216: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 13217: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13198: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13199: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 13200: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13201: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 65402: left sum=3, right sum=0 (left imgs=3, right imgs=0)\nSaved masks for 13202: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 13203: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 13205: left sum=2, right sum=5 (left imgs=2, right imgs=5)\nSaved masks for 13196: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 13206: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 13207: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 13208: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 13209: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 21933: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 70361: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 68050: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 68674: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 68688: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 21942: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 21943: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 21944: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 21945: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 21946: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 21949: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 21950: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 65721: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 21941: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 67663: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 21951: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 21952: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 21953: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 21954: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 21955: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 21956: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 28074: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69368: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 70270: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 28125: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 28108: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28109: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28110: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28111: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 28112: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 28113: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 28114: left sum=3, right sum=5 (left imgs=3, right imgs=5)\nSaved masks for 28115: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 28116: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 28107: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 28118: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28119: left sum=5, right sum=0 (left imgs=5, right imgs=0)\nSaved masks for 28120: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 28121: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28122: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 28123: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 28124: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 69389: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33266: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68093: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 70299: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 33285: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33267: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 33286: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 33268: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 33269: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33270: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 33271: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 33273: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 33274: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 33277: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 33278: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33279: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33280: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33281: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 33282: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 33283: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 33284: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 34822: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69396: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 70308: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 34823: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 34824: left sum=4, right sum=5 (left imgs=4, right imgs=5)\nSaved masks for 34825: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 34826: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 34827: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 34828: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 34829: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 34830: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 34831: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 67873: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 34832: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 34833: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 34834: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 34835: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 34836: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 34837: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 34838: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 34839: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 36927: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69402: left sum=2, right sum=0 (left imgs=2, right imgs=0)\nSaved masks for 36945: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36946: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 36930: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 36931: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 36932: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36933: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36934: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 36936: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 36937: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36938: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 66147: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 66146: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 36929: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67908: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 36939: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36940: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 36941: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 36942: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 36944: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 38982: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69413: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 38994: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 70345: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 68694: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 68770: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 68114: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 38999: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 39000: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 39001: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 39002: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 39003: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 38990: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 67937: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 39004: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 39005: left sum=3, right sum=4 (left imgs=3, right imgs=4)\nSaved masks for 39006: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 38991: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 38995: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 38996: left sum=3, right sum=1 (left imgs=3, right imgs=1)\nSaved masks for 38997: left sum=2, right sum=5 (left imgs=2, right imgs=5)\nSaved masks for 40114: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 69422: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 70310: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 70249: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 68122: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 40115: left sum=1, right sum=3 (left imgs=1, right imgs=3)\nSaved masks for 40116: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 40117: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 40118: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 40119: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 40120: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 40121: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 40122: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 40123: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 66246: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 67949: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 40124: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 40126: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 40128: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 40129: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 40130: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 56552: left sum=5, right sum=4 (left imgs=5, right imgs=4)\nSaved masks for 70323: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 68972: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 56573: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 56574: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 56557: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 56576: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 56577: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 56578: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 56579: left sum=2, right sum=0 (left imgs=2, right imgs=0)\nSaved masks for 56558: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 56556: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 56560: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 56563: left sum=5, right sum=2 (left imgs=5, right imgs=2)\nSaved masks for 56564: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 67275: left sum=3, right sum=1 (left imgs=3, right imgs=1)\nSaved masks for 56566: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 56567: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 56568: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 56569: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 56571: left sum=1, right sum=1 (left imgs=1, right imgs=1)\nSaved masks for 69092: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57645: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57646: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 57628: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 57647: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 57629: left sum=1, right sum=4 (left imgs=1, right imgs=4)\nSaved masks for 57630: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 57631: left sum=5, right sum=3 (left imgs=5, right imgs=3)\nSaved masks for 57632: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 57633: left sum=2, right sum=2 (left imgs=2, right imgs=2)\nSaved masks for 57634: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 67292: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 57635: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 57636: left sum=3, right sum=5 (left imgs=3, right imgs=5)\nSaved masks for 57637: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57638: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 57639: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57640: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57641: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 57642: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 57643: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 62944: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 69531: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 68330: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62967: left sum=3, right sum=1 (left imgs=3, right imgs=1)\nSaved masks for 62968: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 62982: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 62970: left sum=0, right sum=0 (left imgs=0, right imgs=0)\nSaved masks for 62984: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 62971: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 66769: left sum=2, right sum=4 (left imgs=2, right imgs=4)\nSaved masks for 67057: left sum=4, right sum=4 (left imgs=4, right imgs=4)\nSaved masks for 62986: left sum=2, right sum=3 (left imgs=2, right imgs=3)\nSaved masks for 62973: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 62987: left sum=2, right sum=1 (left imgs=2, right imgs=1)\nSaved masks for 62964: left sum=3, right sum=3 (left imgs=3, right imgs=3)\nSaved masks for 67058: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 62988: left sum=4, right sum=3 (left imgs=4, right imgs=3)\nSaved masks for 62975: left sum=5, right sum=5 (left imgs=5, right imgs=5)\nSaved masks for 62965: left sum=1, right sum=2 (left imgs=1, right imgs=2)\nSaved masks for 62979: left sum=5, right sum=3 (left imgs=5, right imgs=3)\nSaved masks for 62966: left sum=3, right sum=2 (left imgs=3, right imgs=2)\nSaved masks for 63662: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 68187: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 68764: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63679: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63665: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63666: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 63667: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63668: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63669: left sum=0, right sum=1 (left imgs=0, right imgs=1)\nSaved masks for 63670: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63671: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 63672: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63673: left sum=0, right sum=5 (left imgs=0, right imgs=5)\nSaved masks for 66798: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 66799: left sum=0, right sum=2 (left imgs=0, right imgs=2)\nSaved masks for 63664: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 67078: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63675: left sum=0, right sum=4 (left imgs=0, right imgs=4)\nSaved masks for 63676: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63677: left sum=0, right sum=3 (left imgs=0, right imgs=3)\nSaved masks for 63678: left sum=0, right sum=4 (left imgs=0, right imgs=4)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import numpy as np\n\nkp = np.load('/kaggle/working/mask/63678_mask_right.npy')\nprint(kp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.418841Z","iopub.execute_input":"2025-06-10T15:53:25.419158Z","iopub.status.idle":"2025-06-10T15:53:25.424720Z","shell.execute_reply.started":"2025-06-10T15:53:25.419113Z","shell.execute_reply":"2025-06-10T15:53:25.424010Z"}},"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Continue ... ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DeformableAttention2D(nn.Module):\n    \"\"\"\n    Deformable 2D attention:\n    - query: (B, N, d_model)     # N: số keypoints\n    - key, value: (B, C, H, W)   # feature map từ RGB backbone\n    - ref_points: (B, N, 2)      # reference keypoint position (pixel, normalized [0,1])\n    \"\"\"\n    def __init__(self, d_model, n_heads, n_points=4):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_points = n_points\n\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Conv2d(d_model, d_model, 1)\n        self.value_proj = nn.Conv2d(d_model, d_model, 1)\n\n        # For each head, predict offset for each reference point (dx, dy) per query\n        self.offset = nn.Linear(d_model, n_heads * n_points * 2)\n        # For each head, predict attention weights for each sampled point\n        self.attn_weight = nn.Linear(d_model, n_heads * n_points)\n\n    def forward(self, query, key, value, ref_points):\n        # query: (B, N, d_model)\n        # key, value: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature size\n        B, N, d_model = query.shape\n        H, W = key.shape[2], key.shape[3]\n\n        # Project query, key, value\n        query_proj = self.query_proj(query)  # (B, N, d_model)\n        key_proj = self.key_proj(key)        # (B, d_model, H, W)\n        value_proj = self.value_proj(value)  # (B, d_model, H, W)\n\n        # Predict offsets and attention weights\n        offset = self.offset(query_proj)     # (B, N, n_heads*n_points*2)\n        offset = offset.view(B, N, self.n_heads, self.n_points, 2)\n\n        attn_weight = self.attn_weight(query_proj)  # (B, N, n_heads*n_points)\n        attn_weight = attn_weight.view(B, N, self.n_heads, self.n_points)\n        attn_weight = F.softmax(attn_weight, dim=-1)\n\n        # Calculate sampling locations (normalized in [0, 1])\n        # ref_points: (B, N, 2), offset: (B, N, n_heads, n_points, 2)\n        # Sampling positions: (B, N, n_heads, n_points, 2)\n        sampling_locations = ref_points.unsqueeze(2).unsqueeze(3) + offset / torch.tensor([W, H], device=offset.device)\n        # Clamp to [0, 1]\n        sampling_locations = sampling_locations.clamp(0, 1)\n\n        # Prepare for grid_sample: scale to [-1, 1]\n        # grid_sample expects normalized coords in [-1, 1]\n        sampling_grid = sampling_locations.clone()\n        sampling_grid[..., 0] = sampling_grid[..., 0] * 2 - 1\n        sampling_grid[..., 1] = sampling_grid[..., 1] * 2 - 1\n\n        # Sample value features at sampled locations\n        # value_proj: (B, d_model, H, W)\n        # For each query (B, N, n_heads, n_points, 2), sample (B, d_model, n_heads, n_points)\n        sampled_feats = []\n        for b in range(B):\n            feats = []\n            for n in range(N):\n                grid = sampling_grid[b, n]  # (n_heads, n_points, 2)\n                grid = grid.view(1, -1, 1, 2)  # (1, n_heads*n_points, 1, 2)\n                # grid_sample: input (B, C, H, W), grid (B, out_H*out_W, 1, 2)\n                sampled = F.grid_sample(\n                    value_proj[b:b+1], grid, mode='bilinear', align_corners=True\n                )  # (1, d_model, n_heads*n_points, 1)\n                sampled = sampled.view(d_model, self.n_heads, self.n_points)\n                feats.append(sampled)\n            # feats: list of N tensors (d_model, n_heads, n_points) -> (N, d_model, n_heads, n_points)\n            feats = torch.stack(feats, dim=0)\n            sampled_feats.append(feats)\n        # (B, N, d_model, n_heads, n_points)\n        sampled_feats = torch.stack(sampled_feats, dim=0)\n\n        # Weighted sum by attention weights\n        # attn_weight: (B, N, n_heads, n_points)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # (B, N, n_points, n_heads)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # back to (B, N, n_heads, n_points)\n        # (B, N, d_model, n_heads, n_points) * (B, N, 1, n_heads, n_points) -> sum over n_points\n        out = (sampled_feats * attn_weight.unsqueeze(2)).sum(-1)  # (B, N, d_model, n_heads)\n        out = out.mean(-1)  # mean over heads: (B, N, d_model)\n\n        return out  # (B, N, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.425623Z","iopub.execute_input":"2025-06-10T15:53:25.425907Z","iopub.status.idle":"2025-06-10T15:53:25.440397Z","shell.execute_reply.started":"2025-06-10T15:53:25.425877Z","shell.execute_reply":"2025-06-10T15:53:25.439740Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class PGFModule(nn.Module):\n    \"\"\"\n    PGF Module: Multi-head deformable attention for fusion keypoint + RGB feature\n    - query_feat: (B, N, d_model) -- e.g. STGCN output per joint\n    - rgb_feat: (B, d_model, H, W) -- output feature map from CNN/ViT\n    - ref_points: (B, N, 2) -- reference position (normalized [0,1]) for each joint\n    Output: (B, N, d_model) fused feature per joint\n    \"\"\"\n    def __init__(self, d_model, n_heads=4, n_points=4):\n        super().__init__()\n        self.attn = DeformableAttention2D(d_model, n_heads, n_points)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, query_feat, rgb_feat, ref_points):\n        # query_feat: (B, N, d_model)\n        # rgb_feat: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature map size\n        fused = self.attn(query_feat, rgb_feat, rgb_feat, ref_points)  # (B, N, d_model)\n        out = self.layer_norm(fused + query_feat)  # residual connection\n        return out  # (B, N, d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.440982Z","iopub.execute_input":"2025-06-10T15:53:25.441222Z","iopub.status.idle":"2025-06-10T15:53:25.454022Z","shell.execute_reply.started":"2025-06-10T15:53:25.441196Z","shell.execute_reply":"2025-06-10T15:53:25.453390Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TemporalEncoder(nn.Module):\n    \"\"\"\n    Temporal encoder cho input dạng (B, T, C, N).\n    Học đặc trưng temporal cho từng joint (giữ spatial/joint riêng).\n    Output: (B, C, N) (nếu pool theo T), hoặc (B, T, C, N) nếu pool=None.\n    \"\"\"\n    def __init__(self, c_model, n_joints, nhead=8, num_layers=2, dim_feedforward=512, dropout=0.1, pool='mean'):\n        \"\"\"\n        c_model: số channels đầu vào cho mỗi joint (ví dụ C).\n        n_joints: số joint (N).\n        pool: 'mean' (mean theo T), 'last' (lấy frame cuối), hoặc None (giữ nguyên (B, T, C, N)).\n        \"\"\"\n        super().__init__()\n        self.n_joints = n_joints\n        self.c_model = c_model\n        self.pool = pool\n        # Một transformer encoder riêng cho mỗi joint\n        self.transformer_list = nn.ModuleList([\n            nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(d_model=c_model, nhead=nhead, \n                                          dim_feedforward=dim_feedforward, \n                                          batch_first=True, dropout=dropout),\n                num_layers=num_layers\n            ) for _ in range(n_joints)\n        ])\n        self.output_dim = c_model * n_joints if pool is None else c_model * n_joints\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, T, C, N)\n        Output:\n            - (B, C, N) nếu pool\n            - (B, T, C, N) nếu pool=None\n        \"\"\"\n        B, T, C, N = x.shape\n        # Chuyển sang list các tensor (B, T, C) cho từng joint\n        outs = []\n        for j in range(N):\n            x_j = x[..., j]            # (B, T, C)\n            # Transformer yêu cầu (B, T, C_model)\n            out_j = self.transformer_list[j](x_j)   # (B, T, C)\n            if self.pool == 'mean':\n                out_j = out_j.mean(dim=1)           # (B, C)\n            elif self.pool == 'last':\n                out_j = out_j[:, -1, :]             # (B, C)\n            # else giữ (B, T, C)\n            outs.append(out_j)\n        if self.pool is not None:\n            # outs: list (B, C), ghép thành (B, C, N)\n            out = torch.stack(outs, dim=-1)         # (B, C, N)\n        else:\n            # outs: list (B, T, C), ghép thành (B, T, C, N)\n            out = torch.stack(outs, dim=-1)         # (B, T, C, N)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.454947Z","iopub.execute_input":"2025-06-10T15:53:25.455221Z","iopub.status.idle":"2025-06-10T15:53:25.471517Z","shell.execute_reply.started":"2025-06-10T15:53:25.455198Z","shell.execute_reply":"2025-06-10T15:53:25.470854Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nimport numpy as np\nimport json\nimport glob\nimport re\n\nDATA_ROOT = '/kaggle/input'\nCROPPED_HANDS_DIR = os.path.join(DATA_ROOT, 'image-mask', 'cropped_hands')\nKEYPOINT_DIR = os.path.join(DATA_ROOT, 'keypoint', 'keypoints')\nMASK_DIR = '/kaggle/working/mask'\n\nSPLIT_TXT = {\n    'train': '/kaggle/working/train.txt',\n    'val': '/kaggle/working/val.txt',\n    'test': '/kaggle/working/test.txt'\n}\n\n# Đọc label_map\nwith open('/kaggle/working/label_map_pose.json') as f:\n    label_map = json.load(f)\n\ndef get_sorted_crop_idxs(crop_dir, video_id, hand):\n    pattern = os.path.join(crop_dir, f\"{video_id}_frame*_{hand}.jpg\")\n    files = glob.glob(pattern)\n    idxs = []\n    for f in files:\n        basename = os.path.basename(f)\n        match = re.search(rf\"{video_id}_frame(\\d+)_{hand}\\.jpg\", basename)\n        if match:\n            idx = int(match.group(1))\n            idxs.append(idx)\n    idxs = sorted(list(set(idxs)))\n    return idxs\n\ndef build_meta_list(split_txt_path):\n    meta_list = []\n    with open(split_txt_path, 'r') as f:\n        lines = f.readlines()\n    for line in lines:\n        video_id, label_str = line.strip().split()\n        label = label_map[label_str]\n\n        keypoint_path = os.path.join(KEYPOINT_DIR, f\"{video_id}_keypoint.npy\")\n        mask_left_path = os.path.join(MASK_DIR, f\"{video_id}_mask_left.npy\")\n        mask_right_path = os.path.join(MASK_DIR, f\"{video_id}_mask_right.npy\")\n        mask_left = np.load(mask_left_path)\n        mask_right = np.load(mask_right_path)\n        num_frames = len(mask_left)\n\n        # Lấy index các frame thực sự có ảnh crop left/right\n        crop_left_idxs = get_sorted_crop_idxs(CROPPED_HANDS_DIR, video_id, 'left')\n        crop_right_idxs = get_sorted_crop_idxs(CROPPED_HANDS_DIR, video_id, 'right')\n\n        # Đếm số lượng 1 trong mask và số lượng ảnh crop\n        n_img_left = len(crop_left_idxs)\n        n_mask_left = int(mask_left.sum())\n        n_img_right = len(crop_right_idxs)\n        n_mask_right = int(mask_right.sum())\n        if n_img_left != n_mask_left:\n            print(f\"[CHECK] video_id={video_id}: LEFT - {n_img_left} images, {n_mask_left} mask=1\")\n        if n_img_right != n_mask_right:\n            print(f\"[CHECK] video_id={video_id}: RIGHT - {n_img_right} images, {n_mask_right} mask=1\")\n\n        crop_left_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left.jpg\") for idx in crop_left_idxs]\n        kp_j_left_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left_kp.npy\") for idx in crop_left_idxs]\n        crop_right_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right.jpg\") for idx in crop_right_idxs]\n        kp_j_right_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right_kp.npy\") for idx in crop_right_idxs]\n\n        meta = {\n            \"video_id\": video_id,\n            \"keypoint_path\": keypoint_path,\n            \"label\": label,\n            \"num_frames\": num_frames,\n            \"mask_left_path\": mask_left_path,\n            \"mask_right_path\": mask_right_path,\n            \"crop_left_files\": crop_left_files,\n            \"kp_j_left_files\": kp_j_left_files,\n            \"crop_right_files\": crop_right_files,\n            \"kp_j_right_files\": kp_j_right_files\n        }\n        meta_list.append(meta)\n    return meta_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.472255Z","iopub.execute_input":"2025-06-10T15:53:25.472491Z","iopub.status.idle":"2025-06-10T15:53:25.487664Z","shell.execute_reply.started":"2025-06-10T15:53:25.472464Z","shell.execute_reply":"2025-06-10T15:53:25.487148Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'train'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:25.488360Z","iopub.execute_input":"2025-06-10T15:53:25.488615Z","iopub.status.idle":"2025-06-10T15:53:31.489737Z","shell.execute_reply.started":"2025-06-10T15:53:25.488593Z","shell.execute_reply":"2025-06-10T15:53:31.488897Z"}},"outputs":[{"name":"stdout","text":"Saved train_meta.json with 456 samples\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'test'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:31.493353Z","iopub.execute_input":"2025-06-10T15:53:31.493591Z","iopub.status.idle":"2025-06-10T15:53:33.411023Z","shell.execute_reply.started":"2025-06-10T15:53:31.493571Z","shell.execute_reply":"2025-06-10T15:53:33.410380Z"}},"outputs":[{"name":"stdout","text":"Saved test_meta.json with 150 samples\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'val'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:33.412007Z","iopub.execute_input":"2025-06-10T15:53:33.412321Z","iopub.status.idle":"2025-06-10T15:53:35.707329Z","shell.execute_reply.started":"2025-06-10T15:53:33.412293Z","shell.execute_reply":"2025-06-10T15:53:35.706655Z"}},"outputs":[{"name":"stdout","text":"Saved val_meta.json with 145 samples\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ClassifySign(nn.Module):\n    def __init__(\n        self,\n        vision_encoder, stgcn_body, stgcn_left, stgcn_right,\n        pgf_module, temporal_encoder_body, temporal_encoder_hand, \n        num_classes,\n        left_idx=21, right_idx=21, body_idx=25,\n        fcn_hidden=256, dropout=0.5,\n        n_joint=67, feature_dim=256\n    ):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.stgcn_body = stgcn_body\n        self.stgcn_left = stgcn_left\n        self.stgcn_right = stgcn_right\n        self.pgf_module = pgf_module\n        self.temporal_encoder_body = temporal_encoder_body\n        self.temporal_encoder_hand = temporal_encoder_hand\n        self.left_idx = left_idx\n        self.right_idx = right_idx\n        self.body_idx = body_idx\n        self.n_joint = n_joint\n        self.feature_dim = feature_dim\n\n        # Tính toán số chiều đầu vào FCN sau pooling, flatten, concat\n        fcn_input_dim = feature_dim * 2 + feature_dim * n_joint\n\n        # FCN phức tạp với nhiều tầng, dropout, relu\n        self.fcn = nn.Sequential(\n            nn.Linear(fcn_input_dim, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, num_classes)\n        )\n\n    def forward(\n        self, keypoint, rgb_imgs, mask_left, mask_right, \n        kp_j_left, kp_j_right, rgb_left_imgs, rgb_right_imgs\n    ):\n        B, T, N, C = keypoint.shape\n\n        # 1. Chia keypoint thành 3 phần\n        kp_body = keypoint[..., :self.body_idx, :]\n        kp_left = keypoint[..., self.body_idx:self.body_idx+self.left_idx, :]\n        kp_right = keypoint[..., self.body_idx+self.left_idx:self.body_idx+self.left_idx+self.right_idx, :]\n\n        # 2. BODY: ST-GCN từng frame --> (B, C_body, Nb); stack lại (B, T, C_body, Nb)\n        body_stgcn_feats = []\n        for t in range(T):\n            x = kp_body[:, t, :, :]\n            _, f = self.stgcn_body(x)\n            body_stgcn_feats.append(f)\n        body_stgcn_feats = torch.stack(body_stgcn_feats, dim=1)\n        body_out = self.temporal_encoder_body(body_stgcn_feats)\n\n        # 3. LEFT HAND: ST-GCN + Fusion, giữ spatial, stack lại (B, T, C_hand, Nl)\n        left_stgcn_feats = []\n        for t in range(T):\n            x = kp_left[:, t, :, :]\n            _, f = self.stgcn_left(x)\n            mask = mask_left[:, t]\n            img = rgb_left_imgs[:, t, :, :, :]\n            ref_j = kp_j_left[:, t, :, :]\n            _, rgb_feat = self.vision_encoder(img, return_feature=True)\n            fused = []\n            for b in range(B):\n                hand_feat_bn = f[b].permute(1, 0)\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        hand_feat_bn.unsqueeze(0), rgb_feat[b:b+1], ref_j[b:b+1]\n                    )\n                    if fused_feat.ndim == 2:\n                        fused_feat = fused_feat.unsqueeze(1).expand(-1, hand_feat_bn.shape[0], -1)\n                    fused.append(fused_feat.squeeze(0).permute(1, 0))\n                else:\n                    fused.append(f[b])\n            left_stgcn_feats.append(torch.stack(fused, dim=0))\n        left_stgcn_feats = torch.stack(left_stgcn_feats, dim=1)\n        left_out = self.temporal_encoder_hand(left_stgcn_feats)\n\n        # 4. RIGHT HAND: ST-GCN + Fusion, giữ spatial, stack lại (B, T, C_hand, Nr)\n        right_stgcn_feats = []\n        for t in range(T):\n            x = kp_right[:, t, :, :]\n            _, f = self.stgcn_right(x)\n            mask = mask_right[:, t]\n            img = rgb_right_imgs[:, t, :, :, :]\n            ref_j = kp_j_right[:, t, :, :]\n            _, rgb_feat = self.vision_encoder(img, return_feature=True)\n            fused = []\n            for b in range(B):\n                hand_feat_bn = f[b].permute(1, 0)\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        hand_feat_bn.unsqueeze(0), rgb_feat[b:b+1], ref_j[b:b+1]\n                    )\n                    if fused_feat.ndim == 2:\n                        fused_feat = fused_feat.unsqueeze(1).expand(-1, hand_feat_bn.shape[0], -1)\n                    fused.append(fused_feat.squeeze(0).permute(1, 0))\n                else:\n                    fused.append(f[b])\n            right_stgcn_feats.append(torch.stack(fused, dim=0))\n        right_stgcn_feats = torch.stack(right_stgcn_feats, dim=1)\n        right_out = self.temporal_encoder_hand(right_stgcn_feats)\n\n        # 5. Ghép feature (B, 256, 67)\n        final_feat = torch.cat([body_out, left_out, right_out], dim=-1)  # (B, 256, 67)\n\n        # Pooling + flatten + concat\n        mean_pool = final_feat.mean(dim=-1)               # (B, 256)\n        max_pool = final_feat.max(dim=-1).values          # (B, 256)\n        flatten = final_feat.view(B, -1)                  # (B, 256*67)\n        concat_feat = torch.cat([mean_pool, max_pool, flatten], dim=-1)  # (B, 256*2 + 256*67)\n\n        logits = self.fcn(concat_feat)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:35.708180Z","iopub.execute_input":"2025-06-10T15:53:35.708749Z","iopub.status.idle":"2025-06-10T15:53:35.723255Z","shell.execute_reply.started":"2025-06-10T15:53:35.708727Z","shell.execute_reply":"2025-06-10T15:53:35.722636Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclass SignDataset(Dataset):\n    def __init__(\n        self, meta_list, transform_crop=None,\n        max_seq=64, crop_shape=(3, 112, 112), kp_num=21\n    ):\n        self.meta_list = meta_list\n        self.transform_crop = transform_crop\n        self.max_seq = max_seq\n        self.crop_shape = crop_shape\n        self.kp_num = kp_num\n        # Pre-allocate zeros for padding\n        self.zero_img = torch.zeros(self.crop_shape, dtype=torch.float32)\n        self.zero_j = torch.zeros(self.kp_num, 2, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.meta_list)\n\n    def pad_seq(self, seq, pad, max_len):\n        # seq: list[tensor], pad: tensor\n        if len(seq) >= max_len:\n            return seq[:max_len]\n        else:\n            return seq + [pad]*(max_len-len(seq))\n\n    def __getitem__(self, idx):\n        meta = self.meta_list[idx]\n        T = meta['num_frames']\n        label = meta['label']\n        keypoint = np.load(meta['keypoint_path'])    # (T, N, 3)\n        mask_left = np.load(meta['mask_left_path'])  # (T,)\n        mask_right = np.load(meta['mask_right_path'])# (T,)\n\n        # Check consistency\n        assert len(meta['crop_left_files']) == int(mask_left.sum()), f\"{meta['video_id']}: crop_left_files/mask_left mismatch\"\n        assert len(meta['crop_right_files']) == int(mask_right.sum()), f\"{meta['video_id']}: crop_right_files/mask_right mismatch\"\n        assert len(meta['kp_j_left_files']) == int(mask_left.sum()), f\"{meta['video_id']}: kp_j_left_files/mask_left mismatch\"\n        assert len(meta['kp_j_right_files']) == int(mask_right.sum()), f\"{meta['video_id']}: kp_j_right_files/mask_right mismatch\"\n\n        crop_left_imgs, kp_j_left = [], []\n        left_idx = 0\n        for t in range(T):\n            if mask_left[t] == 1:\n                img = Image.open(meta['crop_left_files'][left_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = torch.tensor(np.load(meta['kp_j_left_files'][left_idx]), dtype=torch.float32)\n                left_idx += 1\n            else:\n                img = self.zero_img\n                j = self.zero_j\n            crop_left_imgs.append(img)\n            kp_j_left.append(j)\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)   # (T, C, H, W)\n        kp_j_left = torch.stack(kp_j_left, dim=0)             # (T, kp_num, 2)\n\n        crop_right_imgs, kp_j_right = [], []\n        right_idx = 0\n        for t in range(T):\n            if mask_right[t] == 1:\n                img = Image.open(meta['crop_right_files'][right_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = torch.tensor(np.load(meta['kp_j_right_files'][right_idx]), dtype=torch.float32)\n                right_idx += 1\n            else:\n                img = self.zero_img\n                j = self.zero_j\n            crop_right_imgs.append(img)\n            kp_j_right.append(j)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n\n        # Pad/truncate\n        crop_left_imgs = self.pad_seq(list(crop_left_imgs), self.zero_img, self.max_seq)\n        crop_right_imgs = self.pad_seq(list(crop_right_imgs), self.zero_img, self.max_seq)\n        kp_j_left = self.pad_seq(list(kp_j_left), self.zero_j, self.max_seq)\n        kp_j_right = self.pad_seq(list(kp_j_right), self.zero_j, self.max_seq)\n        mask_left = np.pad(mask_left, (0, max(self.max_seq - T, 0)), 'constant')\n        mask_right = np.pad(mask_right, (0, max(self.max_seq - T, 0)), 'constant')\n        keypoint = np.pad(keypoint, ((0, max(self.max_seq - T, 0)), (0,0), (0,0)), 'constant')\n\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_left = torch.stack(kp_j_left, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n        keypoint = torch.tensor(keypoint, dtype=torch.float32)\n        mask_left = torch.tensor(mask_left, dtype=torch.float32)\n        mask_right = torch.tensor(mask_right, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.long)\n\n        if keypoint.shape[0] != self.max_seq:\n            print(f\"[WRONG SHAPE] idx={idx}, video_id={meta['video_id']}, path={meta['keypoint_path']}, shape={keypoint.shape}\")\n\n        return {\n            'keypoint': keypoint,             # (T, N, 3)\n            'mask_left': mask_left,           # (T,)\n            'mask_right': mask_right,         # (T,)\n            'crop_left_imgs': crop_left_imgs, # (T, C, H, W)\n            'crop_right_imgs': crop_right_imgs,\n            'kp_j_left': kp_j_left,           # (T, kp_num, 2)\n            'kp_j_right': kp_j_right,\n            'label': label\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:53:35.724122Z","iopub.execute_input":"2025-06-10T15:53:35.724402Z","iopub.status.idle":"2025-06-10T15:53:35.740585Z","shell.execute_reply.started":"2025-06-10T15:53:35.724373Z","shell.execute_reply":"2025-06-10T15:53:35.739898Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\ntry:\n    torch.cuda.empty_cache()\n    torch.cuda.manual_seed_all(42)\n    print(\"OK\")\nexcept Exception as e:\n    print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:30.722490Z","iopub.execute_input":"2025-06-10T15:56:30.723094Z","iopub.status.idle":"2025-06-10T15:56:31.101984Z","shell.execute_reply.started":"2025-06-10T15:56:30.723064Z","shell.execute_reply":"2025-06-10T15:56:31.101387Z"}},"outputs":[{"name":"stdout","text":"OK\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport os\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef collate_fn(batch):\n    batch_out = {}\n    for k in batch[0].keys():\n        if isinstance(batch[0][k], torch.Tensor):\n            batch_out[k] = torch.stack([item[k] for item in batch], dim=0)\n        else:\n            arr = [item[k] for item in batch]\n            if isinstance(arr[0], (int, float)):\n                batch_out[k] = torch.tensor(arr)\n            else:\n                batch_out[k] = arr\n    return batch_out\n\ndef print_batch_info(batch, batch_idx=None):\n    if batch_idx is not None:\n        print(f\"BATCH ERROR at batch_idx={batch_idx}\")\n    for k in batch:\n        v = batch[k]\n        if isinstance(v, torch.Tensor):\n            print(f\"{k}: shape={v.shape}, dtype={v.dtype}, device={v.device}, min={v.min().item() if v.numel()>0 else 'EMPTY'}, max={v.max().item() if v.numel()>0 else 'EMPTY'}\")\n        elif isinstance(v, list):\n            print(f\"{k}: list of len {len(v)}; first element type: {type(v[0]) if len(v)>0 else 'EMPTY'}\")\n        else:\n            print(f\"{k}: type={type(v)}\")\n\ndef train_epoch(model, loader, optimizer, criterion, device=\"cuda\"):\n    model.train()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch_idx, batch in enumerate(loader):\n        try:\n            print(\"batch['keypoint'].shape:\", batch['keypoint'].shape)\n            for k in batch:\n                if isinstance(batch[k], torch.Tensor):\n                    batch[k] = batch[k].to(device)\n            optimizer.zero_grad()\n            logits = model(\n                keypoint=batch['keypoint'],\n                rgb_imgs=None,\n                mask_left=batch['mask_left'],\n                mask_right=batch['mask_right'],\n                kp_j_left=batch['kp_j_left'],\n                kp_j_right=batch['kp_j_right'],\n                rgb_left_imgs=batch['crop_left_imgs'],\n                rgb_right_imgs=batch['crop_right_imgs']\n            )\n            loss = criterion(logits, batch['label'])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * batch['label'].size(0)\n            pred = logits.argmax(dim=1)\n            total_acc += (pred == batch['label']).sum().item()\n            n += batch['label'].size(0)\n        except Exception as e:\n            print(f\"Exception in train_epoch at batch_idx={batch_idx}: {e}\")\n            print_batch_info(batch, batch_idx)\n            raise\n    return total_loss / n, total_acc / n\n\n@torch.no_grad()\ndef validate_epoch(model, loader, criterion, device=\"cuda\"):\n    model.eval()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch_idx, batch in enumerate(loader):\n        try:\n            for k in batch:\n                if isinstance(batch[k], torch.Tensor):\n                    batch[k] = batch[k].to(device)\n            logits = model(\n                keypoint=batch['keypoint'],\n                rgb_imgs=None,\n                mask_left=batch['mask_left'],\n                mask_right=batch['mask_right'],\n                kp_j_left=batch['kp_j_left'],\n                kp_j_right=batch['kp_j_right'],\n                rgb_left_imgs=batch['crop_left_imgs'],\n                rgb_right_imgs=batch['crop_right_imgs']\n            )\n            loss = criterion(logits, batch['label'])\n            total_loss += loss.item() * batch['label'].size(0)\n            pred = logits.argmax(dim=1)\n            total_acc += (pred == batch['label']).sum().item()\n            n += batch['label'].size(0)\n        except Exception as e:\n            print(f\"Exception in validate_epoch at batch_idx={batch_idx}: {e}\")\n            print_batch_info(batch, batch_idx)\n            raise\n    return total_loss / n, total_acc / n\n\ndef save_checkpoint(model, optimizer, epoch, path, best_acc=None):\n    state = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    if best_acc is not None:\n        state['best_acc'] = best_acc\n    torch.save(state, path)\n\ndef load_checkpoint(model, optimizer, path, device='cuda'):\n    state = torch.load(path, map_location=device)\n    model.load_state_dict(state['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    epoch = state.get('epoch', 0)\n    best_acc = state.get('best_acc', None)\n    return model, optimizer, epoch, best_acc\n\ndef adjust_learning_rate(optimizer, epoch, lr, step=10, decay=0.1):\n    \"\"\" Step LR decay \"\"\"\n    lr_new = lr * (decay ** (epoch // step))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr_new\n    return lr_new","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:33.033877Z","iopub.execute_input":"2025-06-10T15:56:33.034517Z","iopub.status.idle":"2025-06-10T15:56:33.050318Z","shell.execute_reply.started":"2025-06-10T15:56:33.034493Z","shell.execute_reply":"2025-06-10T15:56:33.049692Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import json\n#from data.sign_dataset import SignDataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\n# Load meta_list\nwith open('/kaggle/working/train_meta.json') as f:\n    train_meta_list = json.load(f)\nwith open('/kaggle/working/val_meta.json') as f:\n    val_meta_list = json.load(f)\n\ntrain_ds = SignDataset(meta_list=train_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\nval_ds = SignDataset(meta_list=val_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\n\n#from utils.train_utils import collate_fn\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn, drop_last = True) \nval_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn, drop_last = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:37.172923Z","iopub.execute_input":"2025-06-10T15:56:37.173230Z","iopub.status.idle":"2025-06-10T15:56:37.193672Z","shell.execute_reply.started":"2025-06-10T15:56:37.173206Z","shell.execute_reply":"2025-06-10T15:56:37.192851Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"data_root = \"/kaggle/input/keypoint/keypoints\"\ntrain_txt = \"/kaggle/working/train.txt\"\nval_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\n#batch_size = 32\n#num_workers = 4\n#num_epochs = 10\n#lr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:38.770870Z","iopub.execute_input":"2025-06-10T15:56:38.771681Z","iopub.status.idle":"2025-06-10T15:56:38.776488Z","shell.execute_reply.started":"2025-06-10T15:56:38.771656Z","shell.execute_reply":"2025-06-10T15:56:38.775749Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import torch\nimport torchvision\n#from models.vision import VisionClassifier\n\ndef load_vision_encoder(ckpt_path, num_classes=1000):\n    vision_encoder = VisionClassifier(num_classes=num_classes)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    vision_encoder.load_state_dict(ckpt.get('model_state_dict', ckpt), strict=False)\n    return vision_encoder\n\nvision_encoder = load_vision_encoder('/kaggle/working/pretrained_hand_rgb.pth', num_classes=num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.348436Z","iopub.execute_input":"2025-06-10T15:56:40.349060Z","iopub.status.idle":"2025-06-10T15:56:40.535698Z","shell.execute_reply.started":"2025-06-10T15:56:40.349033Z","shell.execute_reply":"2025-06-10T15:56:40.535118Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"#from models.skeleton import SpatialPoseEncoder\n\ndef load_stgcn(ckpt_path, in_channels, num_joints, A, hid_dim=128, out_dim=256):\n    model = SpatialPoseEncoder(\n        in_channels=in_channels,\n        num_joints=num_joints,\n        num_classes=1,  # Không quan trọng nếu chỉ lấy feature\n        A = A,\n        hid_dim=hid_dim,\n        out_dim=out_dim\n    )\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt.get('model_state_dict', ckpt), strict=False)\n    return model\n\nstgcn_body = load_stgcn('/kaggle/working/spatial_body_best.pth', in_channels=3, num_joints=25, A = get_body_spatial_graph())\nstgcn_left = load_stgcn('/kaggle/working/spatial_left_best.pth', in_channels=3, num_joints=21, A = get_left_hand_spatial_graph())\nstgcn_right = load_stgcn('/kaggle/working/spatial_right_best.pth', in_channels=3, num_joints=21, A = get_right_hand_spatial_graph())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.648163Z","iopub.execute_input":"2025-06-10T15:56:42.648789Z","iopub.status.idle":"2025-06-10T15:56:42.670718Z","shell.execute_reply.started":"2025-06-10T15:56:42.648763Z","shell.execute_reply":"2025-06-10T15:56:42.670182Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"d_model = 256\nmax_seq = 64  # hoặc giá trị max_seq bạn sử dụng\n\ntemporal_encoder_body = TemporalEncoder(c_model=d_model, n_joints = 25, pool='mean')\ntemporal_encoder_hand = TemporalEncoder(c_model=d_model, n_joints = 21, pool='mean')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:44.848352Z","iopub.execute_input":"2025-06-10T15:56:44.848942Z","iopub.status.idle":"2025-06-10T15:56:45.307449Z","shell.execute_reply.started":"2025-06-10T15:56:44.848919Z","shell.execute_reply":"2025-06-10T15:56:45.306645Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"pgf_module = PGFModule(d_model=256, n_heads=8, n_points=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:46.211444Z","iopub.execute_input":"2025-06-10T15:56:46.211738Z","iopub.status.idle":"2025-06-10T15:56:46.218406Z","shell.execute_reply.started":"2025-06-10T15:56:46.211716Z","shell.execute_reply":"2025-06-10T15:56:46.217610Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"model = ClassifySign(\n    vision_encoder = vision_encoder,\n    pgf_module = pgf_module,\n    stgcn_body=stgcn_body,\n    stgcn_left=stgcn_left,\n    stgcn_right=stgcn_right,\n    temporal_encoder_body=temporal_encoder_body,\n    temporal_encoder_hand=temporal_encoder_hand,\n    num_classes=num_classes,  # Sửa cho đúng dataset của bạn\n    # Có thể cần các tham số khác như vision_encoder, pgf_module, left_idx, right_idx,... nếu repo yêu cầu\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:47.291676Z","iopub.execute_input":"2025-06-10T15:56:47.291946Z","iopub.status.idle":"2025-06-10T15:56:47.334753Z","shell.execute_reply.started":"2025-06-10T15:56:47.291926Z","shell.execute_reply":"2025-06-10T15:56:47.334250Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:49.405484Z","iopub.execute_input":"2025-06-10T15:56:49.406043Z","iopub.status.idle":"2025-06-10T15:56:49.409807Z","shell.execute_reply.started":"2025-06-10T15:56:49.406017Z","shell.execute_reply":"2025-06-10T15:56:49.409067Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs.\")\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:51.360558Z","iopub.execute_input":"2025-06-10T15:56:51.361171Z","iopub.status.idle":"2025-06-10T15:56:51.503788Z","shell.execute_reply.started":"2025-06-10T15:56:51.361122Z","shell.execute_reply":"2025-06-10T15:56:51.503181Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"set_seed(42)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n\nnum_epochs = 50\nbest_acc = 0.0\nearlystop_patience = 7\nearlystop_counter = 0\n\nfor epoch in range(num_epochs):\n    adjust_learning_rate(optimizer, epoch, lr=1e-4, step=10, decay=0.1)\n\n    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device=device)\n    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device=device)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n\n    # Lưu checkpoint tốt nhất\n    if val_acc > best_acc:\n        best_acc = val_acc\n        save_checkpoint(model, optimizer, epoch, \"/kaggle/working/best_model.pth\", best_acc)\n        print(\"Saved best model.\")\n        earlystop_counter = 0\n    else:\n        earlystop_counter += 1\n\n    # Early stopping nếu không cải thiện\n    if earlystop_counter >= earlystop_patience:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\nprint(\"Train hoàn tất. Best val acc:\", best_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:54.079847Z","iopub.execute_input":"2025-06-10T15:56:54.080424Z","iopub.status.idle":"2025-06-10T16:01:34.546855Z","shell.execute_reply.started":"2025-06-10T15:56:54.080400Z","shell.execute_reply":"2025-06-10T16:01:34.545610Z"}},"outputs":[{"name":"stdout","text":"batch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nbatch['keypoint'].shape: torch.Size([8, 64, 67, 3])\nException in validate_epoch at batch_idx=0: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_7140/1036480873.py\", line 95, in forward\n    _, rgb_feat = self.vision_encoder(img, return_feature=True)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_7140/3231651763.py\", line 20, in forward\n    feat = self.features(x)                              # (B, 256, H, W)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/efficientnet.py\", line 164, in forward\n    result = self.block(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py\", line 259, in forward\n    scale = self._scale(input)\n            ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py\", line 255, in _scale\n    scale = self.fc2(scale)\n            ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\nRuntimeError: CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\nBATCH ERROR at batch_idx=0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_7140/364441131.py\u001b[0m in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             logits = model(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mkeypoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_7140/1036480873.py\", line 95, in forward\n    _, rgb_feat = self.vision_encoder(img, return_feature=True)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_7140/3231651763.py\", line 20, in forward\n    feat = self.features(x)                              # (B, 256, H, W)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/efficientnet.py\", line 164, in forward\n    result = self.block(input)\n             ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py\", line 259, in forward\n    scale = self._scale(input)\n            ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torchvision/ops/misc.py\", line 255, in _scale\n    scale = self.fc2(scale)\n            ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n    return F.conv2d(\n           ^^^^^^^^^\nRuntimeError: CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_7140/598567053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_7140/364441131.py\u001b[0m in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exception in validate_epoch at batch_idx={batch_idx}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mprint_batch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_7140/364441131.py\u001b[0m in \u001b[0;36mprint_batch_info\u001b[0;34m(batch, batch_idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{k}: shape={v.shape}, dtype={v.dtype}, device={v.device}, min={v.min().item() if v.numel()>0 else 'EMPTY'}, max={v.max().item() if v.numel()>0 else 'EMPTY'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{k}: list of len {len(v)}; first element type: {type(v[0]) if len(v)>0 else 'EMPTY'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: misaligned address\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}