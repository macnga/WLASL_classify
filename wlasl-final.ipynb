{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11964981,"sourceType":"datasetVersion","datasetId":7523697},{"sourceId":11977607,"sourceType":"datasetVersion","datasetId":7532459}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.  All classes","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom einops import rearrange\nimport torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:18.939336Z","iopub.execute_input":"2025-06-03T14:15:18.939636Z","iopub.status.idle":"2025-06-03T14:15:18.944507Z","shell.execute_reply.started":"2025-06-03T14:15:18.939604Z","shell.execute_reply":"2025-06-03T14:15:18.943525Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --------- Graph and ST-GCN -------------\nclass Graph:\n    \"\"\"Graph for skeleton representation (e.g., Mediapipe keypoints).\"\"\"\n    def __init__(self, layout='body', strategy='uniform', max_hop=1, dilation=1):\n        self.max_hop = max_hop\n        self.dilation = dilation\n        self.get_edge(layout)\n        self.hop_dis = self.get_hop_distance(self.num_node, self.edge, max_hop=max_hop)\n        self.A = self.get_adjacency(strategy)\n\n    def get_edge(self, layout):\n        if layout == 'body':\n            self.num_node = 25\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [\n                [0, 1],[1, 2],[2, 3],[3, 7],\n                [0, 4],[4, 5],[5, 6],[6, 8],\n                [9, 10],[11, 12],[11, 13],[13, 15],[15, 21],[15, 19],[15, 17],\n                [17, 19],[11, 23],[12, 14],[14, 16],[16, 18],[16, 20],[16, 22],\n                [18, 20],[12, 24],[23, 24]\n            ]\n            neighbor_link = neighbor_1base\n            self.edge = self_link + neighbor_link\n            self.center = 0\n        elif layout == 'left' or layout == 'right':\n            self.num_node = 21\n            self_link = [(i, i) for i in range(self.num_node)]\n            neighbor_1base = [\n                [0, 1],[1, 2],[2, 3],[3, 4],\n                [0, 5],[5, 6],[6, 7],[7, 8],\n                [0, 9],[9, 10],[10, 11],[11, 12],\n                [0, 13],[13, 14],[14, 15],[15, 16],\n                [0, 17],[17, 18],[18, 19],[19, 20]\n            ]\n            neighbor_link = neighbor_1base\n            self.edge = self_link + neighbor_link\n            self.center = 0\n        else:\n            raise ValueError(\"Unknown layout: {}\".format(layout))\n\n    def get_hop_distance(self, num_node, edge, max_hop=1):\n        A = np.zeros((num_node, num_node))\n        for i, j in edge:\n            A[j, i] = 1\n            A[i, j] = 1\n        hop_dis = np.zeros((num_node, num_node)) + np.inf\n        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n        arrive_mat = np.stack(transfer_mat) > 0\n        for d in range(max_hop, -1, -1):\n            hop_dis[arrive_mat[d]] = d\n        return hop_dis\n\n    def normalize_digraph(self, A):\n        Dl = np.sum(A, 0)\n        num_node = A.shape[0]\n        Dn = np.zeros((num_node, num_node))\n        for i in range(num_node):\n            if Dl[i] > 0:\n                Dn[i, i] = Dl[i] ** (-1)\n        AD = np.dot(A, Dn)\n        return AD\n\n    def get_adjacency(self, strategy):\n        valid_hop = range(0, self.max_hop + 1, self.dilation)\n        adjacency = np.zeros((self.num_node, self.num_node))\n        for hop in valid_hop:\n            adjacency[self.hop_dis == hop] = 1\n        normalize_adjacency = self.normalize_digraph(adjacency)\n        if strategy == 'uniform':\n            A = np.zeros((1, self.num_node, self.num_node))\n            A[0] = normalize_adjacency\n        else:\n            raise NotImplementedError\n        return torch.tensor(A, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:18.956966Z","iopub.execute_input":"2025-06-03T14:15:18.957287Z","iopub.status.idle":"2025-06-03T14:15:19.007165Z","shell.execute_reply.started":"2025-06-03T14:15:18.957263Z","shell.execute_reply":"2025-06-03T14:15:19.006543Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class STGCNBlock(nn.Module):\n    \"\"\"A single ST-GCN block (spatio-temporal).\"\"\"\n    def __init__(self, in_channels, out_channels, A, kernel_size=1, stride=1, dropout=0, residual=True):\n        super().__init__()\n        self.A = A\n        self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        self.tcn = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, (9, 1), (stride, 1), (4, 0)),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout(dropout, inplace=True)\n        )\n        if not residual:\n            self.residual = lambda x: 0\n        elif (in_channels == out_channels) and (stride == 1):\n            self.residual = lambda x: x\n        else:\n            self.residual = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)),\n                nn.BatchNorm2d(out_channels)\n            )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        # x: (B, C, T, N)\n        y = self.gcn(x)\n        y = torch.einsum('nctv,kvw->nctw', (y, self.A))\n        y = self.tcn(y)\n        y = y + self.residual(x)\n        return self.relu(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.008504Z","iopub.execute_input":"2025-06-03T14:15:19.008803Z","iopub.status.idle":"2025-06-03T14:15:19.026837Z","shell.execute_reply.started":"2025-06-03T14:15:19.008775Z","shell.execute_reply":"2025-06-03T14:15:19.026196Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PoseEncoder(nn.Module):\n    \"\"\"Pose encoder with 3 stacked ST-GCN blocks.\"\"\"\n    def __init__(self, A, in_dim=3, hid_dim=64, out_dim=256):\n        super().__init__()\n        self.input_proj = nn.Linear(in_dim, hid_dim)\n        self.stgcn1 = STGCNBlock(hid_dim, 128, A)\n        self.stgcn2 = STGCNBlock(128, 256, A)\n        self.stgcn3 = STGCNBlock(256, out_dim, A)\n    def forward(self, x):\n        # x: (B, T, N, 3)\n        x = self.input_proj(x)           # (B, T, N, hid_dim)\n        x = x.permute(0, 3, 1, 2)        # (B, hid_dim, T, N)\n        x = self.stgcn1(x)\n        x = self.stgcn2(x)\n        x = self.stgcn3(x)\n        x = x.mean(2)                    # (B, out_dim, N), mean over time\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.027561Z","iopub.execute_input":"2025-06-03T14:15:19.027768Z","iopub.status.idle":"2025-06-03T14:15:19.039678Z","shell.execute_reply.started":"2025-06-03T14:15:19.027750Z","shell.execute_reply":"2025-06-03T14:15:19.038997Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --------- Deformable Attention (Fusion) -------------\nclass DeformableAttention2D(nn.Module):\n    \"\"\"Deformable attention for pose-vision fusion.\"\"\"\n    def __init__(self, dim, dim_head=64, heads=8, offset_groups=8):\n        super().__init__()\n        self.heads = heads\n        self.dim_head = dim_head\n        self.scale = dim_head ** -0.5\n        self.offset_groups = offset_groups\n        inner_dim = dim_head * heads\n        self.to_q = nn.Conv1d(dim, inner_dim, 1, bias=False)\n        self.to_kv = nn.Conv2d(dim, inner_dim * 2, 1, bias=False)\n        self.to_offsets = nn.Sequential(\n            nn.Conv1d(dim, dim, 1),\n            nn.ReLU(),\n            nn.Conv1d(dim, offset_groups * 2, 1)\n        )\n        self.proj = nn.Conv1d(inner_dim, dim, 1)\n\n    def forward(self, query_feat, context_feat, ref_points):\n        # query_feat: (B, C, N), context_feat: (B, C, H, W), ref_points: (B, N, 2) normalized [0,1]\n        B, C, N = query_feat.shape\n        _, _, H, W = context_feat.shape\n        q = self.to_q(query_feat)\n        q = q.view(B, self.heads, self.dim_head, N).permute(0, 1, 3, 2)  # (B, heads, N, dim_head)\n        kv = self.to_kv(context_feat)\n        kv = kv.view(B, 2, self.heads, self.dim_head, H, W)\n        k, v = kv[:, 0], kv[:, 1]\n        offsets = self.to_offsets(query_feat).view(B, self.offset_groups, 2, N).permute(0, 1, 3, 2)  # (B, G, N, 2)\n        ref_points = ref_points.unsqueeze(1).repeat(1, self.offset_groups, 1, 1)  # (B, G, N, 2)\n        coords = ref_points + offsets / torch.tensor([W, H], device=query_feat.device)\n        coords = coords.clamp(0, 1)\n        coords = coords.view(B, self.offset_groups * N, 2)\n        coords = coords * 2 - 1\n        coords = coords.view(B, 1, self.offset_groups * N, 1, 2)\n        k = k.view(B * self.heads, self.dim_head, H, W)\n        v = v.view(B * self.heads, self.dim_head, H, W)\n        k_sampled = F.grid_sample(k, coords.expand(-1, self.dim_head, -1, -1, -1), align_corners=True)\n        v_sampled = F.grid_sample(v, coords.expand(-1, self.dim_head, -1, -1, -1), align_corners=True)\n        k_sampled = k_sampled.squeeze(-1).view(B, self.heads, self.dim_head, N, self.offset_groups)\n        v_sampled = v_sampled.squeeze(-1).view(B, self.heads, self.dim_head, N, self.offset_groups)\n        q = q.unsqueeze(-1)  # (B, heads, N, dim_head, 1)\n        attn = (q * k_sampled).sum(3) * self.scale  # (B, heads, N, G)\n        attn = F.softmax(attn, dim=-1)\n        out = (attn.unsqueeze(3) * v_sampled).sum(-1)  # (B, heads, dim_head, N)\n        out = out.permute(0, 1, 3, 2).contiguous().view(B, -1, N)\n        out = self.proj(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.089481Z","iopub.execute_input":"2025-06-03T14:15:19.089779Z","iopub.status.idle":"2025-06-03T14:15:19.101538Z","shell.execute_reply.started":"2025-06-03T14:15:19.089744Z","shell.execute_reply":"2025-06-03T14:15:19.100752Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class PGFModule(nn.Module):\n    \"\"\"Pose-guided fusion of pose and vision feature.\"\"\"\n    def __init__(self, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.gater = nn.Sequential(\n            nn.Conv1d(dim * 2, dim, 1),\n            nn.ReLU(),\n            nn.Conv1d(dim, dim, 1),\n            nn.Sigmoid()\n        )\n        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n        self.deform_attn = DeformableAttention2D(dim, dim_head, heads, offset_groups=heads)\n\n    def forward(self, pose_feat, vision_feat, J):\n        # pose_feat: (B, C, N), vision_feat: (B, C, H, W), J: (B, N, 2)\n        B, C, N = pose_feat.shape\n        Fp = pose_feat\n        Fr = vision_feat\n        Fp_ = Fp.permute(0, 2, 1)  # (B, N, C)\n        Fr_ = Fr.flatten(2).permute(0, 2, 1)  # (B, HW, C)\n        attn_output, _ = self.self_attn(Fp_, Fr_, Fr_)\n        attn_output = attn_output.permute(0, 2, 1)  # (B, C, N)\n        gater_input = torch.cat([Fp, attn_output], dim=1)\n        gate = self.gater(gater_input)\n        Fp_fused = Fp * gate + attn_output * (1 - gate)\n        fused = self.deform_attn(Fp_fused, Fr, J)\n        return fused","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.102845Z","iopub.execute_input":"2025-06-03T14:15:19.103111Z","iopub.status.idle":"2025-06-03T14:15:19.123572Z","shell.execute_reply.started":"2025-06-03T14:15:19.103082Z","shell.execute_reply":"2025-06-03T14:15:19.122714Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --------- Vision Encoder -------------\nclass VisionEncoder(nn.Module):\n    def __init__(self, out_dim=256):\n        super().__init__()\n        backbone = torchvision.models.efficientnet_b0(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-2])\n        self.proj = nn.Conv2d(1280, out_dim, 1)\n    def forward(self, x):\n        # x: (B, 3, H, W)\n        feat = self.feature_extractor(x)  # (B, 1280, H', W')\n        return self.proj(feat)            # (B, out_dim, H', W')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.124856Z","iopub.execute_input":"2025-06-03T14:15:19.125162Z","iopub.status.idle":"2025-06-03T14:15:19.138835Z","shell.execute_reply.started":"2025-06-03T14:15:19.125140Z","shell.execute_reply":"2025-06-03T14:15:19.138234Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --------- Dataset Example -------------\nclass SignKeypointDataset(Dataset):\n    \"\"\"Dataset returning (keypoint, label) with file check.\"\"\"\n    def __init__(self, data_root, video_list_txt, part='body', label_map=None):\n        self.samples = []\n        with open(video_list_txt, 'r') as f:\n            for line in f:\n                items = line.strip().split()\n                if len(items) == 2:\n                    vid, label = items\n                    path = os.path.join(data_root, vid, \"keypoints.npy\")\n                    if os.path.exists(path):\n                        label_idx = label_map[label] if label_map else int(label)\n                        self.samples.append((path, label_idx))\n        self.part = part\n\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        kp_path, label = self.samples[idx]\n        keypoints = np.load(kp_path)  # (T, 67, 3)\n        if self.part == 'body':\n            part_kp = keypoints[:, :25, :]\n        elif self.part == 'left':\n            part_kp = keypoints[:, 25:46, :]\n        else:\n            part_kp = keypoints[:, 46:, :]\n        return torch.tensor(part_kp, dtype=torch.float32), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.139512Z","iopub.execute_input":"2025-06-03T14:15:19.139719Z","iopub.status.idle":"2025-06-03T14:15:19.158951Z","shell.execute_reply.started":"2025-06-03T14:15:19.139703Z","shell.execute_reply":"2025-06-03T14:15:19.158315Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# 2. Pretrain Vision encoder ","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport torchvision\nfrom tqdm import tqdm\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.160529Z","iopub.execute_input":"2025-06-03T14:15:19.160864Z","iopub.status.idle":"2025-06-03T14:15:19.174116Z","shell.execute_reply.started":"2025-06-03T14:15:19.160845Z","shell.execute_reply":"2025-06-03T14:15:19.173467Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def build_label_map(txt_files):\n    labels = set()\n    for txt in txt_files:\n        with open(txt, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) >= 2:\n                    label = ' '.join(parts[1:]).strip()\n                    labels.add(label)\n    labels = sorted(labels)\n    label_map = {lbl: idx for idx, lbl in enumerate(labels)}\n    return label_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.174847Z","iopub.execute_input":"2025-06-03T14:15:19.175106Z","iopub.status.idle":"2025-06-03T14:15:19.188354Z","shell.execute_reply.started":"2025-06-03T14:15:19.175089Z","shell.execute_reply":"2025-06-03T14:15:19.187454Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data_root = \"/kaggle/input/data-wlasl/DATA\"\ntrain_txt = \"/kaggle/input/train-test-valid/train.txt\"\nvalid_txt = \"/kaggle/input/train-test-valid/val.txt\"\ntest_txt = \"/kaggle/input/train-test-valid/test.txt\"\nbatch_size = 64\nnum_workers = 4\nnum_epochs = 6\nlr = 1e-3\nout_ckpt = \"/kaggle/working/pretrained_hand_rgb.pth\"\nout_labelmap = \"/kaggle/working/label_map.json\"\n\nlabel_map = build_label_map([train_txt, valid_txt, test_txt])\nnum_classes = len(label_map)\nprint(\"Số lớp:\", num_classes)\nprint(\"Sample label_map:\", dict(list(label_map.items())[:5]))\n\nwith open(out_labelmap, \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.189222Z","iopub.execute_input":"2025-06-03T14:15:19.189522Z","iopub.status.idle":"2025-06-03T14:15:19.243445Z","shell.execute_reply.started":"2025-06-03T14:15:19.189502Z","shell.execute_reply":"2025-06-03T14:15:19.242623Z"}},"outputs":[{"name":"stdout","text":"Số lớp: 355\nSample label_map: {'accept': 0, 'accident': 1, 'add': 2, 'africa': 3, 'again': 4}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==== 2. Dataset for cropped hand images ====\nclass HandImageDataset(Dataset):\n    def __init__(self, data_root, list_file, label_map, transform=None):\n        self.samples = []\n        self.transform = transform\n        with open(list_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    video_id, label = parts\n                    label_idx = label_map[label]\n                    img_dir = os.path.join(data_root, video_id)\n                    # All *_left.jpg and *_right.jpg (can add filter for frame sampling if needed)\n                    imgs = sorted(glob(os.path.join(img_dir, \"*_left.jpg\"))) + \\\n                           sorted(glob(os.path.join(img_dir, \"*_right.jpg\")))\n                    for img_path in imgs:\n                        if os.path.isfile(img_path):\n                            self.samples.append((img_path, label_idx))\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.244424Z","iopub.execute_input":"2025-06-03T14:15:19.244661Z","iopub.status.idle":"2025-06-03T14:15:19.251555Z","shell.execute_reply.started":"2025-06-03T14:15:19.244642Z","shell.execute_reply":"2025-06-03T14:15:19.250498Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\ntrain_ds = HandImageDataset(data_root, train_txt, label_map, transform=transform)\nval_ds = HandImageDataset(data_root, valid_txt, label_map, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\nprint(f\"Số ảnh train: {len(train_ds)}, Số ảnh val: {len(val_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:15:19.252380Z","iopub.execute_input":"2025-06-03T14:15:19.252574Z","iopub.status.idle":"2025-06-03T14:16:06.517010Z","shell.execute_reply.started":"2025-06-03T14:15:19.252558Z","shell.execute_reply":"2025-06-03T14:16:06.516319Z"}},"outputs":[{"name":"stdout","text":"Số ảnh train: 12891, Số ảnh val: 2555\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ==== 3. Vision Model ====\nclass VisionClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        backbone = torchvision.models.efficientnet_b0(pretrained=True)\n        self.features = backbone.features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(1280, num_classes)\n    def forward(self, x):\n        feat = self.features(x)\n        feat = self.pool(feat).view(x.size(0), -1)\n        return self.classifier(feat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:16:06.518987Z","iopub.execute_input":"2025-06-03T14:16:06.519225Z","iopub.status.idle":"2025-06-03T14:16:06.524622Z","shell.execute_reply.started":"2025-06-03T14:16:06.519167Z","shell.execute_reply":"2025-06-03T14:16:06.523839Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = VisionClassifier(num_classes)\nif torch.cuda.device_count() > 1:\n    print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n    model = nn.DataParallel(model)\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:16:06.525512Z","iopub.execute_input":"2025-06-03T14:16:06.525748Z","iopub.status.idle":"2025-06-03T14:16:07.310728Z","shell.execute_reply.started":"2025-06-03T14:16:06.525721Z","shell.execute_reply":"2025-06-03T14:16:07.310019Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 136MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Using DataParallel with 2 GPUs\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"best_val_acc = 0\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for imgs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\", leave=False):\n        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n    train_acc = correct / total\n    train_loss = total_loss / total\n\n    # Validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=\"Valid\", leave=False):\n            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += imgs.size(0)\n    val_acc = val_correct / val_total\n    val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n    # Save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model\": model.state_dict(),\n            \"label_map\": label_map\n        }, out_ckpt)\n        print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\nprint(\"Done. Best val acc:\", best_val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:17:09.894404Z","iopub.execute_input":"2025-06-03T14:17:09.895046Z","iopub.status.idle":"2025-06-03T14:21:56.443462Z","shell.execute_reply.started":"2025-06-03T14:17:09.895023Z","shell.execute_reply":"2025-06-03T14:21:56.442541Z"}},"outputs":[{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 5.2582 Acc: 0.0600 | Val Loss: 5.5364 Acc: 0.0556\nBest model saved at epoch 1, val_acc=0.0556\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/6 | Train Loss: 3.8517 Acc: 0.2129 | Val Loss: 5.3591 Acc: 0.0779\nBest model saved at epoch 2, val_acc=0.0779\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/6 | Train Loss: 2.9059 Acc: 0.3547 | Val Loss: 5.6515 Acc: 0.0916\nBest model saved at epoch 3, val_acc=0.0916\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/6 | Train Loss: 2.1887 Acc: 0.4837 | Val Loss: 5.9073 Acc: 0.0939\nBest model saved at epoch 4, val_acc=0.0939\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/6 | Train Loss: 1.6259 Acc: 0.6020 | Val Loss: 6.6308 Acc: 0.0935\n","output_type":"stream"},{"name":"stderr","text":"                                                                ","output_type":"stream"},{"name":"stdout","text":"Epoch 6/6 | Train Loss: 1.1757 Acc: 0.7018 | Val Loss: 6.6481 Acc: 0.0986\nBest model saved at epoch 6, val_acc=0.0986\nDone. Best val acc: 0.09863013698630137\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# 3. Pretrain Pose encoder với Spatial GCN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass PoseSpatialPartDataset(Dataset):\n    def __init__(self, data_root, txt_file, label_map, part='body'):\n        self.samples = []\n        self.label_map = label_map\n        self.part = part\n        with open(txt_file) as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    print(f\"WARNING: dòng bị lỗi format: {line}\")\n                    continue\n                npy_id = parts[0]\n                label = ' '.join(parts[1:]).strip()\n                if label not in label_map:\n                    print(f\"WARNING: label '{label}' chưa có trong label_map!\")\n                    continue\n                npy_path = f\"{data_root}/{npy_id}/keypoints.npy\"\n                self.samples.append((npy_path, int(label_map[label])))\n        # Define keypoint slices for each part\n        if part == 'body':\n            self.idx_start, self.idx_end = 0, 25\n        elif part == 'left':\n            self.idx_start, self.idx_end = 25, 46\n        elif part == 'right':\n            self.idx_start, self.idx_end = 46, 67\n        else:\n            raise ValueError(\"Unknown part: \" + part)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        npy_path, label = self.samples[idx]\n        keypoints = np.load(npy_path)  # (T, 67, 3) expected\n        # Auto fix shape if needed\n        if keypoints.shape[-2:] == (67, 3):\n            pass\n        elif keypoints.shape[0] == 67 and keypoints.shape[1] == 3:\n            keypoints = np.transpose(keypoints, (2, 0, 1))\n        elif keypoints.shape[1] == 3 and keypoints.shape[2] == 67:\n            keypoints = np.transpose(keypoints, (0, 2, 1))\n        else:\n            raise RuntimeError(f\"Unrecognized keypoints shape: {keypoints.shape}\")\n        part_kp = keypoints[:, self.idx_start:self.idx_end, :]  # (T, N, 3)\n        if idx == 0:\n            print(f\"Dataset part_kp.shape: {part_kp.shape}\")\n        return torch.tensor(part_kp, dtype=torch.float32), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:34:23.464999Z","iopub.execute_input":"2025-06-03T14:34:23.465591Z","iopub.status.idle":"2025-06-03T14:34:23.475639Z","shell.execute_reply.started":"2025-06-03T14:34:23.465566Z","shell.execute_reply":"2025-06-03T14:34:23.474701Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class SpatialGCNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, A):\n        super().__init__()\n        self.register_buffer('A', A)\n        self.fc = nn.Linear(in_channels, out_channels)\n        self.bn = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):  # x: (B, N, in_channels)\n        h = self.fc(x)  # (B, N, out_channels)\n        h = h.permute(0, 2, 1)  # (B, out_channels, N)\n        h = torch.matmul(h, self.A)  # (B, out_channels, N)\n        h = self.bn(h)  # BatchNorm trên out_channels\n        h = h.permute(0, 2, 1)  # (B, N, out_channels)\n        return torch.relu(h)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:16.326697Z","iopub.execute_input":"2025-06-03T14:22:16.327329Z","iopub.status.idle":"2025-06-03T14:22:16.332649Z","shell.execute_reply.started":"2025-06-03T14:22:16.327303Z","shell.execute_reply":"2025-06-03T14:22:16.331754Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class SpatialPoseEncoder(nn.Module):\n    def __init__(self, in_channels, num_joints, num_classes, A, hid_dim=128, out_dim=256):\n        super().__init__()\n        self.gcn1 = SpatialGCNLayer(in_channels, hid_dim, A)\n        self.gcn2 = SpatialGCNLayer(hid_dim, out_dim, A)\n        self.classifier = nn.Linear(out_dim * num_joints, num_classes)\n\n    def forward(self, x):  # x: (B, T, N, 3)\n        #print(\"Encoder x.shape:\", x.shape)\n        B, T, N, C = x.shape\n        assert N == self.gcn1.A.shape[0], f\"x.shape={x.shape}, A.shape={self.gcn1.A.shape}\"\n        x = x.view(B * T, N, C)  # (B*T, N, 3)\n        h = self.gcn1(x)         # (B*T, N, hid_dim)\n        h = self.gcn2(h)         # (B*T, N, out_dim)\n        h = h.view(B, T, N, -1)  # (B, T, N, out_dim)\n        h = h.mean(1)            # (B, N, out_dim)\n        h = h.permute(0, 2, 1)   # (B, out_dim, N)\n        logits = self.classifier(h.flatten(1))  # (B, num_classes)\n        return logits, h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:35:20.168138Z","iopub.execute_input":"2025-06-03T14:35:20.168774Z","iopub.status.idle":"2025-06-03T14:35:20.175213Z","shell.execute_reply.started":"2025-06-03T14:35:20.168750Z","shell.execute_reply":"2025-06-03T14:35:20.174414Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ---- Example: Fusion lấy feature frame đặc biệt ----\ndef gather_special_frames(pose_feat, mask_indices):\n    \"\"\"\n    pose_feat: (B, C, N, T)\n    mask_indices: list of [tensor(F_b,), ...]  # F_b: số frame của mỗi sample cần fusion\n    Return: list of (B, C, N, F_b)\n    \"\"\"\n    outputs = []\n    for b, idxs in enumerate(mask_indices):\n        # idxs: (F_b,), pose_feat[b]: (C, N, T)\n        sel = pose_feat[b, :, :, idxs]  # (C, N, F_b)\n        outputs.append(sel)\n    return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:20.770735Z","iopub.execute_input":"2025-06-03T14:22:20.771591Z","iopub.status.idle":"2025-06-03T14:22:20.776155Z","shell.execute_reply.started":"2025-06-03T14:22:20.771555Z","shell.execute_reply":"2025-06-03T14:22:20.775372Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def get_spatial_adjacency(num_node, edge):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[i, j] = 1\n        A[j, i] = 1\n    # Normalize\n    Dl = np.sum(A, 0)\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i] ** (-1)\n    A_normalized = np.dot(A, Dn)\n    return torch.tensor(A_normalized, dtype=torch.float32)\n\ndef get_body_spatial_graph():\n    # 25 body keypoints (Mediapipe hoặc OpenPose định nghĩa)\n    num_node = 25\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1), (1, 2), (2, 3), (3, 7),\n        (0, 4), (4, 5), (5, 6), (6, 8),\n        (9, 10), (11, 12), (11, 13), (13, 15), (15, 21), (15, 19), (15, 17),\n        (17, 19), (11, 23), (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),\n        (18, 20), (12, 24), (23, 24)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\n\ndef get_left_hand_spatial_graph():\n    # 21 left hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\n\ndef get_right_hand_spatial_graph():\n    # 21 right hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:21.025063Z","iopub.execute_input":"2025-06-03T14:22:21.025837Z","iopub.status.idle":"2025-06-03T14:22:21.036564Z","shell.execute_reply.started":"2025-06-03T14:22:21.025807Z","shell.execute_reply":"2025-06-03T14:22:21.035876Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ========= 2. Dataset =========\nPART_INFO = {\n    'body':  (0, 25),\n    'left':  (25, 46),\n    'right': (46, 67)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:23.758387Z","iopub.execute_input":"2025-06-03T14:22:23.758689Z","iopub.status.idle":"2025-06-03T14:22:23.762740Z","shell.execute_reply.started":"2025-06-03T14:22:23.758666Z","shell.execute_reply":"2025-06-03T14:22:23.762074Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef train_pose_spatial_part(part, num_joints, get_A_func, best_ckpt_file):\n    print(f\"\\n--- Pretraining {part} ---\")\n    train_ds = PoseSpatialPartDataset(data_root, train_txt, label_map, part=part)\n    val_ds = PoseSpatialPartDataset(data_root, val_txt, label_map, part=part)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    print(f\"Số mẫu train: {len(train_ds)}, val: {len(val_ds)}\")\n    print(f\"Số batch train: {len(train_loader)}, val: {len(val_loader)}\")\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    A = get_A_func().to(device)\n    model = SpatialPoseEncoder(in_channels=3, num_joints=num_joints, num_classes=num_classes, A=A)\n    if torch.cuda.device_count() > 1:\n        print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    best_val_acc = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for x, labels in tqdm(train_loader, desc=f\"Train {part} Epoch {epoch+1}\", leave=False):\n            #print(\"Batch x.shape:\", x.shape)\n            x, labels = x.to(device), labels.to(device)\n            logits, _ = model(x)\n            loss = criterion(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * x.size(0)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += x.size(0)\n        if total > 0:\n            train_acc = correct / total\n            train_loss = total_loss / total\n        else:\n            train_acc = 0\n            train_loss = 0\n            print(\"WARNING: Không có sample nào trong batch train!\")\n\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for x, labels in tqdm(val_loader, desc=f\"Val {part} Epoch {epoch+1}\", leave=False):\n                x, labels = x.to(device), labels.to(device)\n                logits, _ = model(x)\n                loss = criterion(logits, labels)\n                val_loss += loss.item() * x.size(0)\n                preds = logits.argmax(1)\n                val_correct += (preds == labels).sum().item()\n                val_total += x.size(0)\n        if val_total > 0:\n            val_acc = val_correct / val_total\n            val_loss = val_loss / val_total\n        else:\n            val_acc = 0\n            val_loss = 0\n            print(\"WARNING: Không có sample nào trong batch val!\")\n\n        print(f\"[{part}] Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\"model\": model.state_dict(), \"label_map\": label_map}, best_ckpt_file)\n            print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\n    print(f\"Done {part}. Best val acc: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:43:39.626973Z","iopub.execute_input":"2025-06-03T14:43:39.627855Z","iopub.status.idle":"2025-06-03T14:43:39.642872Z","shell.execute_reply.started":"2025-06-03T14:43:39.627822Z","shell.execute_reply":"2025-06-03T14:43:39.642097Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"data_root = \"/kaggle/input/data-wlasl/DATA\"\ntrain_txt = \"/kaggle/input/train-test-valid/train.txt\"\nval_txt = \"/kaggle/input/train-test-valid/val.txt\"\ntest_txt = \"/kaggle/input/train-test-valid/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 10\nlr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)\nwith open(\"label_map_pose.json\", \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:48:53.885256Z","iopub.execute_input":"2025-06-03T14:48:53.886073Z","iopub.status.idle":"2025-06-03T14:48:53.900301Z","shell.execute_reply.started":"2025-06-03T14:48:53.886041Z","shell.execute_reply":"2025-06-03T14:48:53.899371Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"len(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:46.348640Z","iopub.execute_input":"2025-06-03T14:22:46.349430Z","iopub.status.idle":"2025-06-03T14:22:46.355737Z","shell.execute_reply.started":"2025-06-03T14:22:46.349401Z","shell.execute_reply":"2025-06-03T14:22:46.354811Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"355"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(get_body_spatial_graph().shape)       # (25, 25)\nprint(get_left_hand_spatial_graph().shape)  # (21, 21)\nprint(get_right_hand_spatial_graph().shape) # (21, 21)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:22:48.458508Z","iopub.execute_input":"2025-06-03T14:22:48.459074Z","iopub.status.idle":"2025-06-03T14:22:48.507732Z","shell.execute_reply.started":"2025-06-03T14:22:48.459048Z","shell.execute_reply":"2025-06-03T14:22:48.507045Z"}},"outputs":[{"name":"stdout","text":"torch.Size([25, 25])\ntorch.Size([21, 21])\ntorch.Size([21, 21])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"train_pose_spatial_part('body',  num_joints=25, get_A_func=get_body_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_body_best.pth\")\ntrain_pose_spatial_part('left',  num_joints=21, get_A_func=get_left_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_left_best.pth\")\ntrain_pose_spatial_part('right', num_joints=21, get_A_func=get_right_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_right_best.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T14:49:01.988287Z","iopub.execute_input":"2025-06-03T14:49:01.988846Z","iopub.status.idle":"2025-06-03T14:49:57.530054Z","shell.execute_reply.started":"2025-06-03T14:49:01.988816Z","shell.execute_reply":"2025-06-03T14:49:57.529033Z"}},"outputs":[{"name":"stdout","text":"\n--- Pretraining body ---\nSố mẫu train: 2340, val: 464\nSố batch train: 74, val: 15\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 1:  30%|██▉       | 22/74 [00:00<00:01, 30.96it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 1/10 | Train Loss: 8.1367 Acc: 0.0060 | Val Loss: 6.6547 Acc: 0.0172\nBest model saved at epoch 1, val_acc=0.0172\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 2:  64%|██████▎   | 47/74 [00:00<00:00, 61.15it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 2/10 | Train Loss: 6.0157 Acc: 0.0248 | Val Loss: 6.6470 Acc: 0.0194\nBest model saved at epoch 2, val_acc=0.0194\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 3:  76%|███████▌  | 56/74 [00:01<00:00, 61.73it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 3/10 | Train Loss: 5.3881 Acc: 0.0466 | Val Loss: 6.0784 Acc: 0.0302\nBest model saved at epoch 3, val_acc=0.0302\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 4:  68%|██████▊   | 50/74 [00:01<00:00, 56.97it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 4/10 | Train Loss: 4.8871 Acc: 0.0714 | Val Loss: 6.0456 Acc: 0.0366\nBest model saved at epoch 4, val_acc=0.0366\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 5:  46%|████▌     | 34/74 [00:00<00:00, 56.39it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 5/10 | Train Loss: 4.5587 Acc: 0.0902 | Val Loss: 5.8852 Acc: 0.0237\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 6:  66%|██████▌   | 49/74 [00:00<00:00, 62.00it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 6/10 | Train Loss: 4.2505 Acc: 0.1179 | Val Loss: 5.8830 Acc: 0.0496\nBest model saved at epoch 6, val_acc=0.0496\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 7:  27%|██▋       | 20/74 [00:00<00:01, 48.51it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 7/10 | Train Loss: 4.0637 Acc: 0.1261 | Val Loss: 5.8756 Acc: 0.0431\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 8:  19%|█▉        | 14/74 [00:00<00:01, 41.46it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 8/10 | Train Loss: 3.9039 Acc: 0.1419 | Val Loss: 5.8607 Acc: 0.0496\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 9:   8%|▊         | 6/74 [00:00<00:02, 23.01it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 9/10 | Train Loss: 3.7417 Acc: 0.1462 | Val Loss: 5.8270 Acc: 0.0539\nBest model saved at epoch 9, val_acc=0.0539\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 10:  86%|████████▋ | 64/74 [00:01<00:00, 63.92it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 10/10 | Train Loss: 3.6070 Acc: 0.1744 | Val Loss: 5.9243 Acc: 0.0582\nBest model saved at epoch 10, val_acc=0.0582\nDone body. Best val acc: 0.0582\n\n--- Pretraining left ---\nSố mẫu train: 2340, val: 464\nSố batch train: 74, val: 15\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 1:  68%|██████▊   | 50/74 [00:00<00:00, 63.37it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 1/10 | Train Loss: 7.6256 Acc: 0.0073 | Val Loss: 6.2909 Acc: 0.0216\nBest model saved at epoch 1, val_acc=0.0216\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 2:   0%|          | 0/74 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 2/10 | Train Loss: 5.8474 Acc: 0.0291 | Val Loss: 5.8927 Acc: 0.0216\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 3:  46%|████▌     | 34/74 [00:00<00:00, 59.93it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 3/10 | Train Loss: 5.2596 Acc: 0.0530 | Val Loss: 5.7182 Acc: 0.0453\nBest model saved at epoch 3, val_acc=0.0453\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 4:  30%|██▉       | 22/74 [00:00<00:01, 51.77it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 4/10 | Train Loss: 4.8199 Acc: 0.0795 | Val Loss: 5.2725 Acc: 0.0431\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 5:  38%|███▊      | 28/74 [00:00<00:00, 55.61it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 5/10 | Train Loss: 4.4395 Acc: 0.1111 | Val Loss: 5.2689 Acc: 0.0647\nBest model saved at epoch 5, val_acc=0.0647\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 6:   9%|▉         | 7/74 [00:00<00:02, 26.72it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 6/10 | Train Loss: 4.2360 Acc: 0.1312 | Val Loss: 5.2525 Acc: 0.0582\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 7:   1%|▏         | 1/74 [00:00<00:14,  5.06it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 7/10 | Train Loss: 4.0316 Acc: 0.1393 | Val Loss: 5.0787 Acc: 0.1013\nBest model saved at epoch 7, val_acc=0.1013\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 8:  58%|█████▊    | 43/74 [00:00<00:00, 61.60it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 8:  69%|██████▉   | 51/74 [00:00<00:00, 64.21it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 8/10 | Train Loss: 3.8401 Acc: 0.1739 | Val Loss: 5.0418 Acc: 0.0905\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 9:  68%|██████▊   | 50/74 [00:00<00:00, 64.62it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 9/10 | Train Loss: 3.6503 Acc: 0.1983 | Val Loss: 5.3098 Acc: 0.0754\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 10:  82%|████████▏ | 61/74 [00:01<00:00, 65.79it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 10/10 | Train Loss: 3.5234 Acc: 0.2286 | Val Loss: 4.9160 Acc: 0.1121\nBest model saved at epoch 10, val_acc=0.1121\nDone left. Best val acc: 0.1121\n\n--- Pretraining right ---\nSố mẫu train: 2340, val: 464\nSố batch train: 74, val: 15\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 1:  20%|██        | 15/74 [00:00<00:01, 42.67it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 1/10 | Train Loss: 7.5902 Acc: 0.0081 | Val Loss: 6.2549 Acc: 0.0086\nBest model saved at epoch 1, val_acc=0.0086\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 2:  45%|████▍     | 33/74 [00:00<00:00, 54.38it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 2/10 | Train Loss: 6.0033 Acc: 0.0192 | Val Loss: 6.0601 Acc: 0.0216\nBest model saved at epoch 2, val_acc=0.0216\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 3:  47%|████▋     | 35/74 [00:00<00:00, 58.67it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 3/10 | Train Loss: 5.5261 Acc: 0.0329 | Val Loss: 5.5489 Acc: 0.0280\nBest model saved at epoch 3, val_acc=0.0280\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 4:  31%|███       | 23/74 [00:00<00:00, 52.14it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 4/10 | Train Loss: 5.1882 Acc: 0.0423 | Val Loss: 5.5168 Acc: 0.0302\nBest model saved at epoch 4, val_acc=0.0302\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 5:  19%|█▉        | 14/74 [00:00<00:01, 41.47it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 5/10 | Train Loss: 4.9386 Acc: 0.0543 | Val Loss: 5.3314 Acc: 0.0453\nBest model saved at epoch 5, val_acc=0.0453\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 6:  49%|████▊     | 36/74 [00:00<00:00, 58.85it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 6/10 | Train Loss: 4.7297 Acc: 0.0731 | Val Loss: 5.2077 Acc: 0.0474\nBest model saved at epoch 6, val_acc=0.0474\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 7:  68%|██████▊   | 50/74 [00:00<00:00, 63.78it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                  \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 7/10 | Train Loss: 4.6145 Acc: 0.0705 | Val Loss: 5.4570 Acc: 0.0388\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 8:   9%|▉         | 7/74 [00:00<00:02, 27.78it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 8/10 | Train Loss: 4.4385 Acc: 0.0915 | Val Loss: 5.2350 Acc: 0.0647\nBest model saved at epoch 8, val_acc=0.0647\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 9:  86%|████████▋ | 64/74 [00:01<00:00, 67.78it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 9/10 | Train Loss: 4.3654 Acc: 0.0979 | Val Loss: 5.1947 Acc: 0.0647\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 10:  36%|███▋      | 27/74 [00:00<00:00, 55.61it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                   ","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 10/10 | Train Loss: 4.2778 Acc: 0.1188 | Val Loss: 5.1152 Acc: 0.0560\nDone right. Best val acc: 0.0647\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}