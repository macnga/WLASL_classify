{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9fa3ae",
   "metadata": {
    "papermill": {
     "duration": 0.007284,
     "end_time": "2025-06-02T08:29:21.215472",
     "exception": false,
     "start_time": "2025-06-02T08:29:21.208188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset visualize\n",
    "labels có videos <6 - bỏ qua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3eafa7",
   "metadata": {
    "papermill": {
     "duration": 0.004386,
     "end_time": "2025-06-02T08:29:21.224697",
     "exception": false,
     "start_time": "2025-06-02T08:29:21.220311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4868b91f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:21.234275Z",
     "iopub.status.busy": "2025-06-02T08:29:21.234034Z",
     "iopub.status.idle": "2025-06-02T08:29:21.240047Z",
     "shell.execute_reply": "2025-06-02T08:29:21.239538Z"
    },
    "papermill": {
     "duration": 0.011997,
     "end_time": "2025-06-02T08:29:21.241062",
     "exception": false,
     "start_time": "2025-06-02T08:29:21.229065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388848c",
   "metadata": {
    "papermill": {
     "duration": 0.004122,
     "end_time": "2025-06-02T08:29:21.249539",
     "exception": false,
     "start_time": "2025-06-02T08:29:21.245417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spatial - Temporal graph convolution N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a4ce81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:21.258971Z",
     "iopub.status.busy": "2025-06-02T08:29:21.258712Z",
     "iopub.status.idle": "2025-06-02T08:29:26.338243Z",
     "shell.execute_reply": "2025-06-02T08:29:26.337661Z"
    },
    "papermill": {
     "duration": 5.085625,
     "end_time": "2025-06-02T08:29:26.339469",
     "exception": false,
     "start_time": "2025-06-02T08:29:21.253844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pdb\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520b844c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.350270Z",
     "iopub.status.busy": "2025-06-02T08:29:26.350003Z",
     "iopub.status.idle": "2025-06-02T08:29:26.366161Z",
     "shell.execute_reply": "2025-06-02T08:29:26.365620Z"
    },
    "papermill": {
     "duration": 0.022897,
     "end_time": "2025-06-02T08:29:26.367203",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.344306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"The Graph to model the skeletons extracted by the Mediapipe\n",
    "\n",
    "    Args:\n",
    "        strategy (string): must be one of the follow candidates\n",
    "        - uniform: Uniform Labeling\n",
    "        - distance: Distance Partitioning\n",
    "        - spatial: Spatial Configuration\n",
    "        For more information, please refer to the section 'Partition Strategies'\n",
    "            in our paper (https://arxiv.org/abs/1801.07455).\n",
    "\n",
    "        layout (string): must be one of the follow candidates\n",
    "        - openpose: Is consists of 18 joints. For more information, please\n",
    "            refer to https://github.com/CMU-Perceptual-Computing-Lab/openpose#output\n",
    "        - ntu-rgb+d: Is consists of 25 joints. For more information, please\n",
    "            refer to https://github.com/shahroudy/NTURGB-D\n",
    "\n",
    "        max_hop (int): the maximal distance between two connected nodes\n",
    "        dilation (int): controls the spacing between the kernel points\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layout='custom', strategy='uniform', max_hop=1, dilation=1):\n",
    "        self.max_hop = max_hop\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.get_edge(layout)\n",
    "        self.hop_dis = get_hop_distance(self.num_node, self.edge, max_hop=max_hop)\n",
    "        self.get_adjacency(strategy)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_edge(self, layout):\n",
    "        # 'body', 'left', 'right'\n",
    "        # if layout == 'custom_hand21':\n",
    "        if layout == 'left' or layout == 'right':\n",
    "            self.num_node = 21\n",
    "            self_link = [(i, i) for i in range(self.num_node)]\n",
    "            neighbor_1base = [\n",
    "                [0, 1],[1, 2],[2, 3],[3, 4],\n",
    "                [0, 5],[5, 6], [6, 7], [7, 8],\n",
    "                [0, 9], [9, 10],  [10, 11], [11, 12],\n",
    "                [0, 13], [13, 14], [14, 15], [15, 16],\n",
    "                [0, 17], [17, 18], [18, 19],  [19, 20],\n",
    "            ]\n",
    "            neighbor_link = neighbor_1base\n",
    "            self.edge = self_link + neighbor_link\n",
    "            self.center = 0\n",
    "        \n",
    "        elif layout == 'body':\n",
    "            self.num_node = 25\n",
    "            self_link = [(i, i) for i in range(self.num_node)]\n",
    "            neighbor_1base = [\n",
    "                [0, 1],[1, 2],[2, 3],[3, 7],\n",
    "                [0, 4],[4, 5],[5, 6],[6, 8],\n",
    "                [9, 10],\n",
    "                [11, 12],\n",
    "                [11, 13],[13, 15],[15, 21],[15, 19],[15, 17],\n",
    "                [17, 19],\n",
    "                [11, 23],\n",
    "                [12, 14],[14, 16],[16, 18],[16, 20],[16, 22],\n",
    "                [18, 20],\n",
    "                [12, 24],\n",
    "                [23, 24]\n",
    "            ]\n",
    "            neighbor_link = neighbor_1base\n",
    "            self.edge = self_link + neighbor_link\n",
    "            self.center = 0\n",
    "\n",
    "    def get_adjacency(self, strategy):\n",
    "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
    "        adjacency = np.zeros((self.num_node, self.num_node))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[self.hop_dis == hop] = 1\n",
    "        normalize_adjacency = normalize_digraph(adjacency)\n",
    "\n",
    "        if strategy == 'uniform':\n",
    "            A = np.zeros((1, self.num_node, self.num_node))\n",
    "            A[0] = normalize_adjacency\n",
    "            self.A = A\n",
    "        elif strategy == 'distance':\n",
    "            A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "            for i, hop in enumerate(valid_hop):\n",
    "                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "            self.A = A\n",
    "        elif strategy == 'spatial':\n",
    "            A = []\n",
    "            for hop in valid_hop:\n",
    "                a_root = np.zeros((self.num_node, self.num_node))\n",
    "                a_close = np.zeros((self.num_node, self.num_node))\n",
    "                a_further = np.zeros((self.num_node, self.num_node))\n",
    "                for i in range(self.num_node):\n",
    "                    for j in range(self.num_node):\n",
    "                        if self.hop_dis[j, i] == hop:\n",
    "                            if (\n",
    "                                self.hop_dis[j, self.center]\n",
    "                                == self.hop_dis[i, self.center]\n",
    "                            ):\n",
    "                                a_root[j, i] = normalize_adjacency[j, i]\n",
    "                            elif (\n",
    "                                self.hop_dis[j, self.center]\n",
    "                                > self.hop_dis[i, self.center]\n",
    "                            ):\n",
    "                                a_close[j, i] = normalize_adjacency[j, i]\n",
    "                            else:\n",
    "                                a_further[j, i] = normalize_adjacency[j, i]\n",
    "                if hop == 0:\n",
    "                    A.append(a_root)\n",
    "                else:\n",
    "                    A.append(a_root + a_close)\n",
    "                    A.append(a_further)\n",
    "            A = np.stack(A)\n",
    "            self.A = A\n",
    "        else:\n",
    "            raise ValueError(\"Do Not Exist This Strategy\")\n",
    "\n",
    "\n",
    "def get_hop_distance(num_node, edge, max_hop=1):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in edge:\n",
    "        A[j, i] = 1\n",
    "        A[i, j] = 1\n",
    "\n",
    "    # compute hop steps\n",
    "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
    "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n",
    "    arrive_mat = np.stack(transfer_mat) > 0\n",
    "    for d in range(max_hop, -1, -1):\n",
    "        hop_dis[arrive_mat[d]] = d\n",
    "    return hop_dis\n",
    "\n",
    "\n",
    "def normalize_digraph(A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    num_node = A.shape[0]\n",
    "    Dn = np.zeros((num_node, num_node))\n",
    "    for i in range(num_node):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7329250a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.376806Z",
     "iopub.status.busy": "2025-06-02T08:29:26.376565Z",
     "iopub.status.idle": "2025-06-02T08:29:26.388167Z",
     "shell.execute_reply": "2025-06-02T08:29:26.387640Z"
    },
    "papermill": {
     "duration": 0.017625,
     "end_time": "2025-06-02T08:29:26.389154",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.371529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCN_unit(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        A,\n",
    "        adaptive=True,\n",
    "        t_kernel_size=1,\n",
    "        t_stride=1,\n",
    "        t_padding=0,\n",
    "        t_dilation=1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        assert A.size(0) == self.kernel_size\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels * kernel_size,\n",
    "            kernel_size=(t_kernel_size, 1),\n",
    "            padding=(t_padding, 0),\n",
    "            stride=(t_stride, 1),\n",
    "            dilation=(t_dilation, 1),\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.adaptive = adaptive\n",
    "        # print(self.adaptive)\n",
    "        if self.adaptive:\n",
    "            self.A = nn.Parameter(A.clone())\n",
    "        else:\n",
    "            self.register_buffer('A', A)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, len_x):\n",
    "        x = self.conv(x)\n",
    "\n",
    "        n, kc, t, v = x.size()\n",
    "        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n",
    "        x = torch.einsum('nkctv,kvw->nctw', (x, self.A)).contiguous()\n",
    "        y = self.bn(x)\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "class STGCN_block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        A,\n",
    "        adaptive=True,\n",
    "        stride=1,\n",
    "        dropout=0,\n",
    "        residual=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(kernel_size) == 2\n",
    "        assert kernel_size[0] % 2 == 1\n",
    "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
    "        self.gcn = GCN_unit(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size[1],\n",
    "            A,\n",
    "            adaptive=adaptive,\n",
    "        )\n",
    "        if kernel_size[0] > 1:\n",
    "            self.tcn = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    (kernel_size[0], 1),\n",
    "                    (stride, 1),\n",
    "                    padding,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(dropout, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            self.tcn = nn.Identity()\n",
    "\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "\n",
    "        else:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, len_x=None):\n",
    "        res = self.residual(x)\n",
    "        x = self.gcn(x, len_x)\n",
    "        x = self.tcn(x) + res\n",
    "        return self.relu(x)\n",
    "\n",
    "class STGCNChain(nn.Sequential):\n",
    "    def __init__(self, in_dim, block_args, kernel_size, A, adaptive):\n",
    "        super(STGCNChain, self).__init__()\n",
    "        last_dim = in_dim\n",
    "        for i, [channel, depth] in enumerate(block_args):\n",
    "            for j in range(depth):\n",
    "                self.add_module(f'layer{i}_{j}', STGCN_block(last_dim, channel, kernel_size, A.clone(), adaptive))\n",
    "                last_dim = channel\n",
    "\n",
    "def get_stgcn_chain(in_dim, level, kernel_size, A, adaptive):\n",
    "    if level == 'spatial':\n",
    "        block_args = [[64,1], [128,1], [256,1]]\n",
    "    elif level == 'temporal':\n",
    "        block_args = [[256,3]]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return STGCNChain(in_dim, block_args, kernel_size, A, adaptive), block_args[-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23938e6",
   "metadata": {
    "papermill": {
     "duration": 0.004051,
     "end_time": "2025-06-02T08:29:26.397481",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.393430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30444608",
   "metadata": {
    "papermill": {
     "duration": 0.004002,
     "end_time": "2025-06-02T08:29:26.405695",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.401693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Keypoint (T = 64, N_keypoint = 21/25, 3 _(x, y, z))\n",
    "\n",
    "image (112, 112, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2e3394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.415422Z",
     "iopub.status.busy": "2025-06-02T08:29:26.415042Z",
     "iopub.status.idle": "2025-06-02T08:29:26.420789Z",
     "shell.execute_reply": "2025-06-02T08:29:26.420282Z"
    },
    "papermill": {
     "duration": 0.011445,
     "end_time": "2025-06-02T08:29:26.421727",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.410282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SignKeypointDataset(Dataset):\n",
    "    def __init__(self, data_root, video_list_txt, part='body'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root (str): Đường dẫn thư mục chứa các thư mục video_id.\n",
    "            video_list_txt (str): File .txt chứa danh sách video_id cần dùng.\n",
    "            part (str): 'body', 'left', hoặc 'right' để pretrain riêng.\n",
    "        \"\"\"\n",
    "        assert part in ['body', 'left', 'right'], \"part must be 'body', 'left', or 'right'\"\n",
    "        self.data_root = data_root\n",
    "        self.part = part\n",
    "\n",
    "        with open(video_list_txt, 'r') as f:\n",
    "            self.video_ids = [line.strip() for line in f]\n",
    "\n",
    "        self.paths = [os.path.join(data_root, vid, \"keypoints.npy\") for vid in self.video_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        kp_path = self.paths[idx]\n",
    "        keypoints = np.load(kp_path)  # shape: (T, 67, 3)\n",
    "\n",
    "        # Split\n",
    "        if self.part == 'body':\n",
    "            part_kp = keypoints[:, :25, :]  # (T, 25, 3)\n",
    "        elif self.part == 'left':\n",
    "            part_kp = keypoints[:, 25:46, :]  # (T, 21, 3)\n",
    "        elif self.part == 'right':\n",
    "            part_kp = keypoints[:, 46:, :]  # (T, 21, 3)\n",
    "\n",
    "        return torch.tensor(part_kp, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8f353",
   "metadata": {
    "papermill": {
     "duration": 0.004057,
     "end_time": "2025-06-02T08:29:26.430066",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.426009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce61f60",
   "metadata": {
    "papermill": {
     "duration": 0.004111,
     "end_time": "2025-06-02T08:29:26.438433",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.434322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "801dbd65",
   "metadata": {
    "papermill": {
     "duration": 0.004081,
     "end_time": "2025-06-02T08:29:26.446691",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.442610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PGF Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed60fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.456137Z",
     "iopub.status.busy": "2025-06-02T08:29:26.455931Z",
     "iopub.status.idle": "2025-06-02T08:29:26.480265Z",
     "shell.execute_reply": "2025-06-02T08:29:26.479798Z"
    },
    "papermill": {
     "duration": 0.030201,
     "end_time": "2025-06-02T08:29:26.481239",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.451038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97cfdcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.490872Z",
     "iopub.status.busy": "2025-06-02T08:29:26.490665Z",
     "iopub.status.idle": "2025-06-02T08:29:26.500540Z",
     "shell.execute_reply": "2025-06-02T08:29:26.500061Z"
    },
    "papermill": {
     "duration": 0.015988,
     "end_time": "2025-06-02T08:29:26.501513",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.485525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeformableAttention2D(nn.Module):\n",
    "    def __init__(self, dim, dim_head=64, heads=8, dropout=0.1, offset_groups=8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.offset_groups = offset_groups\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        self.to_q = nn.Conv1d(dim, inner_dim, 1, bias=False)\n",
    "        self.to_kv = nn.Conv2d(dim, inner_dim * 2, 1, bias=False)\n",
    "\n",
    "        self.to_offsets = nn.Sequential(\n",
    "            nn.Conv1d(dim, dim, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim, offset_groups * 2, 1)\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Conv1d(inner_dim, dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query_feat, context_feat, ref_points):\n",
    "        \"\"\"\n",
    "        query_feat: (B, C, N) - from pose encoder\n",
    "        context_feat: (B, C, H, W) - from vision encoder\n",
    "        ref_points: (B, N, 2) - normalized [0, 1] reference points\n",
    "        \"\"\"\n",
    "        B, C, N = query_feat.shape\n",
    "        _, _, H, W = context_feat.shape\n",
    "\n",
    "        # Project Q, K, V\n",
    "        q = self.to_q(query_feat)  # (B, heads*dim_head, N)\n",
    "        q = q.view(B, self.heads, self.dim_head, N).permute(0, 1, 3, 2)  # (B, heads, N, dim_head)\n",
    "\n",
    "        kv = self.to_kv(context_feat)  # (B, 2*heads*dim_head, H, W)\n",
    "        kv = kv.view(B, 2, self.heads, self.dim_head, H, W)\n",
    "        k, v = kv[:, 0], kv[:, 1]  # Each: (B, heads, dim_head, H, W)\n",
    "\n",
    "        # Generate sampling offsets (B, offset_groups*2, N)\n",
    "        offsets = self.to_offsets(query_feat)  # (B, offset_groups*2, N)\n",
    "        offsets = offsets.view(B, self.offset_groups, 2, N).permute(0, 1, 3, 2)  # (B, G, N, 2)\n",
    "\n",
    "        # Add offsets to ref_points\n",
    "        ref_points = ref_points.unsqueeze(1).repeat(1, self.offset_groups, 1, 1)  # (B, G, N, 2)\n",
    "        coords = ref_points + offsets / torch.tensor([W, H], device=query_feat.device)  # scale offset\n",
    "        coords = coords.clamp(0, 1)\n",
    "\n",
    "        # Reshape coords for sampling\n",
    "        coords = coords.view(B, self.offset_groups * N, 2)  # (B, G*N, 2)\n",
    "        coords = coords * 2 - 1  # scale to [-1, 1] for grid_sample\n",
    "        coords = coords.view(B, 1, self.offset_groups * N, 1, 2)  # (B, 1, GN, 1, 2)\n",
    "\n",
    "        # Sample K and V with bilinear interpolation\n",
    "        k = k.view(B * self.heads, self.dim_head, H, W)\n",
    "        v = v.view(B * self.heads, self.dim_head, H, W)\n",
    "\n",
    "        k_sampled = F.grid_sample(k, coords.expand(-1, self.dim_head, -1, -1, -1), align_corners=True)\n",
    "        v_sampled = F.grid_sample(v, coords.expand(-1, self.dim_head, -1, -1, -1), align_corners=True)\n",
    "        k_sampled = k_sampled.squeeze(-1).view(B, self.heads, self.dim_head, N, self.offset_groups)\n",
    "        v_sampled = v_sampled.squeeze(-1).view(B, self.heads, self.dim_head, N, self.offset_groups)\n",
    "\n",
    "        # QK attention\n",
    "        q = q.unsqueeze(-1)  # (B, heads, N, dim_head, 1)\n",
    "        attn = (q * k_sampled).sum(3) * self.scale  # (B, heads, N, G)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Apply attention to V\n",
    "        out = (attn.unsqueeze(3) * v_sampled).sum(-1)  # (B, heads, dim_head, N)\n",
    "        out = out.permute(0, 1, 3, 2).contiguous().view(B, -1, N)  # (B, heads*dim_head, N)\n",
    "\n",
    "        out = self.dropout(self.proj(out))  # (B, C, N)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b75f2f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.511088Z",
     "iopub.status.busy": "2025-06-02T08:29:26.510883Z",
     "iopub.status.idle": "2025-06-02T08:29:26.522361Z",
     "shell.execute_reply": "2025-06-02T08:29:26.521904Z"
    },
    "papermill": {
     "duration": 0.017436,
     "end_time": "2025-06-02T08:29:26.523413",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.505977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGFModule(nn.Module):\n",
    "    def __init__(self, dim, dim_head=64, heads=8):\n",
    "        super(PGFModule, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "\n",
    "        # Gater module để tạo attention weights giữa 2 stream\n",
    "        self.gater = nn.Sequential(\n",
    "            nn.Conv1d(dim * 2, dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim, dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Multi-head self-attention (cho pose và vision fusion cơ bản)\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)\n",
    "\n",
    "        # Deformable Attention 2D (lấy từ Uni-Sign)\n",
    "        self.deform_attn = DeformableAttention2D(\n",
    "            dim=dim,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            dropout=0.1,\n",
    "            offset_groups=heads,\n",
    "        )\n",
    "\n",
    "    def forward(self, Fp, Fr, J):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Fp: Pose feature (B, C, N) - từ pose encoder\n",
    "            Fr: Vision feature (B, C, H, W) - từ vision encoder\n",
    "            J: Prior keypoint hand (B, N, 2) - tọa độ tay sau khi scale về [0,1]\n",
    "\n",
    "        Returns:\n",
    "            F_fused: output feature đã fusion (B, C, N)\n",
    "        \"\"\"\n",
    "\n",
    "        B, C, N = Fp.shape\n",
    "\n",
    "        # 1. Tính attention giữa pose và vision (multi-head attention)\n",
    "        # Fp -> (B, N, C), Fr -> (B, HW, C)\n",
    "        Fp_ = rearrange(Fp, 'b c n -> b n c')\n",
    "        Fr_ = rearrange(Fr, 'b c h w -> b (h w) c')\n",
    "\n",
    "        # Self-attention trên pose + vision\n",
    "        attn_output, _ = self.self_attn(Fp_, Fr_, Fr_)\n",
    "        attn_output = rearrange(attn_output, 'b n c -> b c n')\n",
    "\n",
    "        # 2. Fusion với Gater module\n",
    "        gater_input = torch.cat([Fp, attn_output], dim=1)  # (B, 2C, N)\n",
    "        gate = self.gater(gater_input)  # (B, C, N)\n",
    "\n",
    "        # Gated fusion pose stream\n",
    "        Fp_fused = Fp * gate + attn_output * (1 - gate)  # (B, C, N)\n",
    "\n",
    "        # 3. Deformable attention với prior position J (B, N, 2)\n",
    "        F_fused = self.deform_attn(Fp_fused, Fr, J)  # (B, C, N)\n",
    "\n",
    "        return F_fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb686ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.533232Z",
     "iopub.status.busy": "2025-06-02T08:29:26.533036Z",
     "iopub.status.idle": "2025-06-02T08:29:26.541199Z",
     "shell.execute_reply": "2025-06-02T08:29:26.540720Z"
    },
    "papermill": {
     "duration": 0.014076,
     "end_time": "2025-06-02T08:29:26.542148",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.528072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RGBFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(torchvision.models.efficientnet_b0(pretrained=True).children())[:-2])\n",
    "        self.rgb_proj = nn.Conv2d(1280, hidden_dim, kernel_size=1)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, batch_first=True)\n",
    "        self.deform_attn = DeformableAttention2D(\n",
    "            dim=hidden_dim, dim_head=32, heads=8, dropout=0.,\n",
    "            downsample_factor=1, offset_kernel_size=1\n",
    "        )\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim * 2, hidden_dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        for layer in self.gate:\n",
    "            if isinstance(layer, nn.Conv1d):\n",
    "                nn.init.constant_(layer.weight, 0)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, query_feat, rgb_feat, indices, rgb_len, pose_coords):\n",
    "        \"\"\"\n",
    "        query_feat: [B, C, T, N] (from ST-GCN)\n",
    "        rgb_feat: [sum(L), 1280, H, W] (raw backbone output for all frames)\n",
    "        pose_coords: [sum(L), N, 2] (reference for DA2D)\n",
    "        indices: frame indices per video\n",
    "        rgb_len: frame count per video\n",
    "        \"\"\"\n",
    "        b, c, T, n = query_feat.shape\n",
    "        rgb_feat = self.rgb_proj(rgb_feat)  # -> [sum(L), C, H, W]\n",
    "        start = 0\n",
    "\n",
    "        for batch in range(b):\n",
    "            frame_idx = indices[start:start + rgb_len[batch]].to(torch.long)\n",
    "            if rgb_len[batch] == 1 and -1 in frame_idx:\n",
    "                start += rgb_len[batch]\n",
    "                continue\n",
    "\n",
    "            # select\n",
    "            query = query_feat[batch, :, frame_idx]  # [C, L, N]\n",
    "            query = rearrange(query, 'c t n -> t n c')  # [L, N, C]\n",
    "\n",
    "            rgb_patch = rgb_feat[start:start + rgb_len[batch]]  # [L, C, H, W]\n",
    "            coords = pose_coords[start:start + rgb_len[batch]]  # [L, N, 2]\n",
    "\n",
    "            # DA2D over image features using joints\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                kv_feat = self.deform_attn(query.permute(1, 0, 2), rgb_patch, coords)  # [N, L, C]\n",
    "\n",
    "            kv_feat = kv_feat.permute(1, 0, 2)  # [L, N, C]\n",
    "            query = query  # [L, N, C]\n",
    "            fusion_input = torch.cat([kv_feat, query], dim=-1).permute(1, 2, 0)  # [N, 2C, L]\n",
    "            gate_score = self.gate(fusion_input)  # [N, C, L]\n",
    "            fused = gate_score * kv_feat.permute(1, 2, 0) + (1 - gate_score) * query.permute(1, 2, 0)  # [N, C, L]\n",
    "            query_feat[batch, :, frame_idx] = fused.permute(1, 2, 0)  # back to [C, T, N]\n",
    "\n",
    "            start += rgb_len[batch]\n",
    "\n",
    "        return query_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772514b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.551846Z",
     "iopub.status.busy": "2025-06-02T08:29:26.551627Z",
     "iopub.status.idle": "2025-06-02T08:29:26.556256Z",
     "shell.execute_reply": "2025-06-02T08:29:26.555799Z"
    },
    "papermill": {
     "duration": 0.010549,
     "end_time": "2025-06-02T08:29:26.557274",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.546725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Từ keypoint (T, N, 3) linear sang (T, N, 64) để encoder sâu hơn\n",
    "\n",
    "class PoseBranch(nn.Module):\n",
    "    def __init__(self, A, input_dim=3, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "        self.spatial_gcn, final_dim = get_stgcn_chain(embed_dim, 'spatial', (1, A.size(0)), A.clone(), True)\n",
    "        self.temporal_gcn, _ = get_stgcn_chain(final_dim, 'temporal', (5, A.size(0)), A.clone(), True)\n",
    "        self.out_proj = nn.Linear(final_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).permute(0, 3, 1, 2)\n",
    "        x = self.spatial_gcn(x)\n",
    "        x = self.temporal_gcn(x)\n",
    "        x = x.mean(-1).transpose(1, 2)\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38874e89",
   "metadata": {
    "papermill": {
     "duration": 0.004198,
     "end_time": "2025-06-02T08:29:26.565881",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.561683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d719386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T03:05:00.911394Z",
     "iopub.status.busy": "2025-05-28T03:05:00.910666Z",
     "iopub.status.idle": "2025-05-28T03:05:01.107951Z",
     "shell.execute_reply": "2025-05-28T03:05:01.107149Z",
     "shell.execute_reply.started": "2025-05-28T03:05:00.911369Z"
    },
    "papermill": {
     "duration": 0.004014,
     "end_time": "2025-06-02T08:29:26.574214",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.570200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "> Cố định seed để chia reproducible\n",
    "random.seed(42)\n",
    "\n",
    "> Đường dẫn\n",
    "json_path = \"/kaggle/input/wlasl-processed/WLASL_v0.3.json\"\n",
    "data_dir = \"/kaggle/input/data-wlasl/DATA\"\n",
    "\n",
    "> Lấy các video_id (folder name) đã có trong DATA/\n",
    "used_video_ids = set(os.listdir(data_dir))\n",
    "\n",
    "> Bước 1: Nhóm video_id theo nhãn (chỉ giữ những video có trong DATA/)\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "label_to_videos = defaultdict(list)\n",
    "video_to_label = dict()\n",
    "\n",
    "for entry in data:\n",
    "    label = entry['gloss']\n",
    "    for instance in entry['instances']:\n",
    "        video_id = instance['video_id']\n",
    "        if video_id in used_video_ids:\n",
    "            label_to_videos[label].append(video_id)\n",
    "            video_to_label[video_id] = label\n",
    "\n",
    "> Bước 2: Chia theo 6:2:2 cho từng nhãn\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for label, videos in label_to_videos.items():\n",
    "    if len(videos) < 3:\n",
    "        continue  # Không đủ để chia\n",
    "\n",
    "    random.shuffle(videos)\n",
    "    n = len(videos)\n",
    "    n_test = int(n * 0.2)\n",
    "    n_val = int(n * 0.2)\n",
    "\n",
    "    test_list.extend(videos[:n_test])\n",
    "    val_list.extend(videos[n_test:n_test+n_val])\n",
    "    train_list.extend(videos[n_test+n_val:])\n",
    "> Bước 3: Lưu vào file (mỗi dòng: video_id label)\n",
    "def save_list(path, video_ids, video_to_label):\n",
    "    with open(path, \"w\") as f:\n",
    "        for vid in video_ids:\n",
    "            label = video_to_label[vid]\n",
    "            f.write(f\"{vid} {label}\\n\")\n",
    "\n",
    "save_list(\"/kaggle/working/train.txt\", train_list, video_to_label)\n",
    "save_list(\"/kaggle/working/val.txt\", val_list, video_to_label)\n",
    "save_list(\"/kaggle/working/test.txt\", test_list, video_to_label)\n",
    "\n",
    "print(f\"[Done] Train: {len(train_list)} | Val: {len(val_list)} | Test: {len(test_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9e878",
   "metadata": {
    "papermill": {
     "duration": 0.004009,
     "end_time": "2025-06-02T08:29:26.582426",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.578417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3239ca73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:26.591710Z",
     "iopub.status.busy": "2025-06-02T08:29:26.591530Z",
     "iopub.status.idle": "2025-06-02T08:29:31.750608Z",
     "shell.execute_reply": "2025-06-02T08:29:31.749812Z"
    },
    "papermill": {
     "duration": 5.165369,
     "end_time": "2025-06-02T08:29:31.752038",
     "exception": false,
     "start_time": "2025-06-02T08:29:26.586669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import torchvision\n",
    "\n",
    "class Classify_Sign(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Uni_Sign, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.modes = ['body', 'left', 'right']\n",
    "\n",
    "        self.graph, A = {}, []\n",
    "        hidden_dim = args.hidden_dim\n",
    "        self.proj_linear = nn.ModuleDict()\n",
    "        for mode in self.modes:\n",
    "            self.graph[mode] = Graph(layout=mode, strategy='distance', max_hop=1)\n",
    "            A.append(torch.tensor(self.graph[mode].A, dtype=torch.float32, requires_grad=False))\n",
    "            self.proj_linear[mode] = nn.Linear(3, 64)\n",
    "\n",
    "        self.gcn_modules = nn.ModuleDict()\n",
    "        self.fusion_gcn_modules = nn.ModuleDict()\n",
    "        spatial_kernel_size = A[0].size(0)\n",
    "        for index, mode in enumerate(self.modes):\n",
    "            self.gcn_modules[mode], final_dim = get_stgcn_chain(64, 'spatial', (1, spatial_kernel_size), A[index].clone(), True)\n",
    "            self.fusion_gcn_modules[mode], _ = get_stgcn_chain(final_dim, 'temporal', (5, spatial_kernel_size), A[index].clone(), True)\n",
    "\n",
    "        self.proj_pose_body = nn.Linear(final_dim, hidden_dim)\n",
    "        self.proj_pose_left = nn.Linear(final_dim, hidden_dim)\n",
    "        self.proj_pose_right = nn.Linear(final_dim, hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, args.num_classes)\n",
    "        )\n",
    "\n",
    "        if self.args.rgb_support:\n",
    "            self.rgb_support_backbone = torch.nn.Sequential(\n",
    "                *list(torchvision.models.efficientnet_b0(pretrained=True).children())[:-2]\n",
    "            )\n",
    "            self.rgb_proj = nn.Conv2d(1280, hidden_dim, kernel_size=1)\n",
    "\n",
    "            self.fusion_pose_rgb_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "            self.fusion_pose_rgb_DA = DeformableAttention2D(\n",
    "                dim=hidden_dim, dim_head=32, heads=8,\n",
    "                dropout=0., downsample_factor=1,\n",
    "                offset_kernel_size=1\n",
    "            )\n",
    "\n",
    "            self.fusion_gate = nn.Sequential(\n",
    "                nn.Conv1d(hidden_dim * 2, hidden_dim, 1),\n",
    "                nn.GELU(),\n",
    "                nn.Conv1d(hidden_dim, 1, 1),\n",
    "                nn.Tanh(),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for layer in self.fusion_gate:\n",
    "                if isinstance(layer, nn.Conv1d):\n",
    "                    nn.init.constant_(layer.weight, 0)\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def maybe_autocast(self, dtype=torch.float32):\n",
    "        return torch.cuda.amp.autocast(dtype=dtype) if torch.cuda.is_available() else contextlib.nullcontext()\n",
    "\n",
    "    def gather_feat_pose_rgb(self, gcn_feat, rgb_feat, indices, rgb_len, pose_init):\n",
    "        b, c, T, n = gcn_feat.shape\n",
    "        rgb_feat = self.rgb_proj(rgb_feat)\n",
    "        start = 0\n",
    "        for batch in range(b):\n",
    "            index = indices[start:start + rgb_len[batch]].to(torch.long)\n",
    "            if rgb_len[batch] == 1 and -1 in index:\n",
    "                start += rgb_len[batch]\n",
    "                continue\n",
    "            gcn_selected = gcn_feat[batch, :, index]\n",
    "            rgb_selected = rgb_feat[start:start + rgb_len[batch]]\n",
    "            pose_init_selected = pose_init[start:start + rgb_len[batch]]\n",
    "            gcn_selected = rearrange(gcn_selected, 'c t n -> t c n')\n",
    "            pose_init_selected = rearrange(pose_init_selected, 't n c -> t c n')\n",
    "            with self.maybe_autocast():\n",
    "                fused = self.fusion_pose_rgb_DA(gcn_selected, rgb_selected, pose_init_selected)\n",
    "            fused = fused.to(gcn_feat.dtype)\n",
    "            gate_input = torch.cat([fused, gcn_selected], dim=-2)\n",
    "            gate_score = self.fusion_gate(gate_input)\n",
    "            fused = gate_score * fused + (1 - gate_score) * gcn_selected\n",
    "            gcn_feat[batch, :, index] = rearrange(fused, 't c n -> c t n')\n",
    "            start += rgb_len[batch]\n",
    "        return gcn_feat\n",
    "\n",
    "    def forward(self, src_input):\n",
    "        if self.args.rgb_support:\n",
    "            rgb_support = {}\n",
    "            for part in ['left', 'right']:\n",
    "                rgb_feat = self.rgb_support_backbone(src_input[f'{part}_hands'])\n",
    "                rgb_support[f'{part}_hands'] = rgb_feat\n",
    "                rgb_support[f'{part}_sampled_indices'] = src_input[f'{part}_sampled_indices']\n",
    "\n",
    "        part_feats = {}\n",
    "        body_feat = None\n",
    "\n",
    "        for part in self.modes:\n",
    "            proj_feat = self.proj_linear[part](src_input[part]).permute(0, 3, 1, 2)\n",
    "            gcn_feat = self.gcn_modules[part](proj_feat)\n",
    "\n",
    "            if part == 'body':\n",
    "                body_feat = gcn_feat\n",
    "            else:\n",
    "                if part in ['left', 'right']:\n",
    "                    if self.args.rgb_support:\n",
    "                        gcn_feat = self.gather_feat_pose_rgb(\n",
    "                            gcn_feat,\n",
    "                            rgb_support[f'{part}_hands'],\n",
    "                            rgb_support[f'{part}_sampled_indices'],\n",
    "                            src_input[f'{part}_rgb_len'],\n",
    "                            src_input[f'{part}_skeletons_norm']\n",
    "                        )\n",
    "                    offset = -2 if part == 'left' else -1\n",
    "                    gcn_feat = gcn_feat + body_feat[..., offset][..., None].detach()\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "            gcn_feat = self.fusion_gcn_modules[part](gcn_feat)\n",
    "            pooled_feat = gcn_feat.mean(-1).transpose(1, 2)  # [B, T, C]\n",
    "\n",
    "            if part == 'body':\n",
    "                part_feats['body'] = self.proj_pose_body(pooled_feat)\n",
    "            elif part == 'left':\n",
    "                part_feats['left'] = self.proj_pose_left(pooled_feat)\n",
    "            elif part == 'right':\n",
    "                part_feats['right'] = self.proj_pose_right(pooled_feat)\n",
    "\n",
    "        feat_body = part_feats['body'].mean(dim=1)  # [B, hidden_dim]\n",
    "        feat_left = part_feats['left'].mean(dim=1)\n",
    "        feat_right = part_feats['right'].mean(dim=1)\n",
    "\n",
    "        feat_all = torch.cat([feat_body, feat_left, feat_right], dim=-1)\n",
    "        out = self.classifier(feat_all)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b8c20",
   "metadata": {
    "papermill": {
     "duration": 0.004582,
     "end_time": "2025-06-02T08:29:31.761402",
     "exception": false,
     "start_time": "2025-06-02T08:29:31.756820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from stgcn import get_stgcn_chain  # assumed your ST-GCN code is in stgcn.py\n",
    "from dataset_pose import PoseDataset  # custom dataset loading keypoints\n",
    "from dataset_rgb import RGBHandDataset  # custom dataset loading crop hand images\n",
    "\n",
    "# ----------- CONFIG -----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_root = \"Data\"\n",
    "label_file = \"class_name.txt\"\n",
    "train_list = \"train.txt\"\n",
    "num_classes = 100  # or however many classes you have\n",
    "pose_parts = [\"body\", \"left\", \"right\"]\n",
    "model_save_path = \"pretrained_models\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# ----------- ST-GCN TRAINING UTILS -----------\n",
    "def train_one_epoch_stgcn(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for keypoints, labels in tqdm(dataloader):\n",
    "        keypoints, labels = keypoints.to(device), labels.to(device)\n",
    "        out = model(keypoints)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * keypoints.size(0)\n",
    "        total_correct += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), total_correct / len(dataloader.dataset)\n",
    "\n",
    "# ----------- RGB TRAINING UTILS -----------\n",
    "def train_one_epoch_rgb(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for imgs, labels in tqdm(dataloader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_correct += (out.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader.dataset), total_correct / len(dataloader.dataset)\n",
    "\n",
    "# ----------- PRETRAIN ST-GCN -----------\n",
    "def pretrain_stgcn():\n",
    "    for part in pose_parts:\n",
    "        print(f\"\\n[INFO] Pretraining ST-GCN on {part} keypoints\")\n",
    "        graph = Graph(layout=part)\n",
    "        A = torch.tensor(graph.A, dtype=torch.float32)\n",
    "        model, _ = get_stgcn_chain(in_dim=2, level='spatial', kernel_size=(9, A.size(0)), A=A, adaptive=True)\n",
    "        model = nn.Sequential(model, nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(256, num_classes)).to(device)\n",
    "\n",
    "        dataset = PoseDataset(data_root, train_list, part=part)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(10):\n",
    "            loss, acc = train_one_epoch_stgcn(model, dataloader, optimizer, criterion)\n",
    "            print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Acc = {acc*100:.2f}%\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{model_save_path}/stgcn_{part}.pth\")\n",
    "        print(f\"[SAVED] ST-GCN {part} to stgcn_{part}.pth\")\n",
    "\n",
    "# ----------- PRETRAIN VISION ENCODER -----------\n",
    "def pretrain_rgb_encoder():\n",
    "    print(\"\\n[INFO] Pretraining Vision Encoder on cropped RGB hands\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    for side in [\"left\", \"right\"]:\n",
    "        print(f\"\\n[INFO] Pretraining for {side} hand\")\n",
    "        dataset = RGBHandDataset(data_root, train_list, side=side, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "        model = models.efficientnet_b0(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(10):\n",
    "            loss, acc = train_one_epoch_rgb(model, dataloader, optimizer, criterion)\n",
    "            print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Acc = {acc*100:.2f}%\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{model_save_path}/efficientnet_{side}.pth\")\n",
    "        print(f\"[SAVED] EfficientNet {side} to efficientnet_{side}.pth\")\n",
    "\n",
    "# ----------- MAIN -----------\n",
    "if __name__ == '__main__':\n",
    "    pretrain_stgcn()\n",
    "    pretrain_rgb_encoder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448611a",
   "metadata": {
    "papermill": {
     "duration": 0.004114,
     "end_time": "2025-06-02T08:29:31.770234",
     "exception": false,
     "start_time": "2025-06-02T08:29:31.766120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pretrain RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ceb78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:31.780196Z",
     "iopub.status.busy": "2025-06-02T08:29:31.779901Z",
     "iopub.status.idle": "2025-06-02T08:29:31.788185Z",
     "shell.execute_reply": "2025-06-02T08:29:31.787641Z"
    },
    "papermill": {
     "duration": 0.014409,
     "end_time": "2025-06-02T08:29:31.789127",
     "exception": false,
     "start_time": "2025-06-02T08:29:31.774718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HandRGBDataset(Dataset):\n",
    "    def __init__(self, label_file, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load labels\n",
    "        self.data = []\n",
    "        with open(label_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    video_id = parts[0]\n",
    "                    label = \" \".join(parts[1:])  # hỗ trợ label có khoảng trắng\n",
    "                    for vid_id in os.listdir(data_dir):\n",
    "                        for fname in os.listdir(os.path.join(data_dir, vid_id)):\n",
    "                            if fname.startswith(video_id):\n",
    "                                self.data.append((os.path.join(data_dir, vid_id, fname), label))\n",
    "\n",
    "        # Encode label\n",
    "        self.classes = sorted(list(set(label for _, label in self.data)))\n",
    "        self.label2idx = {label: idx for idx, label in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label_idx = self.label2idx[label]\n",
    "        return img, label_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95446d34",
   "metadata": {
    "papermill": {
     "duration": 0.004023,
     "end_time": "2025-06-02T08:29:31.797359",
     "exception": false,
     "start_time": "2025-06-02T08:29:31.793336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Numbers of label:\n",
    "Videos each label: 8-16\n",
    "\n",
    "Data samples\n",
    "- Train: 2341\n",
    "- Validation: 465\n",
    "- Test: 465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90152655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T08:29:31.806688Z",
     "iopub.status.busy": "2025-06-02T08:29:31.806504Z",
     "iopub.status.idle": "2025-06-02T09:51:26.901239Z",
     "shell.execute_reply": "2025-06-02T09:51:26.900311Z"
    },
    "papermill": {
     "duration": 4915.101225,
     "end_time": "2025-06-02T09:51:26.902916",
     "exception": false,
     "start_time": "2025-06-02T08:29:31.801691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset & Transform\n",
    "image_size = 112\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = HandRGBDataset(\n",
    "    label_file='/kaggle/input/train-test-valid/train.txt',\n",
    "    data_dir='/kaggle/input/data-wlasl/DATA',\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = HandRGBDataset(\n",
    "    label_file='/kaggle/input/train-test-valid/val.txt',\n",
    "    data_dir='/kaggle/input/data-wlasl/DATA',\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96d35323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T09:51:26.914169Z",
     "iopub.status.busy": "2025-06-02T09:51:26.913945Z",
     "iopub.status.idle": "2025-06-02T09:51:26.917673Z",
     "shell.execute_reply": "2025-06-02T09:51:26.917175Z"
    },
    "papermill": {
     "duration": 0.010192,
     "end_time": "2025-06-02T09:51:26.918713",
     "exception": false,
     "start_time": "2025-06-02T09:51:26.908521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29aa569d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T09:51:26.928142Z",
     "iopub.status.busy": "2025-06-02T09:51:26.927956Z",
     "iopub.status.idle": "2025-06-02T09:51:27.665103Z",
     "shell.execute_reply": "2025-06-02T09:51:27.664508Z"
    },
    "papermill": {
     "duration": 0.743293,
     "end_time": "2025-06-02T09:51:27.666435",
     "exception": false,
     "start_time": "2025-06-02T09:51:26.923142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 157MB/s]\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ab3a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T09:51:27.677147Z",
     "iopub.status.busy": "2025-06-02T09:51:27.676920Z",
     "iopub.status.idle": "2025-06-02T10:01:01.602058Z",
     "shell.execute_reply": "2025-06-02T10:01:01.600928Z"
    },
    "papermill": {
     "duration": 573.931878,
     "end_time": "2025-06-02T10:01:01.603299",
     "exception": false,
     "start_time": "2025-06-02T09:51:27.671421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 404/404 [00:25<00:00, 15.90it/s]\n",
      "Validation Epoch 1/20: 100%|██████████| 320/320 [00:04<00:00, 68.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=5.3023, Train Acc=0.0631 | Val Loss=5.5614, Val Acc=0.0500\n",
      "✅ Saved best model (Epoch 1, Val Acc: 0.0500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 404/404 [00:22<00:00, 17.69it/s]\n",
      "Validation Epoch 2/20: 100%|██████████| 320/320 [00:04<00:00, 70.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=3.6943, Train Acc=0.2604 | Val Loss=5.3495, Val Acc=0.0735\n",
      "✅ Saved best model (Epoch 2, Val Acc: 0.0735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 404/404 [00:22<00:00, 17.63it/s]\n",
      "Validation Epoch 3/20: 100%|██████████| 320/320 [00:04<00:00, 70.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=2.6125, Train Acc=0.4417 | Val Loss=5.4563, Val Acc=0.0742\n",
      "✅ Saved best model (Epoch 3, Val Acc: 0.0742)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 404/404 [00:23<00:00, 17.23it/s]\n",
      "Validation Epoch 4/20: 100%|██████████| 320/320 [00:04<00:00, 73.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=1.8433, Train Acc=0.5929 | Val Loss=5.5849, Val Acc=0.0942\n",
      "✅ Saved best model (Epoch 4, Val Acc: 0.0942)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 404/404 [00:23<00:00, 16.93it/s]\n",
      "Validation Epoch 5/20: 100%|██████████| 320/320 [00:04<00:00, 73.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=1.2603, Train Acc=0.7175 | Val Loss=5.9976, Val Acc=0.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 404/404 [00:24<00:00, 16.66it/s]\n",
      "Validation Epoch 6/20: 100%|██████████| 320/320 [00:04<00:00, 71.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.8388, Train Acc=0.8110 | Val Loss=6.2146, Val Acc=0.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 404/404 [00:24<00:00, 16.48it/s]\n",
      "Validation Epoch 7/20: 100%|██████████| 320/320 [00:04<00:00, 72.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.5653, Train Acc=0.8711 | Val Loss=6.5565, Val Acc=0.0950\n",
      "✅ Saved best model (Epoch 7, Val Acc: 0.0950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 404/404 [00:24<00:00, 16.68it/s]\n",
      "Validation Epoch 8/20: 100%|██████████| 320/320 [00:04<00:00, 73.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.3949, Train Acc=0.9081 | Val Loss=6.7992, Val Acc=0.0993\n",
      "✅ Saved best model (Epoch 8, Val Acc: 0.0993)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 404/404 [00:24<00:00, 16.70it/s]\n",
      "Validation Epoch 9/20: 100%|██████████| 320/320 [00:04<00:00, 72.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.3032, Train Acc=0.9279 | Val Loss=6.9644, Val Acc=0.0934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 404/404 [00:24<00:00, 16.46it/s]\n",
      "Validation Epoch 10/20: 100%|██████████| 320/320 [00:04<00:00, 72.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.2422, Train Acc=0.9400 | Val Loss=7.2150, Val Acc=0.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 404/404 [00:24<00:00, 16.62it/s]\n",
      "Validation Epoch 11/20: 100%|██████████| 320/320 [00:04<00:00, 71.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.1929, Train Acc=0.9499 | Val Loss=7.4496, Val Acc=0.0953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 404/404 [00:24<00:00, 16.53it/s]\n",
      "Validation Epoch 12/20: 100%|██████████| 320/320 [00:04<00:00, 68.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.1988, Train Acc=0.9446 | Val Loss=7.4286, Val Acc=0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 404/404 [00:24<00:00, 16.62it/s]\n",
      "Validation Epoch 13/20: 100%|██████████| 320/320 [00:04<00:00, 73.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.1863, Train Acc=0.9472 | Val Loss=7.6679, Val Acc=0.0957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 404/404 [00:24<00:00, 16.52it/s]\n",
      "Validation Epoch 14/20: 100%|██████████| 320/320 [00:04<00:00, 72.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.1678, Train Acc=0.9520 | Val Loss=7.8634, Val Acc=0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 404/404 [00:24<00:00, 16.58it/s]\n",
      "Validation Epoch 15/20: 100%|██████████| 320/320 [00:04<00:00, 74.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.1705, Train Acc=0.9484 | Val Loss=8.2023, Val Acc=0.1000\n",
      "✅ Saved best model (Epoch 15, Val Acc: 0.1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 404/404 [00:24<00:00, 16.63it/s]\n",
      "Validation Epoch 16/20: 100%|██████████| 320/320 [00:04<00:00, 75.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.1724, Train Acc=0.9452 | Val Loss=8.2419, Val Acc=0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 404/404 [00:24<00:00, 16.52it/s]\n",
      "Validation Epoch 17/20: 100%|██████████| 320/320 [00:04<00:00, 73.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss=0.1567, Train Acc=0.9509 | Val Loss=8.3370, Val Acc=0.0903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 404/404 [00:24<00:00, 16.64it/s]\n",
      "Validation Epoch 18/20: 100%|██████████| 320/320 [00:04<00:00, 73.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss=0.1541, Train Acc=0.9498 | Val Loss=8.6505, Val Acc=0.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 404/404 [00:24<00:00, 16.62it/s]\n",
      "Validation Epoch 19/20: 100%|██████████| 320/320 [00:04<00:00, 72.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss=0.1382, Train Acc=0.9555 | Val Loss=8.5736, Val Acc=0.0891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 404/404 [00:24<00:00, 16.69it/s]\n",
      "Validation Epoch 20/20: 100%|██████████| 320/320 [00:04<00:00, 71.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss=0.1501, Train Acc=0.9485 | Val Loss=8.8041, Val Acc=0.0903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "best_val_acc = 0.0\n",
    "best_model_path = \"/kaggle/working/best_efficientnet_rgb_pretrain.pth\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_avg_loss = train_loss / train_total\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_avg_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_avg_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_avg_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # Lưu model tốt nhất\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'label2idx': train_dataset.label2idx,\n",
    "            'classes': train_dataset.classes,\n",
    "            'epoch': epoch + 1,\n",
    "            'val_acc': val_acc,\n",
    "        }, best_model_path)\n",
    "        print(f\"✅ Saved best model (Epoch {epoch+1}, Val Acc: {val_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c0005",
   "metadata": {
    "papermill": {
     "duration": 0.205745,
     "end_time": "2025-06-02T10:01:02.003891",
     "exception": false,
     "start_time": "2025-06-02T10:01:01.798146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Tải lại kiến trúc EfficientNet\n",
    "model = models.efficientnet_b0(weights=None)  # Không cần pre-trained weights\n",
    "\n",
    "# Load checkpoint đã lưu trước đó\n",
    "checkpoint = torch.load(\"/kaggle/working/best_efficientnet_rgb_pretrain.pth\", map_location=device)\n",
    "\n",
    "# Chỉ load phần feature extractor\n",
    "model.features.load_state_dict(checkpoint['model_state_dict'], strict=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1589971,
     "sourceId": 2632847,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7523697,
     "sourceId": 11964981,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7532459,
     "sourceId": 11977607,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5508.423575,
   "end_time": "2025-06-02T10:01:04.967224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-02T08:29:16.543649",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
