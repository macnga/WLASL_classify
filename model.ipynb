{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12059660,"sourceType":"datasetVersion","datasetId":7590419},{"sourceId":12072209,"sourceType":"datasetVersion","datasetId":7599152}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom collections import defaultdict\n\ninput_txt = '/kaggle/input/keypoint/videoid_label.txt'\nout_train = '/kaggle/working/train.txt'\nout_val = '/kaggle/working/val.txt'\nout_test = '/kaggle/working/test.txt'\n\n#os.makedirs(out_train, exist_ok=True)\n#os.makedirs(out_val, exist_ok=True)\n#os.makedirs(out_test, exist_ok=True)\n# Đọc file và gom theo nhãn\nlabel_dict = defaultdict(list)\nwith open(input_txt, 'r') as f:\n    for line in f:\n        line = line.strip()\n        if not line: continue\n        video_id, label = line.split()\n        label_dict[label].append(video_id)\n\n# Chia tỉ lệ 6:4:4 cho từng nhãn\ntrain_lines, val_lines, test_lines = [], [], []\nfor label, vids in label_dict.items():\n    vids = list(vids)\n    random.shuffle(vids)\n    n = len(vids)\n    n_train = round(n * 0.6)\n    n_val = round(n * 0.2)\n    n_test = n - n_train - n_val\n    train = vids[:n_train]\n    val = vids[n_train:n_train+n_val]\n    test = vids[n_train+n_val:]\n    train_lines.extend([f\"{vid} {label}\\n\" for vid in train])\n    val_lines.extend([f\"{vid} {label}\\n\" for vid in val])\n    test_lines.extend([f\"{vid} {label}\\n\" for vid in test])\n\n# Shuffle lại từng tập để tránh cùng nhãn đứng liền nhau\nrandom.shuffle(train_lines)\nrandom.shuffle(val_lines)\nrandom.shuffle(test_lines)\n\n# Lưu file\nwith open(out_train, 'w') as f: f.writelines(train_lines)\nwith open(out_val, 'w') as f: f.writelines(val_lines)\nwith open(out_test, 'w') as f: f.writelines(test_lines)\n\nprint(\"Đã chia xong. Train:\", len(train_lines), \"Val:\", len(val_lines), \"Test:\", len(test_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.633006Z","iopub.execute_input":"2025-06-08T14:16:54.633641Z","iopub.status.idle":"2025-06-08T14:16:54.649061Z","shell.execute_reply.started":"2025-06-08T14:16:54.633603Z","shell.execute_reply":"2025-06-08T14:16:54.648335Z"}},"outputs":[{"name":"stdout","text":"Đã chia xong. Train: 456 Val: 145 Test: 150\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1. Pretrain Vision encoder","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport torchvision\nfrom tqdm import tqdm\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.650561Z","iopub.execute_input":"2025-06-08T14:16:54.650794Z","iopub.status.idle":"2025-06-08T14:16:54.664214Z","shell.execute_reply.started":"2025-06-08T14:16:54.650769Z","shell.execute_reply":"2025-06-08T14:16:54.663682Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_label_map(txt_files):\n    labels = set()\n    for txt in txt_files:\n        with open(txt, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) >= 2:\n                    label = ' '.join(parts[1:]).strip()\n                    labels.add(label)\n    labels = sorted(labels)\n    label_map = {lbl: idx for idx, lbl in enumerate(labels)}\n    return label_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.665010Z","iopub.execute_input":"2025-06-08T14:16:54.665204Z","iopub.status.idle":"2025-06-08T14:16:54.680867Z","shell.execute_reply.started":"2025-06-08T14:16:54.665184Z","shell.execute_reply":"2025-06-08T14:16:54.680080Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_root = \"/kaggle/input/image-mask/cropped_hands\"\ntrain_txt = \"/kaggle/working/train.txt\"\nvalid_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 6\nlr = 1e-3\nout_ckpt = \"/kaggle/working/pretrained_hand_rgb.pth\"\nout_labelmap = \"/kaggle/working/label_map.json\"\n\nlabel_map = build_label_map([train_txt, valid_txt, test_txt])\nnum_classes = len(label_map)\nprint(\"Số lớp:\", num_classes)\nprint(\"Sample label_map:\", dict(list(label_map.items())[:5]))\n\nwith open(out_labelmap, \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.681619Z","iopub.execute_input":"2025-06-08T14:16:54.681888Z","iopub.status.idle":"2025-06-08T14:16:54.701548Z","shell.execute_reply.started":"2025-06-08T14:16:54.681866Z","shell.execute_reply":"2025-06-08T14:16:54.700825Z"}},"outputs":[{"name":"stdout","text":"Số lớp: 32\nSample label_map: {'all': 0, 'before': 1, 'black': 2, 'book': 3, 'candy': 4}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==== 2. Dataset for cropped hand images ====\nclass HandImageDataset(Dataset):\n    def __init__(self, data_root, list_file, label_map, transform=None):\n        self.samples = []\n        self.transform = transform\n        with open(list_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    video_id, label = parts\n                    label_idx = label_map[label]\n                    #img_dir = os.path.join(data_root, video_id)\n                    # All *_left.jpg and *_right.jpg (can add filter for frame sampling if needed)\n                    imgs = sorted(glob(os.path.join(data_root, f\"{video_id}_*_left.jpg\"))) + \\\n                           sorted(glob(os.path.join(data_root, f\"{video_id}_*_right.jpg\")))\n                    for img_path in imgs:\n                        if os.path.isfile(img_path):\n                            self.samples.append((img_path, label_idx))\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.702863Z","iopub.execute_input":"2025-06-08T14:16:54.703045Z","iopub.status.idle":"2025-06-08T14:16:54.716298Z","shell.execute_reply.started":"2025-06-08T14:16:54.703031Z","shell.execute_reply":"2025-06-08T14:16:54.715614Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\ntrain_ds = HandImageDataset(data_root, train_txt, label_map, transform=transform)\nval_ds = HandImageDataset(data_root, valid_txt, label_map, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\nprint(f\"Số ảnh train: {len(train_ds)}, Số ảnh val: {len(val_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:16:54.716993Z","iopub.execute_input":"2025-06-08T14:16:54.717213Z","iopub.status.idle":"2025-06-08T14:17:06.929975Z","shell.execute_reply.started":"2025-06-08T14:16:54.717192Z","shell.execute_reply":"2025-06-08T14:17:06.929202Z"}},"outputs":[{"name":"stdout","text":"Số ảnh train: 2266, Số ảnh val: 732\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==== 3. Vision Model ====\nclass VisionClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        backbone = torchvision.models.efficientnet_b0(pretrained=True)\n        self.features = backbone.features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(1280, num_classes)\n    def forward(self, x):\n        feat = self.features(x)\n        feat = self.pool(feat).view(x.size(0), -1)\n        return self.classifier(feat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:17:06.930799Z","iopub.execute_input":"2025-06-08T14:17:06.931026Z","iopub.status.idle":"2025-06-08T14:17:06.936273Z","shell.execute_reply.started":"2025-06-08T14:17:06.931007Z","shell.execute_reply":"2025-06-08T14:17:06.935732Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = VisionClassifier(num_classes)\nif torch.cuda.device_count() > 1:\n    print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n    model = nn.DataParallel(model)\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:17:06.937773Z","iopub.execute_input":"2025-06-08T14:17:06.937976Z","iopub.status.idle":"2025-06-08T14:17:07.656729Z","shell.execute_reply.started":"2025-06-08T14:17:06.937962Z","shell.execute_reply":"2025-06-08T14:17:07.655895Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 168MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Using DataParallel with 2 GPUs\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"best_val_acc = 0\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for imgs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\", leave=False):\n        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n    train_acc = correct / total\n    train_loss = total_loss / total\n\n    # Validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=\"Valid\", leave=False):\n            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += imgs.size(0)\n    val_acc = val_correct / val_total\n    val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n    # Save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model\": model.state_dict(),\n            \"label_map\": label_map\n        }, out_ckpt)\n        print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\nprint(\"Done. Best val acc:\", best_val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:17:07.657522Z","iopub.execute_input":"2025-06-08T14:17:07.657742Z","iopub.status.idle":"2025-06-08T14:18:30.295586Z","shell.execute_reply.started":"2025-06-08T14:17:07.657726Z","shell.execute_reply":"2025-06-08T14:18:30.294613Z"}},"outputs":[{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 3.0918 Acc: 0.1703 | Val Loss: 2.9388 Acc: 0.1967\nBest model saved at epoch 1, val_acc=0.1967\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/6 | Train Loss: 2.4080 Acc: 0.3455 | Val Loss: 2.8100 Acc: 0.2842\nBest model saved at epoch 2, val_acc=0.2842\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/6 | Train Loss: 1.9299 Acc: 0.4590 | Val Loss: 2.9182 Acc: 0.2732\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/6 | Train Loss: 1.5556 Acc: 0.5556 | Val Loss: 2.8235 Acc: 0.3265\nBest model saved at epoch 4, val_acc=0.3265\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/6 | Train Loss: 1.2423 Acc: 0.6390 | Val Loss: 3.0118 Acc: 0.3183\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch 6/6 | Train Loss: 0.9920 Acc: 0.7039 | Val Loss: 3.3368 Acc: 0.3156\nDone. Best val acc: 0.32650273224043713\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 2. pretrain STGCN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass PoseSpatialPartDataset(Dataset):\n    def __init__(self, data_root, txt_file, label_map, part='body'):\n        self.samples = []\n        self.label_map = label_map\n        self.part = part\n        with open(txt_file) as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    print(f\"WARNING: dòng bị lỗi format: {line}\")\n                    continue\n                npy_id = parts[0]\n                label = ' '.join(parts[1:]).strip()\n                if label not in label_map:\n                    print(f\"WARNING: label '{label}' chưa có trong label_map!\")\n                    continue\n                npy_path = f\"{data_root}/{npy_id}_keypoint.npy\"\n                self.samples.append((npy_path, int(label_map[label])))\n        # Define keypoint slices for each part\n        if part == 'body':\n            self.idx_start, self.idx_end = 0, 25\n        elif part == 'left':\n            self.idx_start, self.idx_end = 25, 46\n        elif part == 'right':\n            self.idx_start, self.idx_end = 46, 67\n        else:\n            raise ValueError(\"Unknown part: \" + part)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        npy_path, label = self.samples[idx]\n        keypoints = np.load(npy_path)  # (T, 67, 3) expected\n        # Auto fix shape if needed\n        if keypoints.shape[-2:] == (67, 3):\n            pass\n        elif keypoints.shape[0] == 67 and keypoints.shape[1] == 3:\n            keypoints = np.transpose(keypoints, (2, 0, 1))\n        elif keypoints.shape[1] == 3 and keypoints.shape[2] == 67:\n            keypoints = np.transpose(keypoints, (0, 2, 1))\n        else:\n            raise RuntimeError(f\"Unrecognized keypoints shape: {keypoints.shape}\")\n        part_kp = keypoints[:, self.idx_start:self.idx_end, :]  # (T, N, 3)\n        if idx == 0:\n            print(f\"Dataset part_kp.shape: {part_kp.shape}\")\n        return torch.tensor(part_kp, dtype=torch.float32), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.296599Z","iopub.execute_input":"2025-06-08T14:18:30.296877Z","iopub.status.idle":"2025-06-08T14:18:30.307126Z","shell.execute_reply.started":"2025-06-08T14:18:30.296854Z","shell.execute_reply":"2025-06-08T14:18:30.306470Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SpatialGCNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, A):\n        super().__init__()\n        self.register_buffer('A', A)\n        self.fc = nn.Linear(in_channels, out_channels)\n        self.bn = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):  # x: (B, N, in_channels)\n        h = self.fc(x)  # (B, N, out_channels)\n        h = h.permute(0, 2, 1)  # (B, out_channels, N)\n        h = torch.matmul(h, self.A)  # (B, out_channels, N)\n        h = self.bn(h)  # BatchNorm trên out_channels\n        h = h.permute(0, 2, 1)  # (B, N, out_channels)\n        return torch.relu(h)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.307745Z","iopub.execute_input":"2025-06-08T14:18:30.307926Z","iopub.status.idle":"2025-06-08T14:18:30.323604Z","shell.execute_reply.started":"2025-06-08T14:18:30.307912Z","shell.execute_reply":"2025-06-08T14:18:30.322933Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class SpatialPoseEncoder(nn.Module):\n    def __init__(self, in_channels, num_joints, num_classes, A, hid_dim=128, out_dim=256):\n        super().__init__()\n        self.gcn1 = SpatialGCNLayer(in_channels, hid_dim, A)\n        self.gcn2 = SpatialGCNLayer(hid_dim, out_dim, A)\n        self.classifier = nn.Linear(out_dim * num_joints, num_classes)\n\n    def forward(self, x):  # x: (B, T, N, 3)\n        #print(\"Encoder x.shape:\", x.shape)\n        B, T, N, C = x.shape\n        assert N == self.gcn1.A.shape[0], f\"x.shape={x.shape}, A.shape={self.gcn1.A.shape}\"\n        x = x.view(B * T, N, C)  # (B*T, N, 3)\n        h = self.gcn1(x)         # (B*T, N, hid_dim)\n        h = self.gcn2(h)         # (B*T, N, out_dim)\n        h = h.view(B, T, N, -1)  # (B, T, N, out_dim)\n        h = h.mean(1)            # (B, N, out_dim)\n        h = h.permute(0, 2, 1)   # (B, out_dim, N)\n        logits = self.classifier(h.flatten(1))  # (B, num_classes)\n        return logits, h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.324348Z","iopub.execute_input":"2025-06-08T14:18:30.324551Z","iopub.status.idle":"2025-06-08T14:18:30.338990Z","shell.execute_reply.started":"2025-06-08T14:18:30.324537Z","shell.execute_reply":"2025-06-08T14:18:30.338482Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def gather_special_frames(pose_feat, mask_indices):\n    \"\"\"\n    pose_feat: (B, C, N, T)\n    mask_indices: list of [tensor(F_b,), ...]  # F_b: số frame của mỗi sample cần fusion\n    Return: list of (B, C, N, F_b)\n    \"\"\"\n    outputs = []\n    for b, idxs in enumerate(mask_indices):\n        # idxs: (F_b,), pose_feat[b]: (C, N, T)\n        sel = pose_feat[b, :, :, idxs]  # (C, N, F_b)\n        outputs.append(sel)\n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.339581Z","iopub.execute_input":"2025-06-08T14:18:30.339739Z","iopub.status.idle":"2025-06-08T14:18:30.351464Z","shell.execute_reply.started":"2025-06-08T14:18:30.339727Z","shell.execute_reply":"2025-06-08T14:18:30.350842Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def get_spatial_adjacency(num_node, edge):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[i, j] = 1\n        A[j, i] = 1\n    # Normalize\n    Dl = np.sum(A, 0)\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i] ** (-1)\n    A_normalized = np.dot(A, Dn)\n    return torch.tensor(A_normalized, dtype=torch.float32)\n\ndef get_body_spatial_graph():\n    # 25 body keypoints (Mediapipe hoặc OpenPose định nghĩa)\n    num_node = 25\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1), (1, 2), (2, 3), (3, 7),\n        (0, 4), (4, 5), (5, 6), (6, 8),\n        (9, 10), (11, 12), (11, 13), (13, 15), (15, 21), (15, 19), (15, 17),\n        (17, 19), (11, 23), (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),\n        (18, 20), (12, 24), (23, 24)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\n\ndef get_left_hand_spatial_graph():\n    # 21 left hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\ndef get_right_hand_spatial_graph():\n    # 21 right hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.352063Z","iopub.execute_input":"2025-06-08T14:18:30.352232Z","iopub.status.idle":"2025-06-08T14:18:30.363023Z","shell.execute_reply.started":"2025-06-08T14:18:30.352219Z","shell.execute_reply":"2025-06-08T14:18:30.362316Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"PART_INFO = {\n    'body':  (0, 25),\n    'left':  (25, 46),\n    'right': (46, 67)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.365345Z","iopub.execute_input":"2025-06-08T14:18:30.365566Z","iopub.status.idle":"2025-06-08T14:18:30.378848Z","shell.execute_reply.started":"2025-06-08T14:18:30.365551Z","shell.execute_reply":"2025-06-08T14:18:30.378124Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef train_pose_spatial_part(part, num_joints, get_A_func, best_ckpt_file):\n    print(f\"\\n--- Pretraining {part} ---\")\n    train_ds = PoseSpatialPartDataset(data_root, train_txt, label_map, part=part)\n    val_ds = PoseSpatialPartDataset(data_root, val_txt, label_map, part=part)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    print(f\"Số mẫu train: {len(train_ds)}, val: {len(val_ds)}\")\n    print(f\"Số batch train: {len(train_loader)}, val: {len(val_loader)}\")\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    A = get_A_func().to(device)\n    model = SpatialPoseEncoder(in_channels=3, num_joints=num_joints, num_classes=num_classes, A=A)\n    if torch.cuda.device_count() > 1:\n        print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    best_val_acc = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for x, labels in tqdm(train_loader, desc=f\"Train {part} Epoch {epoch+1}\", leave=False):\n            #print(\"Batch x.shape:\", x.shape)\n            x, labels = x.to(device), labels.to(device)\n            logits, _ = model(x)\n            loss = criterion(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * x.size(0)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += x.size(0)\n        if total > 0:\n            train_acc = correct / total\n            train_loss = total_loss / total\n        else:\n            train_acc = 0\n            train_loss = 0\n            print(\"WARNING: Không có sample nào trong batch train!\")\n        \n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for x, labels in tqdm(val_loader, desc=f\"Val {part} Epoch {epoch+1}\", leave=False):\n                x, labels = x.to(device), labels.to(device)\n                logits, _ = model(x)\n                loss = criterion(logits, labels)\n                val_loss += loss.item() * x.size(0)\n                preds = logits.argmax(1)\n                val_correct += (preds == labels).sum().item()\n                val_total += x.size(0)\n        if val_total > 0:\n            val_acc = val_correct / val_total\n            val_loss = val_loss / val_total\n        else:\n            val_acc = 0\n            val_loss = 0\n            print(\"WARNING: Không có sample nào trong batch val!\")\n\n        print(f\"[{part}] Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\"model\": model.state_dict(), \"label_map\": label_map}, best_ckpt_file)\n            print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\n    print(f\"Done {part}. Best val acc: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.379524Z","iopub.execute_input":"2025-06-08T14:18:30.379698Z","iopub.status.idle":"2025-06-08T14:18:30.399821Z","shell.execute_reply.started":"2025-06-08T14:18:30.379685Z","shell.execute_reply":"2025-06-08T14:18:30.399125Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"data_root = \"/kaggle/input/keypoint/keypoints\"\ntrain_txt = \"/kaggle/working/train.txt\"\nval_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 10\nlr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)\nwith open(\"label_map_pose.json\", \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.400456Z","iopub.execute_input":"2025-06-08T14:18:30.400669Z","iopub.status.idle":"2025-06-08T14:18:30.417113Z","shell.execute_reply.started":"2025-06-08T14:18:30.400655Z","shell.execute_reply":"2025-06-08T14:18:30.416489Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(get_body_spatial_graph().shape)       # (25, 25)\nprint(get_left_hand_spatial_graph().shape)  # (21, 21)\nprint(get_right_hand_spatial_graph().shape) # (21, 21)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.417858Z","iopub.execute_input":"2025-06-08T14:18:30.418135Z","iopub.status.idle":"2025-06-08T14:18:30.439221Z","shell.execute_reply.started":"2025-06-08T14:18:30.418111Z","shell.execute_reply":"2025-06-08T14:18:30.438721Z"}},"outputs":[{"name":"stdout","text":"torch.Size([25, 25])\ntorch.Size([21, 21])\ntorch.Size([21, 21])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_pose_spatial_part('body',  num_joints=25, get_A_func=get_body_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_body_best.pth\")\ntrain_pose_spatial_part('left',  num_joints=21, get_A_func=get_left_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_left_best.pth\")\ntrain_pose_spatial_part('right', num_joints=21, get_A_func=get_right_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_right_best.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:30.439823Z","iopub.execute_input":"2025-06-08T14:18:30.440017Z","iopub.status.idle":"2025-06-08T14:18:48.465663Z","shell.execute_reply.started":"2025-06-08T14:18:30.440004Z","shell.execute_reply":"2025-06-08T14:18:48.464811Z"}},"outputs":[{"name":"stdout","text":"\n--- Pretraining body ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n  return F.linear(input, self.weight, self.bias)\nVal body Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 1/10 | Train Loss: 4.3657 Acc: 0.0614 | Val Loss: 3.3577 Acc: 0.0621\nBest model saved at epoch 1, val_acc=0.0621\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 2/10 | Train Loss: 3.2634 Acc: 0.1469 | Val Loss: 3.4562 Acc: 0.0690\nBest model saved at epoch 2, val_acc=0.0690\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 3/10 | Train Loss: 2.8637 Acc: 0.1930 | Val Loss: 3.2576 Acc: 0.1517\nBest model saved at epoch 3, val_acc=0.1517\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 4/10 | Train Loss: 2.6601 Acc: 0.2237 | Val Loss: 3.2883 Acc: 0.1448\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 5:   7%|▋         | 1/15 [00:00<00:02,  6.71it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 5/10 | Train Loss: 2.5869 Acc: 0.2566 | Val Loss: 3.2973 Acc: 0.1172\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 6:   7%|▋         | 1/15 [00:00<00:02,  6.27it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 6/10 | Train Loss: 2.4920 Acc: 0.2544 | Val Loss: 3.2158 Acc: 0.1655\nBest model saved at epoch 6, val_acc=0.1655\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 7/10 | Train Loss: 2.4000 Acc: 0.2719 | Val Loss: 3.3411 Acc: 0.1793\nBest model saved at epoch 7, val_acc=0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 8:   7%|▋         | 1/15 [00:00<00:02,  6.77it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 8/10 | Train Loss: 2.3434 Acc: 0.3114 | Val Loss: 3.2132 Acc: 0.1724\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 9/10 | Train Loss: 2.1409 Acc: 0.3355 | Val Loss: 3.2905 Acc: 0.2207\nBest model saved at epoch 9, val_acc=0.2207\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 10:   7%|▋         | 1/15 [00:00<00:02,  6.69it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 10/10 | Train Loss: 2.1077 Acc: 0.3509 | Val Loss: 3.1557 Acc: 0.2000\nDone body. Best val acc: 0.2207\n\n--- Pretraining left ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 1/10 | Train Loss: 3.8045 Acc: 0.0658 | Val Loss: 3.3602 Acc: 0.0690\nBest model saved at epoch 1, val_acc=0.0690\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 2:   7%|▋         | 1/15 [00:00<00:02,  6.38it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 2/10 | Train Loss: 3.4826 Acc: 0.0899 | Val Loss: 3.3589 Acc: 0.0828\nBest model saved at epoch 2, val_acc=0.0828\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 3:   7%|▋         | 1/15 [00:00<00:02,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 3/10 | Train Loss: 3.2417 Acc: 0.1140 | Val Loss: 3.1763 Acc: 0.0828\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 4:   7%|▋         | 1/15 [00:00<00:02,  6.56it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 4/10 | Train Loss: 2.9599 Acc: 0.1513 | Val Loss: 3.1629 Acc: 0.1931\nBest model saved at epoch 4, val_acc=0.1931\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 5/10 | Train Loss: 2.9665 Acc: 0.1469 | Val Loss: 3.0735 Acc: 0.1724\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 6/10 | Train Loss: 2.8759 Acc: 0.1842 | Val Loss: 3.2153 Acc: 0.1448\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 7/10 | Train Loss: 2.8054 Acc: 0.1667 | Val Loss: 2.9121 Acc: 0.2138\nBest model saved at epoch 7, val_acc=0.2138\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 8/10 | Train Loss: 2.8123 Acc: 0.1645 | Val Loss: 3.0690 Acc: 0.1862\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 9/10 | Train Loss: 2.7550 Acc: 0.1974 | Val Loss: 2.9618 Acc: 0.2276\nBest model saved at epoch 9, val_acc=0.2276\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 10:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 10/10 | Train Loss: 2.6778 Acc: 0.1930 | Val Loss: 3.0419 Acc: 0.2552\nBest model saved at epoch 10, val_acc=0.2552\nDone left. Best val acc: 0.2552\n\n--- Pretraining right ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 1/10 | Train Loss: 3.9503 Acc: 0.0548 | Val Loss: 3.6890 Acc: 0.0483\nBest model saved at epoch 1, val_acc=0.0483\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 2/10 | Train Loss: 3.2540 Acc: 0.1535 | Val Loss: 4.0165 Acc: 0.0414\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 3/10 | Train Loss: 2.8737 Acc: 0.2193 | Val Loss: 3.3095 Acc: 0.1103\nBest model saved at epoch 3, val_acc=0.1103\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 4:   7%|▋         | 1/15 [00:00<00:02,  5.70it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 4/10 | Train Loss: 2.6515 Acc: 0.2259 | Val Loss: 2.8566 Acc: 0.2276\nBest model saved at epoch 4, val_acc=0.2276\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 5/10 | Train Loss: 2.4515 Acc: 0.3004 | Val Loss: 2.9559 Acc: 0.2207\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 6/10 | Train Loss: 2.3108 Acc: 0.3180 | Val Loss: 2.6523 Acc: 0.2759\nBest model saved at epoch 6, val_acc=0.2759\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 7/10 | Train Loss: 2.1734 Acc: 0.3575 | Val Loss: 2.4173 Acc: 0.3724\nBest model saved at epoch 7, val_acc=0.3724\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 8/10 | Train Loss: 2.0875 Acc: 0.3618 | Val Loss: 2.3656 Acc: 0.3448\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 9/10 | Train Loss: 2.0713 Acc: 0.3794 | Val Loss: 2.4705 Acc: 0.4069\nBest model saved at epoch 9, val_acc=0.4069\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 10:   7%|▋         | 1/15 [00:00<00:02,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 ","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 10/10 | Train Loss: 2.0224 Acc: 0.3860 | Val Loss: 2.4860 Acc: 0.3241\nDone right. Best val acc: 0.4069\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# 3. WLASL Module","metadata":{}},{"cell_type":"markdown","source":"# 3.1. Fusion Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DeformableAttention2D(nn.Module):\n    \"\"\"\n    Deformable 2D attention:\n    - query: (B, N, d_model)     # N: số keypoints\n    - key, value: (B, C, H, W)   # feature map từ RGB backbone\n    - ref_points: (B, N, 2)      # reference keypoint position (pixel, normalized [0,1])\n    \"\"\"\n    def __init__(self, d_model, n_heads, n_points=4):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_points = n_points\n\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Conv2d(d_model, d_model, 1)\n        self.value_proj = nn.Conv2d(d_model, d_model, 1)\n\n        # For each head, predict offset for each reference point (dx, dy) per query\n        self.offset = nn.Linear(d_model, n_heads * n_points * 2)\n        # For each head, predict attention weights for each sampled point\n        self.attn_weight = nn.Linear(d_model, n_heads * n_points)\n\n    def forward(self, query, key, value, ref_points):\n        # query: (B, N, d_model)\n        # key, value: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature size\n        B, N, d_model = query.shape\n        H, W = key.shape[2], key.shape[3]\n\n        # Project query, key, value\n        query_proj = self.query_proj(query)  # (B, N, d_model)\n        key_proj = self.key_proj(key)        # (B, d_model, H, W)\n        value_proj = self.value_proj(value)  # (B, d_model, H, W)\n\n        # Predict offsets and attention weights\n        offset = self.offset(query_proj)     # (B, N, n_heads*n_points*2)\n        offset = offset.view(B, N, self.n_heads, self.n_points, 2)\n\n        attn_weight = self.attn_weight(query_proj)  # (B, N, n_heads*n_points)\n        attn_weight = attn_weight.view(B, N, self.n_heads, self.n_points)\n        attn_weight = F.softmax(attn_weight, dim=-1)\n\n        # Calculate sampling locations (normalized in [0, 1])\n        # ref_points: (B, N, 2), offset: (B, N, n_heads, n_points, 2)\n        # Sampling positions: (B, N, n_heads, n_points, 2)\n        sampling_locations = ref_points.unsqueeze(2).unsqueeze(3) + offset / torch.tensor([W, H], device=offset.device)\n        # Clamp to [0, 1]\n        sampling_locations = sampling_locations.clamp(0, 1)\n\n        # Prepare for grid_sample: scale to [-1, 1]\n        # grid_sample expects normalized coords in [-1, 1]\n        sampling_grid = sampling_locations.clone()\n        sampling_grid[..., 0] = sampling_grid[..., 0] * 2 - 1\n        sampling_grid[..., 1] = sampling_grid[..., 1] * 2 - 1\n\n        # Sample value features at sampled locations\n        # value_proj: (B, d_model, H, W)\n        # For each query (B, N, n_heads, n_points, 2), sample (B, d_model, n_heads, n_points)\n        sampled_feats = []\n        for b in range(B):\n            feats = []\n            for n in range(N):\n                grid = sampling_grid[b, n]  # (n_heads, n_points, 2)\n                grid = grid.view(1, -1, 1, 2)  # (1, n_heads*n_points, 1, 2)\n                # grid_sample: input (B, C, H, W), grid (B, out_H*out_W, 1, 2)\n                sampled = F.grid_sample(\n                    value_proj[b:b+1], grid, mode='bilinear', align_corners=True\n                )  # (1, d_model, n_heads*n_points, 1)\n                sampled = sampled.view(d_model, self.n_heads, self.n_points)\n                feats.append(sampled)\n            # feats: list of N tensors (d_model, n_heads, n_points) -> (N, d_model, n_heads, n_points)\n            feats = torch.stack(feats, dim=0)\n            sampled_feats.append(feats)\n        # (B, N, d_model, n_heads, n_points)\n        sampled_feats = torch.stack(sampled_feats, dim=0)\n\n        # Weighted sum by attention weights\n        # attn_weight: (B, N, n_heads, n_points)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # (B, N, n_points, n_heads)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # back to (B, N, n_heads, n_points)\n        # (B, N, d_model, n_heads, n_points) * (B, N, 1, n_heads, n_points) -> sum over n_points\n        out = (sampled_feats * attn_weight.unsqueeze(2)).sum(-1)  # (B, N, d_model, n_heads)\n        out = out.mean(-1)  # mean over heads: (B, N, d_model)\n\n        return out  # (B, N, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:48.466726Z","iopub.execute_input":"2025-06-08T14:18:48.467021Z","iopub.status.idle":"2025-06-08T14:18:48.479094Z","shell.execute_reply.started":"2025-06-08T14:18:48.466997Z","shell.execute_reply":"2025-06-08T14:18:48.478335Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class PGFModule(nn.Module):\n    \"\"\"\n    PGF Module: Multi-head deformable attention for fusion keypoint + RGB feature\n    - query_feat: (B, N, d_model) -- e.g. STGCN output per joint\n    - rgb_feat: (B, d_model, H, W) -- output feature map from CNN/ViT\n    - ref_points: (B, N, 2) -- reference position (normalized [0,1]) for each joint\n    Output: (B, N, d_model) fused feature per joint\n    \"\"\"\n    def __init__(self, d_model, n_heads=4, n_points=4):\n        super().__init__()\n        self.attn = DeformableAttention2D(d_model, n_heads, n_points)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, query_feat, rgb_feat, ref_points):\n        # query_feat: (B, N, d_model)\n        # rgb_feat: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature map size\n        fused = self.attn(query_feat, rgb_feat, rgb_feat, ref_points)  # (B, N, d_model)\n        out = self.layer_norm(fused + query_feat)  # residual connection\n        return out  # (B, N, d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:48.479916Z","iopub.execute_input":"2025-06-08T14:18:48.480205Z","iopub.status.idle":"2025-06-08T14:18:48.497272Z","shell.execute_reply.started":"2025-06-08T14:18:48.480187Z","shell.execute_reply":"2025-06-08T14:18:48.496722Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=400):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x: (B, T, d_model)\n        return x + self.pe[:, :x.size(1)]\n\nclass TemporalEncoder(nn.Module):\n    \"\"\"\n    Temporal encoder as in UniSign: Transformer-based sequence encoder\n    Input: (B, T, N, d)  --> (B, T, d)\n    \"\"\"\n    def __init__(self, d_model, nhead=8, num_layers=2, dim_feedforward=512, dropout=0.1, max_len=400, pool='mean'):\n        super().__init__()\n        self.pool = pool\n        self.position_encoding = PositionalEncoding(d_model, max_len)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        # x: (B, T, N, d)\n        # UniSign: Thường tổng hợp theo joints (mean), còn nếu muốn giữ riêng body/left/right thì tách theo index rồi pooling từng phần\n        x = x.mean(dim=2)  # (B, T, d), mean theo joints\n        x = self.position_encoding(x)\n        x = self.transformer(x)  # (B, T, d)\n        if self.pool == 'mean':\n            x = x.mean(dim=1)   # (B, d)\n        elif self.pool == 'last':\n            x = x[:, -1, :]     # (B, d)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:48.498067Z","iopub.execute_input":"2025-06-08T14:18:48.498299Z","iopub.status.idle":"2025-06-08T14:18:48.512702Z","shell.execute_reply.started":"2025-06-08T14:18:48.498279Z","shell.execute_reply":"2025-06-08T14:18:48.512109Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nimport numpy as np\nimport json\n\n# Đường dẫn đến các thư mục\nDATA_ROOT = '/kaggle/input'\nCROPPED_HANDS_DIR = os.path.join(DATA_ROOT, 'image-mask', 'cropped_hands')\nKEYPOINT_DIR = os.path.join(DATA_ROOT, 'keypoint', 'keypoints')\nMASK_DIR = CROPPED_HANDS_DIR\n\nSPLIT_TXT = {\n    'train': '/kaggle/working/train.txt',\n    'val': '/kaggle/working/val.txt',\n    'test': '/kaggle/working/test.txt'\n}\n\n# Đọc label_map\nwith open('/kaggle/working/label_map.json') as f:\n    label_map = json.load(f)\n\ndef build_meta_list(split_txt_path):\n    meta_list = []\n    with open(split_txt_path, 'r') as f:\n        lines = f.readlines()\n    for line in lines:\n        video_id, label_str = line.strip().split()\n        label = label_map[label_str]  # Lấy label id từ label_map\n\n        # Keypoint path\n        keypoint_path = os.path.join(KEYPOINT_DIR, f\"{video_id}_keypoint.npy\")\n        # Mask\n        mask_left_path = os.path.join(MASK_DIR, f\"{video_id}_mask_left.npy\")\n        mask_right_path = os.path.join(MASK_DIR, f\"{video_id}_mask_right.npy\")\n\n        mask_left = np.load(mask_left_path)\n        mask_right = np.load(mask_right_path)\n        num_frames = len(mask_left)\n\n        crop_left_files = []\n        kp_j_left_files = []\n        for idx in range(num_frames):\n            if mask_left[idx] == 1:\n                crop_left_files.append(os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left.jpg\"))\n                kp_j_left_files.append(os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left_kp.npy\"))\n        crop_right_files = []\n        kp_j_right_files = []\n        for idx in range(num_frames):\n            if mask_right[idx] == 1:\n                crop_right_files.append(os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right.jpg\"))\n                kp_j_right_files.append(os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right_kp.npy\"))\n\n        meta = {\n            \"video_id\": video_id,\n            \"keypoint_path\": keypoint_path,\n            \"label\": label,\n            \"num_frames\": num_frames,\n            \"mask_left_path\": mask_left_path,\n            \"mask_right_path\": mask_right_path,\n            \"crop_left_files\": crop_left_files,\n            \"kp_j_left_files\": kp_j_left_files,\n            \"crop_right_files\": crop_right_files,\n            \"kp_j_right_files\": kp_j_right_files\n        }\n        meta_list.append(meta)\n    return meta_list\n\n# Build cho các tập\nfor split, txt_path in SPLIT_TXT.items():\n    meta_list = build_meta_list(txt_path)\n    with open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n        json.dump(meta_list, f, indent=2)\n    print(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:48.513302Z","iopub.execute_input":"2025-06-08T14:18:48.513523Z","iopub.status.idle":"2025-06-08T14:18:53.620250Z","shell.execute_reply.started":"2025-06-08T14:18:48.513508Z","shell.execute_reply":"2025-06-08T14:18:53.619662Z"}},"outputs":[{"name":"stdout","text":"Saved train_meta.json with 456 samples\nSaved val_meta.json with 145 samples\nSaved test_meta.json with 150 samples\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass ClassifySign(nn.Module):\n    \"\"\"\n    Đầu vào:\n        - keypoint: (B, T, N, 3)\n        - rgb_imgs: (B, T, 3, H, W)\n        - vision_encoder: backbone trích đặc trưng ảnh (pretrained)\n        - stgcn_body, stgcn_left, stgcn_right: các backbone STGCN đã pretrain cho từng phần\n        - PGFModule: khối fusion ảnh & keypoint cho left/right hand\n        - temporal_encoder_body, temporal_encoder_hand: temporal encoder cho body, left, right\n        - FCN: phân loại đầu ra với dropout để tránh overfit\n    \"\"\"\n\n    def __init__(self, vision_encoder, stgcn_body, stgcn_left, stgcn_right,\n                 pgf_module, temporal_encoder_body, temporal_encoder_hand, \n                 num_classes,\n                 left_idx=21, right_idx=21, body_idx=25,\n                 fcn_hidden=256, dropout=0.5):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.stgcn_body = stgcn_body\n        self.stgcn_left = stgcn_left\n        self.stgcn_right = stgcn_right\n        self.pgf_module = pgf_module\n        self.temporal_encoder_body = temporal_encoder_body\n        self.temporal_encoder_hand = temporal_encoder_hand\n        out_dim_body = temporal_encoder_body.output_dim\n        out_dim_hand = temporal_encoder_hand.output_dim\n        self.fcn = nn.Sequential(\n            nn.Linear(out_dim_body + 2 * out_dim_hand, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, num_classes)\n        )\n        self.left_idx = left_idx\n        self.right_idx = right_idx\n        self.body_idx = body_idx\n\n    def forward(self, keypoint, rgb_imgs, mask_left, mask_right, \n                kp_j_left, kp_j_right, rgb_left_imgs, rgb_right_imgs):\n        \"\"\"\n        keypoint: (B, T, N, 3)\n        rgb_imgs: (B, T, 3, H, W)\n        mask_left, mask_right: (B, T)  # 1: dùng fusion, 0: bỏ qua fusion\n        kp_j_left, kp_j_right: (B, T, K, 2)\n        rgb_left_imgs, rgb_right_imgs: (B, T, 3, H, W)\n        \"\"\"\n        B, T, N, C = keypoint.shape\n\n        # 1. Chia keypoint thành 3 phần\n        kp_body = keypoint[..., self.body_idx, :]      # (B, T, Nb, 3)\n        kp_left = keypoint[..., self.left_idx, :]      # (B, T, Nl, 3)\n        kp_right = keypoint[..., self.right_idx, :]    # (B, T, Nr, 3)\n\n        # 2. Body: Chuỗi toàn bộ frames -> STGCN -> TemporalEncoder\n        body_feat = self.stgcn_body(kp_body)           # (B, T, d)\n        body_out = self.temporal_encoder_body(body_feat)  # (B, d_body)\n\n        # 3. Xử lý left hand\n        left_feats = []\n        for t in range(T):\n            frame_kp = kp_left[:, t, :, :]             # (B, Nl, 3)\n            mask = mask_left[:, t]                     # (B,)\n            img = rgb_left_imgs[:, t, :, :, :]         # (B, 3, H, W)\n            ref_j = kp_j_left[:, t, :, :]              # (B, Kl, 2)\n            stgcn_feat = self.stgcn_left(frame_kp)     # (B, d)\n            stgcn_feat = stgcn_feat.unsqueeze(1)       # (B, 1, d)\n            rgb_feat = self.vision_encoder(img)        # (B, d, H', W') hoặc (B, d)\n            fused = []\n            for b in range(B):\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        stgcn_feat[b:b+1], rgb_feat[b:b+1], ref_j[b:b+1])\n                    fused.append(fused_feat.squeeze(0))\n                else:\n                    fused.append(stgcn_feat[b, 0])\n            left_feats.append(torch.stack(fused, dim=0))\n        left_feats = torch.stack(left_feats, dim=1)\n        left_out = self.temporal_encoder_hand(left_feats)  # (B, d_hand)\n\n        # 4. Xử lý right hand (Tương tự)\n        right_feats = []\n        for t in range(T):\n            frame_kp = kp_right[:, t, :, :]\n            mask = mask_right[:, t]\n            img = rgb_right_imgs[:, t, :, :, :]\n            ref_j = kp_j_right[:, t, :, :]\n            stgcn_feat = self.stgcn_right(frame_kp)\n            stgcn_feat = stgcn_feat.unsqueeze(1)\n            rgb_feat = self.vision_encoder(img)\n            fused = []\n            for b in range(B):\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        stgcn_feat[b:b+1], rgb_feat[b:b+1], ref_j[b:b+1])\n                    fused.append(fused_feat.squeeze(0))\n                else:\n                    fused.append(stgcn_feat[b, 0])\n            right_feats.append(torch.stack(fused, dim=0))\n        right_feats = torch.stack(right_feats, dim=1)\n        right_out = self.temporal_encoder_hand(right_feats)  # (B, d_hand)\n\n        # 5. Ghép 3 feature\n        final_feat = torch.cat([body_out, left_out, right_out], dim=-1)  # (B, d_body+2*d_hand)\n        logits = self.fcn(final_feat)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:53.621514Z","iopub.execute_input":"2025-06-08T14:18:53.621756Z","iopub.status.idle":"2025-06-08T14:18:53.634761Z","shell.execute_reply.started":"2025-06-08T14:18:53.621738Z","shell.execute_reply":"2025-06-08T14:18:53.634144Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclass SignDataset(Dataset):\n    \"\"\"\n    Dataset cho mô hình ClassifySign với logic:\n    - Ảnh crop/J chỉ lấy ở frame có mask==1, mapping đúng vị trí mask==1, còn lại (mask==0) điền zeros.\n    \"\"\"\n    def __init__(\n        self, meta_list, transform_crop=None,\n        max_seq=64, crop_shape=(3, 112, 112), kp_num=21\n    ):\n        \"\"\"\n        meta_list: list các dict, mỗi dict gồm:\n            'video_id'\n            'keypoint_path'\n            'label'\n            'mask_left_path'\n            'mask_right_path'\n            'crop_left_files': list file ảnh left (đúng thứ tự mask==1)\n            'crop_right_files': list file ảnh right (đúng thứ tự mask==1)\n            'kp_j_left_files': list file J left (đúng thứ tự mask==1)\n            'kp_j_right_files': list file J right (đúng thứ tự mask==1)\n            'num_frames'\n        transform_crop: transform ảnh crop\n        crop_shape: shape cho ảnh zeros (C, H, W)\n        kp_num: số keypoint bàn tay (J)\n        \"\"\"\n        self.meta_list = meta_list\n        self.transform_crop = transform_crop\n        self.max_seq = max_seq\n        self.crop_shape = crop_shape\n        self.kp_num = kp_num\n\n    def __len__(self):\n        return len(self.meta_list)\n\n    def __getitem__(self, idx):\n        meta = self.meta_list[idx]\n        T = meta['num_frames']\n        label = meta['label']\n        keypoint = np.load(meta['keypoint_path'])    # (T, N, 3)\n        mask_left = np.load(meta['mask_left_path'])  # (T,)\n        mask_right = np.load(meta['mask_right_path'])# (T,)\n\n        # Chuẩn bị ảnh/J left/right (len đúng bằng số frame T)\n        crop_left_imgs, kp_j_left = [], []\n        left_idx = 0\n        for t in range(T):\n            if mask_left[t] == 1:\n                img = Image.open(meta['crop_left_files'][left_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = np.load(meta['kp_j_left_files'][left_idx])\n                j = torch.tensor(j, dtype=torch.float32)\n                left_idx += 1\n            else:\n                img = torch.zeros(self.crop_shape, dtype=torch.float32)\n                j = torch.zeros(self.kp_num, 2, dtype=torch.float32)\n            crop_left_imgs.append(img)\n            kp_j_left.append(j)\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)   # (T, C, H, W)\n        kp_j_left = torch.stack(kp_j_left, dim=0)             # (T, kp_num, 2)\n\n        crop_right_imgs, kp_j_right = [], []\n        right_idx = 0\n        for t in range(T):\n            if mask_right[t] == 1:\n                img = Image.open(meta['crop_right_files'][right_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = np.load(meta['kp_j_right_files'][right_idx])\n                j = torch.tensor(j, dtype=torch.float32)\n                right_idx += 1\n            else:\n                img = torch.zeros(self.crop_shape, dtype=torch.float32)\n                j = torch.zeros(self.kp_num, 2, dtype=torch.float32)\n            crop_right_imgs.append(img)\n            kp_j_right.append(j)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n\n        # Nếu cần pad/truncate về self.max_seq\n        def pad_seq(seq, pad, max_len):\n            if len(seq) >= max_len:\n                return seq[:max_len]\n            else:\n                return seq + [pad]*(max_len-len(seq))\n        \n        T_cur = T\n        crop_left_imgs = pad_seq(list(crop_left_imgs), torch.zeros(self.crop_shape), self.max_seq)\n        crop_right_imgs = pad_seq(list(crop_right_imgs), torch.zeros(self.crop_shape), self.max_seq)\n        kp_j_left = pad_seq(list(kp_j_left), torch.zeros(self.kp_num, 2), self.max_seq)\n        kp_j_right = pad_seq(list(kp_j_right), torch.zeros(self.kp_num, 2), self.max_seq)\n        mask_left = np.pad(mask_left, (0, max(self.max_seq - T_cur, 0)), 'constant')\n        mask_right = np.pad(mask_right, (0, max(self.max_seq - T_cur, 0)), 'constant')\n        keypoint = np.pad(keypoint, ((0, max(self.max_seq - T_cur, 0)), (0,0), (0,0)), 'constant')\n\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_left = torch.stack(kp_j_left, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n        keypoint = torch.tensor(keypoint, dtype=torch.float32)\n        mask_left = torch.tensor(mask_left, dtype=torch.float32)\n        mask_right = torch.tensor(mask_right, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.long)\n\n        return {\n            'keypoint': keypoint,             # (T, N, 3)\n            'mask_left': mask_left,           # (T,)\n            'mask_right': mask_right,         # (T,)\n            'crop_left_imgs': crop_left_imgs, # (T, C, H, W)\n            'crop_right_imgs': crop_right_imgs,\n            'kp_j_left': kp_j_left,           # (T, kp_num, 2)\n            'kp_j_right': kp_j_right,\n            'label': label\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:53.635570Z","iopub.execute_input":"2025-06-08T14:18:53.636214Z","iopub.status.idle":"2025-06-08T14:18:53.660014Z","shell.execute_reply.started":"2025-06-08T14:18:53.636191Z","shell.execute_reply":"2025-06-08T14:18:53.659304Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport os\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef collate_fn(batch):\n    \"\"\"\n    Batch là list các dict từ Dataset, ghép lại thành tensor.\n    \"\"\"\n    batch_out = {}\n    for k in batch[0].keys():\n        if isinstance(batch[0][k], torch.Tensor):\n            batch_out[k] = torch.stack([item[k] for item in batch], dim=0)\n        else:\n            # Nếu là int/float, convert sang tensor\n            arr = [item[k] for item in batch]\n            if isinstance(arr[0], (int, float)):\n                batch_out[k] = torch.tensor(arr)\n            else:\n                batch_out[k] = arr\n    return batch_out\n\ndef train_epoch(model, loader, optimizer, criterion, device=\"cuda\"):\n    model.train()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch in loader:\n        for k in batch:\n            if isinstance(batch[k], torch.Tensor):\n                batch[k] = batch[k].to(device)\n        optimizer.zero_grad()\n        logits = model(\n            keypoint=batch['keypoint'],\n            rgb_imgs=None,  # Nếu không dùng, truyền None hoặc bỏ field này khỏi model\n            mask_left=batch['mask_left'],\n            mask_right=batch['mask_right'],\n            kp_j_left=batch['kp_j_left'],\n            kp_j_right=batch['kp_j_right'],\n            rgb_left_imgs=batch['crop_left_imgs'],\n            rgb_right_imgs=batch['crop_right_imgs']\n        )\n        loss = criterion(logits, batch['label'])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch['label'].size(0)\n        pred = logits.argmax(dim=1)\n        total_acc += (pred == batch['label']).sum().item()\n        n += batch['label'].size(0)\n    return total_loss / n, total_acc / n\n\n@torch.no_grad()\ndef validate_epoch(model, loader, criterion, device=\"cuda\"):\n    model.eval()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch in loader:\n        for k in batch:\n            if isinstance(batch[k], torch.Tensor):\n                batch[k] = batch[k].to(device)\n        logits = model(\n            keypoint=batch['keypoint'],\n            rgb_imgs=None,\n            mask_left=batch['mask_left'],\n            mask_right=batch['mask_right'],\n            kp_j_left=batch['kp_j_left'],\n            kp_j_right=batch['kp_j_right'],\n            rgb_left_imgs=batch['crop_left_imgs'],\n            rgb_right_imgs=batch['crop_right_imgs']\n        )\n        loss = criterion(logits, batch['label'])\n        total_loss += loss.item() * batch['label'].size(0)\n        pred = logits.argmax(dim=1)\n        total_acc += (pred == batch['label']).sum().item()\n        n += batch['label'].size(0)\n    return total_loss / n, total_acc / n\n\ndef save_checkpoint(model, optimizer, epoch, path, best_acc=None):\n    state = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    if best_acc is not None:\n        state['best_acc'] = best_acc\n    torch.save(state, path)\n\ndef load_checkpoint(model, optimizer, path, device='cuda'):\n    state = torch.load(path, map_location=device)\n    model.load_state_dict(state['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    epoch = state.get('epoch', 0)\n    best_acc = state.get('best_acc', None)\n    return model, optimizer, epoch, best_acc\n\ndef adjust_learning_rate(optimizer, epoch, lr, step=10, decay=0.1):\n    \"\"\" Step LR decay \"\"\"\n    lr_new = lr * (decay ** (epoch // step))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr_new\n    return lr_new","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:18:53.660831Z","iopub.execute_input":"2025-06-08T14:18:53.661041Z","iopub.status.idle":"2025-06-08T14:18:53.681025Z","shell.execute_reply.started":"2025-06-08T14:18:53.661018Z","shell.execute_reply":"2025-06-08T14:18:53.680464Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import json\n#from data.sign_dataset import SignDataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\n# Load meta_list\nwith open('/kaggle/working/train_meta.json') as f:\n    train_meta_list = json.load(f)\nwith open('/kaggle/working/val_meta.json') as f:\n    val_meta_list = json.load(f)\n\ntrain_ds = SignDataset(meta_list=train_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\nval_ds = SignDataset(meta_list=val_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\n\n#from utils.train_utils import collate_fn\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\nval_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T14:19:18.605358Z","iopub.execute_input":"2025-06-08T14:19:18.606092Z","iopub.status.idle":"2025-06-08T14:19:18.618732Z","shell.execute_reply.started":"2025-06-08T14:19:18.606068Z","shell.execute_reply":"2025-06-08T14:19:18.618061Z"}},"outputs":[],"execution_count":27}]}