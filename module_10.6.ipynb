{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3153248,"sourceType":"datasetVersion","datasetId":1903686},{"sourceId":12059660,"sourceType":"datasetVersion","datasetId":7590419},{"sourceId":12072209,"sourceType":"datasetVersion","datasetId":7599152}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom collections import defaultdict\n\ninput_txt = '/kaggle/input/keypoint/videoid_label.txt'\nout_train = '/kaggle/working/train.txt'\nout_val = '/kaggle/working/val.txt'\nout_test = '/kaggle/working/test.txt'\n\n#os.makedirs(out_train, exist_ok=True)\n#os.makedirs(out_val, exist_ok=True)\n#os.makedirs(out_test, exist_ok=True)\n# Đọc file và gom theo nhãn\nlabel_dict = defaultdict(list)\nwith open(input_txt, 'r') as f:\n    for line in f:\n        line = line.strip()\n        if not line: continue\n        video_id, label = line.split()\n        label_dict[label].append(video_id)\n\n# Chia tỉ lệ 6:4:4 cho từng nhãn\ntrain_lines, val_lines, test_lines = [], [], []\nfor label, vids in label_dict.items():\n    vids = list(vids)\n    random.shuffle(vids)\n    n = len(vids)\n    n_train = round(n * 0.6)\n    n_val = round(n * 0.2)\n    n_test = n - n_train - n_val\n    train = vids[:n_train]\n    val = vids[n_train:n_train+n_val]\n    test = vids[n_train+n_val:]\n    train_lines.extend([f\"{vid} {label}\\n\" for vid in train])\n    val_lines.extend([f\"{vid} {label}\\n\" for vid in val])\n    test_lines.extend([f\"{vid} {label}\\n\" for vid in test])\n\n# Shuffle lại từng tập để tránh cùng nhãn đứng liền nhau\nrandom.shuffle(train_lines)\nrandom.shuffle(val_lines)\nrandom.shuffle(test_lines)\n\n# Lưu file\nwith open(out_train, 'w') as f: f.writelines(train_lines)\nwith open(out_val, 'w') as f: f.writelines(val_lines)\nwith open(out_test, 'w') as f: f.writelines(test_lines)\n\nprint(\"Đã chia xong. Train:\", len(train_lines), \"Val:\", len(val_lines), \"Test:\", len(test_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:42:59.440907Z","iopub.execute_input":"2025-06-10T03:42:59.441205Z","iopub.status.idle":"2025-06-10T03:42:59.454008Z","shell.execute_reply.started":"2025-06-10T03:42:59.441184Z","shell.execute_reply":"2025-06-10T03:42:59.453343Z"}},"outputs":[{"name":"stdout","text":"Đã chia xong. Train: 456 Val: 145 Test: 150\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# 1. Pretrain Vision encoder","metadata":{}},{"cell_type":"code","source":"import os\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport torchvision\nfrom tqdm import tqdm\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:43:07.773814Z","iopub.execute_input":"2025-06-10T03:43:07.774148Z","iopub.status.idle":"2025-06-10T03:43:07.779525Z","shell.execute_reply.started":"2025-06-10T03:43:07.774124Z","shell.execute_reply":"2025-06-10T03:43:07.778813Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def build_label_map(txt_files):\n    labels = set()\n    for txt in txt_files:\n        with open(txt, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) >= 2:\n                    label = ' '.join(parts[1:]).strip()\n                    labels.add(label)\n    labels = sorted(labels)\n    label_map = {lbl: idx for idx, lbl in enumerate(labels)}\n    return label_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:43:09.094061Z","iopub.execute_input":"2025-06-10T03:43:09.094747Z","iopub.status.idle":"2025-06-10T03:43:09.099971Z","shell.execute_reply.started":"2025-06-10T03:43:09.094719Z","shell.execute_reply":"2025-06-10T03:43:09.099022Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"data_root = \"/kaggle/input/image-mask/cropped_hands\"\ntrain_txt = \"/kaggle/working/train.txt\"\nvalid_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 6\nlr = 1e-3\nout_ckpt = \"/kaggle/working/pretrained_hand_rgb.pth\"\nout_labelmap = \"/kaggle/working/label_map.json\"\n\nlabel_map = build_label_map([train_txt, valid_txt, test_txt])\nnum_classes = len(label_map)\nprint(\"Số lớp:\", num_classes)\nprint(\"Sample label_map:\", dict(list(label_map.items())[:5]))\n\nwith open(out_labelmap, \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:02.518620Z","iopub.execute_input":"2025-06-10T03:44:02.518920Z","iopub.status.idle":"2025-06-10T03:44:02.526510Z","shell.execute_reply.started":"2025-06-10T03:44:02.518902Z","shell.execute_reply":"2025-06-10T03:44:02.525845Z"}},"outputs":[{"name":"stdout","text":"Số lớp: 32\nSample label_map: {'all': 0, 'before': 1, 'black': 2, 'book': 3, 'candy': 4}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ==== 2. Dataset for cropped hand images ====\nclass HandImageDataset(Dataset):\n    def __init__(self, data_root, list_file, label_map, transform=None):\n        self.samples = []\n        self.transform = transform\n        with open(list_file, \"r\") as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    video_id, label = parts\n                    label_idx = label_map[label]\n                    #img_dir = os.path.join(data_root, video_id)\n                    # All *_left.jpg and *_right.jpg (can add filter for frame sampling if needed)\n                    imgs = sorted(glob(os.path.join(data_root, f\"{video_id}_*_left.jpg\"))) + \\\n                           sorted(glob(os.path.join(data_root, f\"{video_id}_*_right.jpg\")))\n                    for img_path in imgs:\n                        if os.path.isfile(img_path):\n                            self.samples.append((img_path, label_idx))\n    def __len__(self):\n        return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:05.589294Z","iopub.execute_input":"2025-06-10T03:44:05.589574Z","iopub.status.idle":"2025-06-10T03:44:05.596260Z","shell.execute_reply.started":"2025-06-10T03:44:05.589552Z","shell.execute_reply":"2025-06-10T03:44:05.595419Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\ntrain_ds = HandImageDataset(data_root, train_txt, label_map, transform=transform)\nval_ds = HandImageDataset(data_root, valid_txt, label_map, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\nprint(f\"Số ảnh train: {len(train_ds)}, Số ảnh val: {len(val_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:07.922813Z","iopub.execute_input":"2025-06-10T03:44:07.923699Z","iopub.status.idle":"2025-06-10T03:44:19.587106Z","shell.execute_reply.started":"2025-06-10T03:44:07.923651Z","shell.execute_reply":"2025-06-10T03:44:19.586017Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1012940438.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHandImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2108273086.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_root, list_file, label_map, transform)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;31m# All *_left.jpg and *_right.jpg (can add filter for frame sampling if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{video_id}_*_left.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                            \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{video_id}_*_right.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mzero\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdirectories\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msubdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m     return list(iglob(pathname, root_dir=root_dir, dir_fd=dir_fd, recursive=recursive,\n\u001b[0m\u001b[1;32m     29\u001b[0m                       include_hidden=include_hidden))\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mglob_in_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_glob0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         for name in glob_in_dir(_join(root_dir, dirname), basename, dir_fd, dironly,\n\u001b[0m\u001b[1;32m     98\u001b[0m                                include_hidden=include_hidden):\n\u001b[1;32m     99\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_hidden\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minclude_hidden\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_glob0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/fnmatch.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(names, pat)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# normcase on posix is NOP. Optimize it away from the loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"# ==== 3. Vision Model ====\nclass VisionClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        backbone = torchvision.models.efficientnet_b0(pretrained=True)\n        self.features = backbone.features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(1280, num_classes)\n    def forward(self, x):\n        feat = self.features(x)\n        feat = self.pool(feat).view(x.size(0), -1)\n        return self.classifier(feat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:23.129716Z","iopub.execute_input":"2025-06-10T03:44:23.130069Z","iopub.status.idle":"2025-06-10T03:44:23.135906Z","shell.execute_reply.started":"2025-06-10T03:44:23.130041Z","shell.execute_reply":"2025-06-10T03:44:23.134971Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = VisionClassifier(num_classes)\nif torch.cuda.device_count() > 1:\n    print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n    model = nn.DataParallel(model)\nmodel = model.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:51:38.350084Z","iopub.execute_input":"2025-06-09T03:51:38.350278Z","iopub.status.idle":"2025-06-09T03:51:38.896196Z","shell.execute_reply.started":"2025-06-09T03:51:38.350254Z","shell.execute_reply":"2025-06-09T03:51:38.895663Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 144MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Using DataParallel with 2 GPUs\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"best_val_acc = 0\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for imgs, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\", leave=False):\n        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n    train_acc = correct / total\n    train_loss = total_loss / total\n\n    # Validation\n    model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    with torch.no_grad():\n        for imgs, labels in tqdm(val_loader, desc=\"Valid\", leave=False):\n            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n            val_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += imgs.size(0)\n    val_acc = val_correct / val_total\n    val_loss = val_loss / val_total\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n\n    # Save best\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            \"model\": model.state_dict(),\n            \"label_map\": label_map\n        }, out_ckpt)\n        print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\nprint(\"Done. Best val acc:\", best_val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:51:38.896929Z","iopub.execute_input":"2025-06-09T03:51:38.897139Z","iopub.status.idle":"2025-06-09T03:53:03.508669Z","shell.execute_reply.started":"2025-06-09T03:51:38.897124Z","shell.execute_reply":"2025-06-09T03:53:03.507719Z"}},"outputs":[{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 3.0069 Acc: 0.1904 | Val Loss: 3.0029 Acc: 0.1866\nBest model saved at epoch 1, val_acc=0.1866\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/6 | Train Loss: 2.2842 Acc: 0.3689 | Val Loss: 3.0003 Acc: 0.2493\nBest model saved at epoch 2, val_acc=0.2493\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/6 | Train Loss: 1.8309 Acc: 0.4877 | Val Loss: 3.1770 Acc: 0.2563\nBest model saved at epoch 3, val_acc=0.2563\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/6 | Train Loss: 1.4783 Acc: 0.5706 | Val Loss: 3.0427 Acc: 0.2869\nBest model saved at epoch 4, val_acc=0.2869\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/6 | Train Loss: 1.1804 Acc: 0.6531 | Val Loss: 3.4547 Acc: 0.2646\n","output_type":"stream"},{"name":"stderr","text":"                                                              ","output_type":"stream"},{"name":"stdout","text":"Epoch 6/6 | Train Loss: 0.9957 Acc: 0.7171 | Val Loss: 3.5249 Acc: 0.2758\nDone. Best val acc: 0.28690807799442897\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 2. pretrain STGCN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass PoseSpatialPartDataset(Dataset):\n    def __init__(self, data_root, txt_file, label_map, part='body'):\n        self.samples = []\n        self.label_map = label_map\n        self.part = part\n        with open(txt_file) as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) < 2:\n                    print(f\"WARNING: dòng bị lỗi format: {line}\")\n                    continue\n                npy_id = parts[0]\n                label = ' '.join(parts[1:]).strip()\n                if label not in label_map:\n                    print(f\"WARNING: label '{label}' chưa có trong label_map!\")\n                    continue\n                npy_path = f\"{data_root}/{npy_id}_keypoint.npy\"\n                self.samples.append((npy_path, int(label_map[label])))\n        # Define keypoint slices for each part\n        if part == 'body':\n            self.idx_start, self.idx_end = 0, 25\n        elif part == 'left':\n            self.idx_start, self.idx_end = 25, 46\n        elif part == 'right':\n            self.idx_start, self.idx_end = 46, 67\n        else:\n            raise ValueError(\"Unknown part: \" + part)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        npy_path, label = self.samples[idx]\n        keypoints = np.load(npy_path)  # (T, 67, 3) expected\n        # Auto fix shape if needed\n        if keypoints.shape[-2:] == (67, 3):\n            pass\n        elif keypoints.shape[0] == 67 and keypoints.shape[1] == 3:\n            keypoints = np.transpose(keypoints, (2, 0, 1))\n        elif keypoints.shape[1] == 3 and keypoints.shape[2] == 67:\n            keypoints = np.transpose(keypoints, (0, 2, 1))\n        else:\n            raise RuntimeError(f\"Unrecognized keypoints shape: {keypoints.shape}\")\n        part_kp = keypoints[:, self.idx_start:self.idx_end, :]  # (T, N, 3)\n        if idx == 0:\n            print(f\"Dataset part_kp.shape: {part_kp.shape}\")\n        return torch.tensor(part_kp, dtype=torch.float32), label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:29.961228Z","iopub.execute_input":"2025-06-10T03:44:29.961510Z","iopub.status.idle":"2025-06-10T03:44:29.972159Z","shell.execute_reply.started":"2025-06-10T03:44:29.961488Z","shell.execute_reply":"2025-06-10T03:44:29.971365Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class SpatialGCNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, A):\n        super().__init__()\n        self.register_buffer('A', A)\n        self.fc = nn.Linear(in_channels, out_channels)\n        self.bn = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):  # x: (B, N, in_channels)\n        h = self.fc(x)  # (B, N, out_channels)\n        h = h.permute(0, 2, 1)  # (B, out_channels, N)\n        h = torch.matmul(h, self.A)  # (B, out_channels, N)\n        h = self.bn(h)  # BatchNorm trên out_channels\n        h = h.permute(0, 2, 1)  # (B, N, out_channels)\n        return torch.relu(h)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:32.978114Z","iopub.execute_input":"2025-06-10T03:44:32.978688Z","iopub.status.idle":"2025-06-10T03:44:32.983755Z","shell.execute_reply.started":"2025-06-10T03:44:32.978639Z","shell.execute_reply":"2025-06-10T03:44:32.982832Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class SpatialPoseEncoder(nn.Module):\n    def __init__(self, in_channels, num_joints, num_classes, A, hid_dim=128, out_dim=256):\n        super().__init__()\n        self.gcn1 = SpatialGCNLayer(in_channels, hid_dim, A)\n        self.gcn2 = SpatialGCNLayer(hid_dim, out_dim, A)\n        self.classifier = nn.Linear(out_dim * num_joints, num_classes)\n\n    def forward(self, x):  # x: (B, T, N, 3)\n        #print(\"Encoder x.shape:\", x.shape)\n        B, T, N, C = x.shape\n        assert N == self.gcn1.A.shape[0], f\"x.shape={x.shape}, A.shape={self.gcn1.A.shape}\"\n        x = x.view(B * T, N, C)  # (B*T, N, 3)\n        h = self.gcn1(x)         # (B*T, N, hid_dim)\n        h = self.gcn2(h)         # (B*T, N, out_dim)\n        h = h.view(B, T, N, -1)  # (B, T, N, out_dim)\n        h = h.mean(1)            # (B, N, out_dim)\n        h = h.permute(0, 2, 1)   # (B, out_dim, N)\n        logits = self.classifier(h.flatten(1))  # (B, num_classes)\n        return logits, h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:34.672787Z","iopub.execute_input":"2025-06-10T03:44:34.673070Z","iopub.status.idle":"2025-06-10T03:44:34.679408Z","shell.execute_reply.started":"2025-06-10T03:44:34.673048Z","shell.execute_reply":"2025-06-10T03:44:34.678715Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def gather_special_frames(pose_feat, mask_indices):\n    \"\"\"\n    pose_feat: (B, C, N, T)\n    mask_indices: list of [tensor(F_b,), ...]  # F_b: số frame của mỗi sample cần fusion\n    Return: list of (B, C, N, F_b)\n    \"\"\"\n    outputs = []\n    for b, idxs in enumerate(mask_indices):\n        # idxs: (F_b,), pose_feat[b]: (C, N, T)\n        sel = pose_feat[b, :, :, idxs]  # (C, N, F_b)\n        outputs.append(sel)\n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:35.877952Z","iopub.execute_input":"2025-06-10T03:44:35.878711Z","iopub.status.idle":"2025-06-10T03:44:35.883013Z","shell.execute_reply.started":"2025-06-10T03:44:35.878674Z","shell.execute_reply":"2025-06-10T03:44:35.882120Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def get_spatial_adjacency(num_node, edge):\n    A = np.zeros((num_node, num_node))\n    for i, j in edge:\n        A[i, j] = 1\n        A[j, i] = 1\n    # Normalize\n    Dl = np.sum(A, 0)\n    Dn = np.zeros((num_node, num_node))\n    for i in range(num_node):\n        if Dl[i] > 0:\n            Dn[i, i] = Dl[i] ** (-1)\n    A_normalized = np.dot(A, Dn)\n    return torch.tensor(A_normalized, dtype=torch.float32)\n\ndef get_body_spatial_graph():\n    # 25 body keypoints (Mediapipe hoặc OpenPose định nghĩa)\n    num_node = 25\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1), (1, 2), (2, 3), (3, 7),\n        (0, 4), (4, 5), (5, 6), (6, 8),\n        (9, 10), (11, 12), (11, 13), (13, 15), (15, 21), (15, 19), (15, 17),\n        (17, 19), (11, 23), (12, 14), (14, 16), (16, 18), (16, 20), (16, 22),\n        (18, 20), (12, 24), (23, 24)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\n\ndef get_left_hand_spatial_graph():\n    # 21 left hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)\ndef get_right_hand_spatial_graph():\n    # 21 right hand keypoints (Mediapipe)\n    num_node = 21\n    self_link = [(i, i) for i in range(num_node)]\n    neighbor_link = [\n        (0, 1),(1, 2),(2, 3),(3, 4),\n        (0, 5),(5, 6),(6, 7),(7, 8),\n        (0, 9),(9, 10),(10, 11),(11, 12),\n        (0, 13),(13, 14),(14, 15),(15, 16),\n        (0, 17),(17, 18),(18, 19),(19, 20)\n    ]\n    edge = self_link + neighbor_link\n    return get_spatial_adjacency(num_node, edge)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:37.200040Z","iopub.execute_input":"2025-06-10T03:44:37.200298Z","iopub.status.idle":"2025-06-10T03:44:37.211340Z","shell.execute_reply.started":"2025-06-10T03:44:37.200280Z","shell.execute_reply":"2025-06-10T03:44:37.210518Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"PART_INFO = {\n    'body':  (0, 25),\n    'left':  (25, 46),\n    'right': (46, 67)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:40.041499Z","iopub.execute_input":"2025-06-10T03:44:40.042359Z","iopub.status.idle":"2025-06-10T03:44:40.046211Z","shell.execute_reply.started":"2025-06-10T03:44:40.042334Z","shell.execute_reply":"2025-06-10T03:44:40.045465Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef train_pose_spatial_part(part, num_joints, get_A_func, best_ckpt_file):\n    print(f\"\\n--- Pretraining {part} ---\")\n    train_ds = PoseSpatialPartDataset(data_root, train_txt, label_map, part=part)\n    val_ds = PoseSpatialPartDataset(data_root, val_txt, label_map, part=part)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    print(f\"Số mẫu train: {len(train_ds)}, val: {len(val_ds)}\")\n    print(f\"Số batch train: {len(train_loader)}, val: {len(val_loader)}\")\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    A = get_A_func().to(device)\n    model = SpatialPoseEncoder(in_channels=3, num_joints=num_joints, num_classes=num_classes, A=A)\n    if torch.cuda.device_count() > 1:\n        print(\"Using DataParallel with {} GPUs\".format(torch.cuda.device_count()))\n        model = nn.DataParallel(model)\n    model = model.to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    best_val_acc = 0\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for x, labels in tqdm(train_loader, desc=f\"Train {part} Epoch {epoch+1}\", leave=False):\n            #print(\"Batch x.shape:\", x.shape)\n            x, labels = x.to(device), labels.to(device)\n            logits, _ = model(x)\n            loss = criterion(logits, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item() * x.size(0)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += x.size(0)\n        if total > 0:\n            train_acc = correct / total\n            train_loss = total_loss / total\n        else:\n            train_acc = 0\n            train_loss = 0\n            print(\"WARNING: Không có sample nào trong batch train!\")\n        \n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for x, labels in tqdm(val_loader, desc=f\"Val {part} Epoch {epoch+1}\", leave=False):\n                x, labels = x.to(device), labels.to(device)\n                logits, _ = model(x)\n                loss = criterion(logits, labels)\n                val_loss += loss.item() * x.size(0)\n                preds = logits.argmax(1)\n                val_correct += (preds == labels).sum().item()\n                val_total += x.size(0)\n        if val_total > 0:\n            val_acc = val_correct / val_total\n            val_loss = val_loss / val_total\n        else:\n            val_acc = 0\n            val_loss = 0\n            print(\"WARNING: Không có sample nào trong batch val!\")\n\n        print(f\"[{part}] Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\"model\": model.state_dict(), \"label_map\": label_map}, best_ckpt_file)\n            print(f\"Best model saved at epoch {epoch+1}, val_acc={val_acc:.4f}\")\n\n    print(f\"Done {part}. Best val acc: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:44:40.915061Z","iopub.execute_input":"2025-06-10T03:44:40.915387Z","iopub.status.idle":"2025-06-10T03:44:40.928575Z","shell.execute_reply.started":"2025-06-10T03:44:40.915365Z","shell.execute_reply":"2025-06-10T03:44:40.927719Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"data_root = \"/kaggle/input/keypoint/keypoints\"\ntrain_txt = \"/kaggle/working/train.txt\"\nval_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\nbatch_size = 32\nnum_workers = 4\nnum_epochs = 10\nlr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)\nwith open(\"label_map_pose.json\", \"w\") as f:\n    json.dump(label_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:03.647208Z","iopub.execute_input":"2025-06-09T03:53:03.647450Z","iopub.status.idle":"2025-06-09T03:53:03.670957Z","shell.execute_reply.started":"2025-06-09T03:53:03.647429Z","shell.execute_reply":"2025-06-09T03:53:03.670237Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(get_body_spatial_graph().shape)       # (25, 25)\nprint(get_left_hand_spatial_graph().shape)  # (21, 21)\nprint(get_right_hand_spatial_graph().shape) # (21, 21)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:03.671678Z","iopub.execute_input":"2025-06-09T03:53:03.672000Z","iopub.status.idle":"2025-06-09T03:53:03.700913Z","shell.execute_reply.started":"2025-06-09T03:53:03.671979Z","shell.execute_reply":"2025-06-09T03:53:03.700247Z"}},"outputs":[{"name":"stdout","text":"torch.Size([25, 25])\ntorch.Size([21, 21])\ntorch.Size([21, 21])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_pose_spatial_part('body',  num_joints=25, get_A_func=get_body_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_body_best.pth\")\ntrain_pose_spatial_part('left',  num_joints=21, get_A_func=get_left_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_left_best.pth\")\ntrain_pose_spatial_part('right', num_joints=21, get_A_func=get_right_hand_spatial_graph, best_ckpt_file=\"/kaggle/working/spatial_right_best.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:03.701630Z","iopub.execute_input":"2025-06-09T03:53:03.701809Z","iopub.status.idle":"2025-06-09T03:53:22.573325Z","shell.execute_reply.started":"2025-06-09T03:53:03.701794Z","shell.execute_reply":"2025-06-09T03:53:22.572372Z"}},"outputs":[{"name":"stdout","text":"\n--- Pretraining body ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n  return F.linear(input, self.weight, self.bias)\nTrain body Epoch 1:   7%|▋         | 1/15 [00:00<00:05,  2.67it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]            ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 1/10 | Train Loss: 4.4215 Acc: 0.0768 | Val Loss: 3.4786 Acc: 0.0828\nBest model saved at epoch 1, val_acc=0.0828\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 2/10 | Train Loss: 3.2853 Acc: 0.1513 | Val Loss: 3.2851 Acc: 0.0966\nBest model saved at epoch 2, val_acc=0.0966\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 3/10 | Train Loss: 2.7811 Acc: 0.2105 | Val Loss: 3.1009 Acc: 0.1310\nBest model saved at epoch 3, val_acc=0.1310\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 4/10 | Train Loss: 2.6684 Acc: 0.2456 | Val Loss: 3.1395 Acc: 0.1517\nBest model saved at epoch 4, val_acc=0.1517\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 5/10 | Train Loss: 2.4458 Acc: 0.2807 | Val Loss: 2.9012 Acc: 0.2483\nBest model saved at epoch 5, val_acc=0.2483\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 6/10 | Train Loss: 2.3623 Acc: 0.3399 | Val Loss: 3.3267 Acc: 0.1586\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 7:   7%|▋         | 1/15 [00:00<00:02,  5.71it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 7/10 | Train Loss: 2.3649 Acc: 0.3399 | Val Loss: 3.1095 Acc: 0.2276\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 8/10 | Train Loss: 2.1686 Acc: 0.3575 | Val Loss: 3.0652 Acc: 0.2345\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 9:   7%|▋         | 1/15 [00:00<00:02,  6.78it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 9/10 | Train Loss: 2.2193 Acc: 0.3487 | Val Loss: 3.0835 Acc: 0.2345\n","output_type":"stream"},{"name":"stderr","text":"Train body Epoch 10:   7%|▋         | 1/15 [00:00<00:02,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val body Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 25, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[body] Epoch 10/10 | Train Loss: 2.1658 Acc: 0.3860 | Val Loss: 2.8810 Acc: 0.2483\nDone body. Best val acc: 0.2483\n\n--- Pretraining left ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 1/10 | Train Loss: 4.0836 Acc: 0.0504 | Val Loss: 3.4254 Acc: 0.0552\nBest model saved at epoch 1, val_acc=0.0552\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 2:   7%|▋         | 1/15 [00:00<00:02,  6.77it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 2/10 | Train Loss: 3.2574 Acc: 0.1009 | Val Loss: 3.4425 Acc: 0.0621\nBest model saved at epoch 2, val_acc=0.0621\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 3/10 | Train Loss: 2.9865 Acc: 0.1601 | Val Loss: 3.1755 Acc: 0.0966\nBest model saved at epoch 3, val_acc=0.0966\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 4/10 | Train Loss: 3.0205 Acc: 0.1645 | Val Loss: 3.3075 Acc: 0.1241\nBest model saved at epoch 4, val_acc=0.1241\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 5/10 | Train Loss: 2.8814 Acc: 0.2215 | Val Loss: 2.8851 Acc: 0.1241\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 6/10 | Train Loss: 2.7432 Acc: 0.2259 | Val Loss: 3.0136 Acc: 0.1793\nBest model saved at epoch 6, val_acc=0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 7:   7%|▋         | 1/15 [00:00<00:02,  6.87it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 7/10 | Train Loss: 2.7708 Acc: 0.1974 | Val Loss: 2.9659 Acc: 0.2138\nBest model saved at epoch 7, val_acc=0.2138\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 8/10 | Train Loss: 2.7001 Acc: 0.2039 | Val Loss: 2.9721 Acc: 0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 9/10 | Train Loss: 2.6738 Acc: 0.2215 | Val Loss: 2.7349 Acc: 0.2069\n","output_type":"stream"},{"name":"stderr","text":"Train left Epoch 10:  53%|█████▎    | 8/15 [00:00<00:00, 35.96it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val left Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[left] Epoch 10/10 | Train Loss: 2.5775 Acc: 0.2522 | Val Loss: 2.7471 Acc: 0.2069\nDone left. Best val acc: 0.2138\n\n--- Pretraining right ---\nSố mẫu train: 456, val: 145\nSố batch train: 15, val: 5\nUsing DataParallel with 2 GPUs\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 1:   7%|▋         | 1/15 [00:00<00:02,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 1/10 | Train Loss: 3.9400 Acc: 0.0789 | Val Loss: 3.5538 Acc: 0.0276\nBest model saved at epoch 1, val_acc=0.0276\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 2:   7%|▋         | 1/15 [00:00<00:02,  6.02it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 2/10 | Train Loss: 3.1969 Acc: 0.1645 | Val Loss: 3.7053 Acc: 0.0483\nBest model saved at epoch 2, val_acc=0.0483\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 3/10 | Train Loss: 2.8253 Acc: 0.2039 | Val Loss: 3.2625 Acc: 0.1103\nBest model saved at epoch 3, val_acc=0.1103\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 4/10 | Train Loss: 2.6072 Acc: 0.2675 | Val Loss: 2.8024 Acc: 0.1793\nBest model saved at epoch 4, val_acc=0.1793\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 5/10 | Train Loss: 2.5614 Acc: 0.2785 | Val Loss: 2.7881 Acc: 0.2138\nBest model saved at epoch 5, val_acc=0.2138\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 6:   7%|▋         | 1/15 [00:00<00:02,  6.68it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 6/10 | Train Loss: 2.4573 Acc: 0.2873 | Val Loss: 2.5611 Acc: 0.2828\nBest model saved at epoch 6, val_acc=0.2828\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 7:   7%|▋         | 1/15 [00:00<00:02,  6.13it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 7/10 | Train Loss: 2.3096 Acc: 0.3268 | Val Loss: 2.5289 Acc: 0.3103\nBest model saved at epoch 7, val_acc=0.3103\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 8/10 | Train Loss: 2.0770 Acc: 0.3969 | Val Loss: 2.4767 Acc: 0.2690\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                \r","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 9/10 | Train Loss: 2.0913 Acc: 0.3816 | Val Loss: 2.3523 Acc: 0.3586\nBest model saved at epoch 9, val_acc=0.3586\n","output_type":"stream"},{"name":"stderr","text":"Train right Epoch 10:  47%|████▋     | 7/15 [00:00<00:00, 32.02it/s]","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"Val right Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s]           ","output_type":"stream"},{"name":"stdout","text":"Dataset part_kp.shape: (64, 21, 3)\n","output_type":"stream"},{"name":"stderr","text":"                                                                 ","output_type":"stream"},{"name":"stdout","text":"[right] Epoch 10/10 | Train Loss: 1.9968 Acc: 0.4276 | Val Loss: 2.2870 Acc: 0.3241\nDone right. Best val acc: 0.3586\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# 3. WLASL Module","metadata":{}},{"cell_type":"markdown","source":"# 3.1. Fusion Module","metadata":{}},{"cell_type":"markdown","source":"# sửa lại mask","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T03:53:22.574539Z","iopub.execute_input":"2025-06-09T03:53:22.574789Z","iopub.status.idle":"2025-06-09T03:53:22.869922Z","shell.execute_reply.started":"2025-06-09T03:53:22.574764Z","shell.execute_reply":"2025-06-09T03:53:22.869229Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport numpy as np\nimport glob\nimport cv2\n\ndef get_n_frames(video_path):\n    cap = cv2.VideoCapture(video_path)\n    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    cap.release()\n    return n_frames\n    \ndef get_sorted_indexes(crop_dir, video_id, hand):\n    # hand: 'left' hoặc 'right'\n    pattern = os.path.join(crop_dir, f\"{video_id}_frame*_{hand}.jpg\")\n    files = glob.glob(pattern)\n    idxs = []\n    for f in files:\n        # Lấy index từ tên file: ..._frame<idx>_left.jpg hoặc ..._frame<idx>_right.jpg\n        basename = os.path.basename(f)\n        idx = int(basename.split(\"_frame\")[1].split(f\"_{hand}\")[0])\n        idxs.append(idx)\n    idxs = sorted(list(set(idxs)))\n    return idxs\n\ndef make_mask_for_hand(n_frames, crop_idxs, out_len=64):\n    \"\"\"\n    n_frames: số frame gốc của video\n    crop_idxs: list index frame gốc có ảnh crop tay\n    out_len: số frame muốn nội suy (thường = 64)\n    Đảm bảo số mask[i]=1 đúng bằng số crop_idxs (số ảnh crop)\n    \"\"\"\n    idxs_interp = np.linspace(0, n_frames - 1, out_len)\n    mask = np.zeros(out_len, dtype=np.uint8)\n    used_positions = set()\n    for crop_idx in crop_idxs:\n        # Tìm vị trí chưa được gán nào gần nhất crop_idx\n        distances = np.abs(idxs_interp - crop_idx)\n        for i in np.argsort(distances):\n            if i not in used_positions:\n                mask[i] = 1\n                used_positions.add(i)\n                break\n    return mask\n\ndef save_masks_for_video(video_id, crop_dir, video_dir, out_dir, out_len=64):\n    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n    n_frames = get_n_frames(video_path)\n    idxs_left = get_sorted_indexes(crop_dir, video_id, 'left')\n    idxs_right = get_sorted_indexes(crop_dir, video_id, 'right')\n\n    mask_left = make_mask_for_hand(n_frames, idxs_left, out_len)\n    mask_right = make_mask_for_hand(n_frames, idxs_right, out_len)\n\n    np.save(os.path.join(out_dir, f\"{video_id}_mask_left.npy\"), mask_left)\n    np.save(os.path.join(out_dir, f\"{video_id}_mask_right.npy\"), mask_right)\n    print(f\"Saved masks for {video_id}: left sum={mask_left.sum()}, right sum={mask_right.sum()} (left imgs={len(idxs_left)}, right imgs={len(idxs_right)})\")\n\nCROPPED_HANDS_DIR = \"/kaggle/input/image-mask/cropped_hands\"\nVIDEO_DIR = \"/kaggle/input/wlasl-processed/WLASL/videos\"\nOUT_DIR = '/kaggle/working/mask'\n\nos.makedirs(OUT_DIR, exist_ok=True)\n\ndef get_video_ids_from_txt(txt_path):\n    video_ids = []\n    with open(txt_path, \"r\") as f:\n        for line in f:\n            video_id = line.strip().split()[0]\n            video_ids.append(video_id)\n    return video_ids\n\n# Ví dụ xử lý cho train\ntxt_path = \"/kaggle/input/keypoint/videoid_label.txt\"\nvideo_ids = get_video_ids_from_txt(txt_path)\n\nfor video_id in video_ids:\n    save_masks_for_video(video_id, CROPPED_HANDS_DIR, VIDEO_DIR, OUT_DIR, out_len=64)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nkp = np.load('/kaggle/working/mask/01986_mask_right.npy')\nprint(kp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:35:48.095454Z","iopub.execute_input":"2025-06-10T03:35:48.095844Z","iopub.status.idle":"2025-06-10T03:35:48.101680Z","shell.execute_reply.started":"2025-06-10T03:35:48.095819Z","shell.execute_reply":"2025-06-10T03:35:48.100778Z"}},"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Continue ... ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DeformableAttention2D(nn.Module):\n    \"\"\"\n    Deformable 2D attention:\n    - query: (B, N, d_model)     # N: số keypoints\n    - key, value: (B, C, H, W)   # feature map từ RGB backbone\n    - ref_points: (B, N, 2)      # reference keypoint position (pixel, normalized [0,1])\n    \"\"\"\n    def __init__(self, d_model, n_heads, n_points=4):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.n_points = n_points\n\n        self.query_proj = nn.Linear(d_model, d_model)\n        self.key_proj = nn.Conv2d(d_model, d_model, 1)\n        self.value_proj = nn.Conv2d(d_model, d_model, 1)\n\n        # For each head, predict offset for each reference point (dx, dy) per query\n        self.offset = nn.Linear(d_model, n_heads * n_points * 2)\n        # For each head, predict attention weights for each sampled point\n        self.attn_weight = nn.Linear(d_model, n_heads * n_points)\n\n    def forward(self, query, key, value, ref_points):\n        # query: (B, N, d_model)\n        # key, value: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature size\n        B, N, d_model = query.shape\n        H, W = key.shape[2], key.shape[3]\n\n        # Project query, key, value\n        query_proj = self.query_proj(query)  # (B, N, d_model)\n        key_proj = self.key_proj(key)        # (B, d_model, H, W)\n        value_proj = self.value_proj(value)  # (B, d_model, H, W)\n\n        # Predict offsets and attention weights\n        offset = self.offset(query_proj)     # (B, N, n_heads*n_points*2)\n        offset = offset.view(B, N, self.n_heads, self.n_points, 2)\n\n        attn_weight = self.attn_weight(query_proj)  # (B, N, n_heads*n_points)\n        attn_weight = attn_weight.view(B, N, self.n_heads, self.n_points)\n        attn_weight = F.softmax(attn_weight, dim=-1)\n\n        # Calculate sampling locations (normalized in [0, 1])\n        # ref_points: (B, N, 2), offset: (B, N, n_heads, n_points, 2)\n        # Sampling positions: (B, N, n_heads, n_points, 2)\n        sampling_locations = ref_points.unsqueeze(2).unsqueeze(3) + offset / torch.tensor([W, H], device=offset.device)\n        # Clamp to [0, 1]\n        sampling_locations = sampling_locations.clamp(0, 1)\n\n        # Prepare for grid_sample: scale to [-1, 1]\n        # grid_sample expects normalized coords in [-1, 1]\n        sampling_grid = sampling_locations.clone()\n        sampling_grid[..., 0] = sampling_grid[..., 0] * 2 - 1\n        sampling_grid[..., 1] = sampling_grid[..., 1] * 2 - 1\n\n        # Sample value features at sampled locations\n        # value_proj: (B, d_model, H, W)\n        # For each query (B, N, n_heads, n_points, 2), sample (B, d_model, n_heads, n_points)\n        sampled_feats = []\n        for b in range(B):\n            feats = []\n            for n in range(N):\n                grid = sampling_grid[b, n]  # (n_heads, n_points, 2)\n                grid = grid.view(1, -1, 1, 2)  # (1, n_heads*n_points, 1, 2)\n                # grid_sample: input (B, C, H, W), grid (B, out_H*out_W, 1, 2)\n                sampled = F.grid_sample(\n                    value_proj[b:b+1], grid, mode='bilinear', align_corners=True\n                )  # (1, d_model, n_heads*n_points, 1)\n                sampled = sampled.view(d_model, self.n_heads, self.n_points)\n                feats.append(sampled)\n            # feats: list of N tensors (d_model, n_heads, n_points) -> (N, d_model, n_heads, n_points)\n            feats = torch.stack(feats, dim=0)\n            sampled_feats.append(feats)\n        # (B, N, d_model, n_heads, n_points)\n        sampled_feats = torch.stack(sampled_feats, dim=0)\n\n        # Weighted sum by attention weights\n        # attn_weight: (B, N, n_heads, n_points)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # (B, N, n_points, n_heads)\n        attn_weight = attn_weight.permute(0, 1, 3, 2)  # back to (B, N, n_heads, n_points)\n        # (B, N, d_model, n_heads, n_points) * (B, N, 1, n_heads, n_points) -> sum over n_points\n        out = (sampled_feats * attn_weight.unsqueeze(2)).sum(-1)  # (B, N, d_model, n_heads)\n        out = out.mean(-1)  # mean over heads: (B, N, d_model)\n\n        return out  # (B, N, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:35:53.296358Z","iopub.execute_input":"2025-06-10T03:35:53.296870Z","iopub.status.idle":"2025-06-10T03:35:53.307710Z","shell.execute_reply.started":"2025-06-10T03:35:53.296845Z","shell.execute_reply":"2025-06-10T03:35:53.306817Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PGFModule(nn.Module):\n    \"\"\"\n    PGF Module: Multi-head deformable attention for fusion keypoint + RGB feature\n    - query_feat: (B, N, d_model) -- e.g. STGCN output per joint\n    - rgb_feat: (B, d_model, H, W) -- output feature map from CNN/ViT\n    - ref_points: (B, N, 2) -- reference position (normalized [0,1]) for each joint\n    Output: (B, N, d_model) fused feature per joint\n    \"\"\"\n    def __init__(self, d_model, n_heads=4, n_points=4):\n        super().__init__()\n        self.attn = DeformableAttention2D(d_model, n_heads, n_points)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, query_feat, rgb_feat, ref_points):\n        # query_feat: (B, N, d_model)\n        # rgb_feat: (B, d_model, H, W)\n        # ref_points: (B, N, 2) in [0, 1], normalized by feature map size\n        fused = self.attn(query_feat, rgb_feat, rgb_feat, ref_points)  # (B, N, d_model)\n        out = self.layer_norm(fused + query_feat)  # residual connection\n        return out  # (B, N, d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:35:54.462136Z","iopub.execute_input":"2025-06-10T03:35:54.462384Z","iopub.status.idle":"2025-06-10T03:35:54.467568Z","shell.execute_reply.started":"2025-06-10T03:35:54.462367Z","shell.execute_reply":"2025-06-10T03:35:54.466726Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TemporalEncoder(nn.Module):\n    \"\"\"\n    Temporal encoder cho input dạng (B, T, D), D = C*N (flatten spatial)\n    hoặc (B, T, C, N) nếu muốn giữ spatial riêng.\n    Thông thường dùng (B, T, D) để học đặc trưng temporal toàn khung hình.\n\n    Nếu muốn giữ spatial/joint, sửa lại forward để pooling/attention riêng trên từng joint.\n    \"\"\"\n    def __init__(self, d_model, nhead=8, num_layers=2, dim_feedforward=512, dropout=0.1, pool='mean'):\n        \"\"\"\n        d_model: C*N (nếu flatten spatial), hoặc C nếu giữ từng joint riêng.\n        pool: 'mean' (mean theo T), 'last' (lấy frame cuối), hoặc None (giữ nguyên (B, T, d_model))\n        \"\"\"\n        super().__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n                                                  dim_feedforward=dim_feedforward, \n                                                  batch_first=True, dropout=dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.pool = pool\n        self.output_dim = d_model\n\n    def forward(self, x):\n        \"\"\"\n        x: (B, T, D) (flatten spatial: D=C*N)\n        -> output: (B, D) nếu pool, hoặc (B, T, D) nếu pool=None\n        \"\"\"\n        # Nếu input là (B, T, C, N) thì flatten thành (B, T, C*N)\n        if x.dim() == 4:\n            B, T, C, N = x.shape\n            x = x.view(B, T, C * N)\n        # x: (B, T, D)\n        x = self.transformer(x)  # (B, T, D)\n        if self.pool == 'mean':\n            x = x.mean(dim=1)    # (B, D)\n        elif self.pool == 'last':\n            x = x[:, -1, :]      # (B, D)\n        # else giữ nguyên (B, T, D)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:44:18.953205Z","iopub.execute_input":"2025-06-10T04:44:18.953456Z","iopub.status.idle":"2025-06-10T04:44:18.960373Z","shell.execute_reply.started":"2025-06-10T04:44:18.953440Z","shell.execute_reply":"2025-06-10T04:44:18.959560Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"import os\nimport numpy as np\nimport json\nimport glob\nimport re\n\nDATA_ROOT = '/kaggle/input'\nCROPPED_HANDS_DIR = os.path.join(DATA_ROOT, 'image-mask', 'cropped_hands')\nKEYPOINT_DIR = os.path.join(DATA_ROOT, 'keypoint', 'keypoints')\nMASK_DIR = '/kaggle/working/mask'\n\nSPLIT_TXT = {\n    'train': '/kaggle/working/train.txt',\n    'val': '/kaggle/working/val.txt',\n    'test': '/kaggle/working/test.txt'\n}\n\n# Đọc label_map\nwith open('/kaggle/working/label_map.json') as f:\n    label_map = json.load(f)\n\ndef get_sorted_crop_idxs(crop_dir, video_id, hand):\n    pattern = os.path.join(crop_dir, f\"{video_id}_frame*_{hand}.jpg\")\n    files = glob.glob(pattern)\n    idxs = []\n    for f in files:\n        basename = os.path.basename(f)\n        match = re.search(rf\"{video_id}_frame(\\d+)_{hand}\\.jpg\", basename)\n        if match:\n            idx = int(match.group(1))\n            idxs.append(idx)\n    idxs = sorted(list(set(idxs)))\n    return idxs\n\ndef build_meta_list(split_txt_path):\n    meta_list = []\n    with open(split_txt_path, 'r') as f:\n        lines = f.readlines()\n    for line in lines:\n        video_id, label_str = line.strip().split()\n        label = label_map[label_str]\n\n        keypoint_path = os.path.join(KEYPOINT_DIR, f\"{video_id}_keypoint.npy\")\n        mask_left_path = os.path.join(MASK_DIR, f\"{video_id}_mask_left.npy\")\n        mask_right_path = os.path.join(MASK_DIR, f\"{video_id}_mask_right.npy\")\n        mask_left = np.load(mask_left_path)\n        mask_right = np.load(mask_right_path)\n        num_frames = len(mask_left)\n\n        # Lấy index các frame thực sự có ảnh crop left/right\n        crop_left_idxs = get_sorted_crop_idxs(CROPPED_HANDS_DIR, video_id, 'left')\n        crop_right_idxs = get_sorted_crop_idxs(CROPPED_HANDS_DIR, video_id, 'right')\n\n        # Đếm số lượng 1 trong mask và số lượng ảnh crop\n        n_img_left = len(crop_left_idxs)\n        n_mask_left = int(mask_left.sum())\n        n_img_right = len(crop_right_idxs)\n        n_mask_right = int(mask_right.sum())\n        if n_img_left != n_mask_left:\n            print(f\"[CHECK] video_id={video_id}: LEFT - {n_img_left} images, {n_mask_left} mask=1\")\n        if n_img_right != n_mask_right:\n            print(f\"[CHECK] video_id={video_id}: RIGHT - {n_img_right} images, {n_mask_right} mask=1\")\n\n        crop_left_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left.jpg\") for idx in crop_left_idxs]\n        kp_j_left_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_left_kp.npy\") for idx in crop_left_idxs]\n        crop_right_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right.jpg\") for idx in crop_right_idxs]\n        kp_j_right_files = [os.path.join(CROPPED_HANDS_DIR, f\"{video_id}_frame{idx}_right_kp.npy\") for idx in crop_right_idxs]\n\n        meta = {\n            \"video_id\": video_id,\n            \"keypoint_path\": keypoint_path,\n            \"label\": label,\n            \"num_frames\": num_frames,\n            \"mask_left_path\": mask_left_path,\n            \"mask_right_path\": mask_right_path,\n            \"crop_left_files\": crop_left_files,\n            \"kp_j_left_files\": kp_j_left_files,\n            \"crop_right_files\": crop_right_files,\n            \"kp_j_right_files\": kp_j_right_files\n        }\n        meta_list.append(meta)\n    return meta_list\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:37:46.446185Z","iopub.execute_input":"2025-06-10T03:37:46.446892Z","iopub.status.idle":"2025-06-10T03:37:53.005952Z","shell.execute_reply.started":"2025-06-10T03:37:46.446870Z","shell.execute_reply":"2025-06-10T03:37:53.005131Z"}},"outputs":[{"name":"stdout","text":"Saved train_meta.json with 456 samples\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'train'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:40:40.523495Z","iopub.execute_input":"2025-06-10T03:40:40.524063Z","iopub.status.idle":"2025-06-10T03:40:47.051554Z","shell.execute_reply.started":"2025-06-10T03:40:40.524038Z","shell.execute_reply":"2025-06-10T03:40:47.050780Z"}},"outputs":[{"name":"stdout","text":"Saved train_meta.json with 456 samples\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'test'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:40:51.996922Z","iopub.execute_input":"2025-06-10T03:40:51.997525Z","iopub.status.idle":"2025-06-10T03:40:54.308737Z","shell.execute_reply.started":"2025-06-10T03:40:51.997505Z","shell.execute_reply":"2025-06-10T03:40:54.308078Z"}},"outputs":[{"name":"stdout","text":"Saved test_meta.json with 150 samples\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Ví dụ chạy cho 1 tập:\nsplit = 'val'\ntxt_path = SPLIT_TXT[split]\nmeta_list = build_meta_list(txt_path)\n# Nếu muốn lưu lại:\nwith open(f'/kaggle/working/{split}_meta.json', 'w') as f:\n    json.dump(meta_list, f, indent=2)\nprint(f\"Saved {split}_meta.json with {len(meta_list)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:41:05.611810Z","iopub.execute_input":"2025-06-10T03:41:05.612081Z","iopub.status.idle":"2025-06-10T03:41:07.772184Z","shell.execute_reply.started":"2025-06-10T03:41:05.612063Z","shell.execute_reply":"2025-06-10T03:41:07.771452Z"}},"outputs":[{"name":"stdout","text":"Saved val_meta.json with 145 samples\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ClassifySign(nn.Module):\n    \"\"\"\n    Đầu vào:\n        - keypoint: (B, T, N, 3)\n        - rgb_imgs: (B, T, 3, H, W)\n        - stgcn_body, stgcn_left, stgcn_right: các backbone STGCN đã pretrain cho từng phần\n        - pgf_module: khối fusion ảnh & keypoint cho left/right hand\n        - temporal_encoder_body, temporal_encoder_hand: temporal encoder cho body, left, right\n        - FCN: phân loại đầu ra với dropout để tránh overfit\n    \"\"\"\n    def __init__(self, vision_encoder, stgcn_body, stgcn_left, stgcn_right,\n                 pgf_module, temporal_encoder_body, temporal_encoder_hand, \n                 num_classes,\n                 left_idx=21, right_idx=21, body_idx=25,\n                 fcn_hidden=256, dropout=0.5):\n        super().__init__()\n        self.vision_encoder = vision_encoder\n        self.stgcn_body = stgcn_body\n        self.stgcn_left = stgcn_left\n        self.stgcn_right = stgcn_right\n        self.pgf_module = pgf_module\n        self.temporal_encoder_body = temporal_encoder_body\n        self.temporal_encoder_hand = temporal_encoder_hand\n        out_dim_body = temporal_encoder_body.output_dim\n        out_dim_hand = temporal_encoder_hand.output_dim\n        self.fcn = nn.Sequential(\n            nn.Linear(out_dim_body + 2 * out_dim_hand, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, fcn_hidden),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(fcn_hidden, num_classes)\n        )\n        self.left_idx = left_idx\n        self.right_idx = right_idx\n        self.body_idx = body_idx\n\n    def forward(self, keypoint, rgb_imgs, mask_left, mask_right, \n                kp_j_left, kp_j_right, rgb_left_imgs, rgb_right_imgs):\n        \"\"\"\n        keypoint: (B, T, N, 3)\n        rgb_imgs: (B, T, 3, H, W)\n        mask_left, mask_right: (B, T)\n        kp_j_left, kp_j_right: (B, T, K, 2)\n        rgb_left_imgs, rgb_right_imgs: (B, T, 3, H, W)\n        \"\"\"\n        B, T, N, C = keypoint.shape\n\n        # 1. Chia keypoint thành 3 phần\n        kp_body = keypoint[..., :self.body_idx, :]  # (B, T, Nb, 3)\n        kp_left = keypoint[..., self.body_idx:self.body_idx+self.left_idx, :]  # (B, T, Nl, 3)\n        kp_right = keypoint[..., self.body_idx+self.left_idx:self.body_idx+self.left_idx+self.right_idx, :]  # (B, T, Nr, 3)\n\n        # 2. Body: ST-GCN cho từng frame, stack lại (B, T, C_body, Nb)\n        body_stgcn_feats = []\n        for t in range(T):\n            frame_kp = kp_body[:, t, :, :]           # (B, Nb, 3)\n            _, h = self.stgcn_body(frame_kp)         # (B, C_body, Nb)\n            body_stgcn_feats.append(h)\n        body_stgcn_feats = torch.stack(body_stgcn_feats, dim=1)  # (B, T, C_body, Nb)\n        # Flatten spatial: (B, T, C_body * Nb)\n        #body_stgcn_feats = body_stgcn_feats.flatten(2, 3)        # (B, T, C_body*Nb)\n        body_out = self.temporal_encoder_body(body_stgcn_feats)   # (B, d_body)\n\n        # 3. Left hand: ST-GCN + fusion từng frame, stack lại (B, T, C_hand)\n        left_feats = []\n        for t in range(T):\n            frame_kp = kp_left[:, t, :, :]            # (B, Nl, 3)\n            mask = mask_left[:, t]                    # (B,)\n            img = rgb_left_imgs[:, t, :, :, :]        # (B, 3, H, W)\n            ref_j = kp_j_left[:, t, :, :]             # (B, Kl, 2)\n            _, stgcn_feat = self.stgcn_left(frame_kp) # (B, C_hand, Nl)\n            #stgcn_feat = stgcn_feat.mean(-1)          # (B, C_hand)\n            rgb_feat = self.vision_encoder.features(img)       # (B, d_img)\n            fused = []\n            for b in range(B):\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        stgcn_feat[b:b+1], rgb_feat[b:b+1], ref_j[b:b+1]\n                    )  # (1, C_hand)\n                    fused.append(fused_feat.squeeze(0))\n                else:\n                    fused.append(stgcn_feat[b])\n            left_feats.append(torch.stack(fused, dim=0))  # (B, C_hand)\n        left_feats = torch.stack(left_feats, dim=1)       # (B, T, C_hand)\n        left_out = self.temporal_encoder_hand(left_feats)  # (B, d_hand)\n\n        # 4. Right hand: ST-GCN + fusion từng frame, stack lại (B, T, C_hand)\n        right_feats = []\n        for t in range(T):\n            frame_kp = kp_right[:, t, :, :]            # (B, Nr, 3)\n            mask = mask_right[:, t]                    # (B,)\n            img = rgb_right_imgs[:, t, :, :, :]        # (B, 3, H, W)\n            ref_j = kp_j_right[:, t, :, :]             # (B, Kr, 2)\n            _, stgcn_feat = self.stgcn_right(frame_kp) # (B, C_hand, Nr)\n            #stgcn_feat = stgcn_feat.mean(-1)           # (B, C_hand)\n            rgb_feat = self.vision_encoder.features(img)        # (B, d_img)\n            fused = []\n            for b in range(B):\n                if mask[b] == 1:\n                    fused_feat = self.pgf_module(\n                        stgcn_feat[b:b+1], rgb_feat[b:b+1], ref_j[b:b+1]\n                    )  # (1, C_hand)\n                    fused.append(fused_feat.squeeze(0))\n                else:\n                    fused.append(stgcn_feat[b])\n            right_feats.append(torch.stack(fused, dim=0))  # (B, C_hand)\n        right_feats = torch.stack(right_feats, dim=1)      # (B, T, C_hand)\n        right_out = self.temporal_encoder_hand(right_feats) # (B, d_hand)\n\n        # 5. Ghép 3 feature\n        final_feat = torch.cat([body_out, left_out, right_out], dim=-1)  # (B, d_body+2*d_hand)\n        logits = self.fcn(final_feat)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:23.486245Z","iopub.execute_input":"2025-06-10T04:58:23.486564Z","iopub.status.idle":"2025-06-10T04:58:23.503281Z","shell.execute_reply.started":"2025-06-10T04:58:23.486542Z","shell.execute_reply":"2025-06-10T04:58:23.502406Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom PIL import Image\nimport os\n\nclass SignDataset(Dataset):\n    def __init__(\n        self, meta_list, transform_crop=None,\n        max_seq=64, crop_shape=(3, 112, 112), kp_num=21\n    ):\n        self.meta_list = meta_list\n        self.transform_crop = transform_crop\n        self.max_seq = max_seq\n        self.crop_shape = crop_shape\n        self.kp_num = kp_num\n        # Pre-allocate zeros for padding\n        self.zero_img = torch.zeros(self.crop_shape, dtype=torch.float32)\n        self.zero_j = torch.zeros(self.kp_num, 2, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.meta_list)\n\n    def pad_seq(self, seq, pad, max_len):\n        # seq: list[tensor], pad: tensor\n        if len(seq) >= max_len:\n            return seq[:max_len]\n        else:\n            return seq + [pad]*(max_len-len(seq))\n\n    def __getitem__(self, idx):\n        meta = self.meta_list[idx]\n        T = meta['num_frames']\n        label = meta['label']\n        keypoint = np.load(meta['keypoint_path'])    # (T, N, 3)\n        mask_left = np.load(meta['mask_left_path'])  # (T,)\n        mask_right = np.load(meta['mask_right_path'])# (T,)\n\n        # Check consistency\n        assert len(meta['crop_left_files']) == int(mask_left.sum()), f\"{meta['video_id']}: crop_left_files/mask_left mismatch\"\n        assert len(meta['crop_right_files']) == int(mask_right.sum()), f\"{meta['video_id']}: crop_right_files/mask_right mismatch\"\n        assert len(meta['kp_j_left_files']) == int(mask_left.sum()), f\"{meta['video_id']}: kp_j_left_files/mask_left mismatch\"\n        assert len(meta['kp_j_right_files']) == int(mask_right.sum()), f\"{meta['video_id']}: kp_j_right_files/mask_right mismatch\"\n\n        crop_left_imgs, kp_j_left = [], []\n        left_idx = 0\n        for t in range(T):\n            if mask_left[t] == 1:\n                img = Image.open(meta['crop_left_files'][left_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = torch.tensor(np.load(meta['kp_j_left_files'][left_idx]), dtype=torch.float32)\n                left_idx += 1\n            else:\n                img = self.zero_img\n                j = self.zero_j\n            crop_left_imgs.append(img)\n            kp_j_left.append(j)\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)   # (T, C, H, W)\n        kp_j_left = torch.stack(kp_j_left, dim=0)             # (T, kp_num, 2)\n\n        crop_right_imgs, kp_j_right = [], []\n        right_idx = 0\n        for t in range(T):\n            if mask_right[t] == 1:\n                img = Image.open(meta['crop_right_files'][right_idx]).convert(\"RGB\")\n                if self.transform_crop:\n                    img = self.transform_crop(img)\n                j = torch.tensor(np.load(meta['kp_j_right_files'][right_idx]), dtype=torch.float32)\n                right_idx += 1\n            else:\n                img = self.zero_img\n                j = self.zero_j\n            crop_right_imgs.append(img)\n            kp_j_right.append(j)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n\n        # Pad/truncate\n        crop_left_imgs = self.pad_seq(list(crop_left_imgs), self.zero_img, self.max_seq)\n        crop_right_imgs = self.pad_seq(list(crop_right_imgs), self.zero_img, self.max_seq)\n        kp_j_left = self.pad_seq(list(kp_j_left), self.zero_j, self.max_seq)\n        kp_j_right = self.pad_seq(list(kp_j_right), self.zero_j, self.max_seq)\n        mask_left = np.pad(mask_left, (0, max(self.max_seq - T, 0)), 'constant')\n        mask_right = np.pad(mask_right, (0, max(self.max_seq - T, 0)), 'constant')\n        keypoint = np.pad(keypoint, ((0, max(self.max_seq - T, 0)), (0,0), (0,0)), 'constant')\n\n        crop_left_imgs = torch.stack(crop_left_imgs, dim=0)\n        crop_right_imgs = torch.stack(crop_right_imgs, dim=0)\n        kp_j_left = torch.stack(kp_j_left, dim=0)\n        kp_j_right = torch.stack(kp_j_right, dim=0)\n        keypoint = torch.tensor(keypoint, dtype=torch.float32)\n        mask_left = torch.tensor(mask_left, dtype=torch.float32)\n        mask_right = torch.tensor(mask_right, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.long)\n\n        if keypoint.shape[0] != self.max_seq:\n            print(f\"[WRONG SHAPE] idx={idx}, video_id={meta['video_id']}, path={meta['keypoint_path']}, shape={keypoint.shape}\")\n\n        return {\n            'keypoint': keypoint,             # (T, N, 3)\n            'mask_left': mask_left,           # (T,)\n            'mask_right': mask_right,         # (T,)\n            'crop_left_imgs': crop_left_imgs, # (T, C, H, W)\n            'crop_right_imgs': crop_right_imgs,\n            'kp_j_left': kp_j_left,           # (T, kp_num, 2)\n            'kp_j_right': kp_j_right,\n            'label': label\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:16:32.404692Z","iopub.execute_input":"2025-06-10T04:16:32.404973Z","iopub.status.idle":"2025-06-10T04:16:32.421261Z","shell.execute_reply.started":"2025-06-10T04:16:32.404954Z","shell.execute_reply":"2025-06-10T04:16:32.420565Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport os\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef collate_fn(batch):\n    \"\"\"\n    Batch là list các dict từ Dataset, ghép lại thành tensor.\n    \"\"\"\n    batch_out = {}\n    for k in batch[0].keys():\n        if isinstance(batch[0][k], torch.Tensor):\n            batch_out[k] = torch.stack([item[k] for item in batch], dim=0)\n        else:\n            # Nếu là int/float, convert sang tensor\n            arr = [item[k] for item in batch]\n            if isinstance(arr[0], (int, float)):\n                batch_out[k] = torch.tensor(arr)\n            else:\n                batch_out[k] = arr\n    return batch_out\n\ndef train_epoch(model, loader, optimizer, criterion, device=\"cuda\"):\n    \n    model.train()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch in loader:\n        print(\"batch['keypoint'].shape:\", batch['keypoint'].shape)\n        for k in batch:\n            if isinstance(batch[k], torch.Tensor):\n                batch[k] = batch[k].to(device)\n        optimizer.zero_grad()\n        logits = model(\n            keypoint=batch['keypoint'],\n            rgb_imgs=None,  # Nếu không dùng, truyền None hoặc bỏ field này khỏi model\n            mask_left=batch['mask_left'],\n            mask_right=batch['mask_right'],\n            kp_j_left=batch['kp_j_left'],\n            kp_j_right=batch['kp_j_right'],\n            rgb_left_imgs=batch['crop_left_imgs'],\n            rgb_right_imgs=batch['crop_right_imgs']\n        )\n        loss = criterion(logits, batch['label'])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * batch['label'].size(0)\n        pred = logits.argmax(dim=1)\n        total_acc += (pred == batch['label']).sum().item()\n        n += batch['label'].size(0)\n    return total_loss / n, total_acc / n\n\n@torch.no_grad()\ndef validate_epoch(model, loader, criterion, device=\"cuda\"):\n    model.eval()\n    total_loss, total_acc, n = 0.0, 0.0, 0\n    for batch in loader:\n        for k in batch:\n            if isinstance(batch[k], torch.Tensor):\n                batch[k] = batch[k].to(device)\n        logits = model(\n            keypoint=batch['keypoint'],\n            rgb_imgs=None,\n            mask_left=batch['mask_left'],\n            mask_right=batch['mask_right'],\n            kp_j_left=batch['kp_j_left'],\n            kp_j_right=batch['kp_j_right'],\n            rgb_left_imgs=batch['crop_left_imgs'],\n            rgb_right_imgs=batch['crop_right_imgs']\n        )\n        loss = criterion(logits, batch['label'])\n        total_loss += loss.item() * batch['label'].size(0)\n        pred = logits.argmax(dim=1)\n        total_acc += (pred == batch['label']).sum().item()\n        n += batch['label'].size(0)\n    return total_loss / n, total_acc / n\n\ndef save_checkpoint(model, optimizer, epoch, path, best_acc=None):\n    state = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    if best_acc is not None:\n        state['best_acc'] = best_acc\n    torch.save(state, path)\n\ndef load_checkpoint(model, optimizer, path, device='cuda'):\n    state = torch.load(path, map_location=device)\n    model.load_state_dict(state['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    epoch = state.get('epoch', 0)\n    best_acc = state.get('best_acc', None)\n    return model, optimizer, epoch, best_acc\n\ndef adjust_learning_rate(optimizer, epoch, lr, step=10, decay=0.1):\n    \"\"\" Step LR decay \"\"\"\n    lr_new = lr * (decay ** (epoch // step))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr_new\n    return lr_new","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:07:56.772863Z","iopub.execute_input":"2025-06-10T04:07:56.773599Z","iopub.status.idle":"2025-06-10T04:07:56.788539Z","shell.execute_reply.started":"2025-06-10T04:07:56.773575Z","shell.execute_reply":"2025-06-10T04:07:56.787737Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"import json\n#from data.sign_dataset import SignDataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((112, 112)),\n    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\n# Load meta_list\nwith open('/kaggle/working/train_meta.json') as f:\n    train_meta_list = json.load(f)\nwith open('/kaggle/working/val_meta.json') as f:\n    val_meta_list = json.load(f)\n\ntrain_ds = SignDataset(meta_list=train_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\nval_ds = SignDataset(meta_list=val_meta_list, transform_crop=transform, max_seq=64, crop_shape=(3,112,112), kp_num=21)\n\n#from utils.train_utils import collate_fn\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4, collate_fn=collate_fn)\nval_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:41:24.737356Z","iopub.execute_input":"2025-06-10T03:41:24.737591Z","iopub.status.idle":"2025-06-10T03:41:28.809104Z","shell.execute_reply.started":"2025-06-10T03:41:24.737574Z","shell.execute_reply":"2025-06-10T03:41:28.808292Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"data_root = \"/kaggle/input/keypoint/keypoints\"\ntrain_txt = \"/kaggle/working/train.txt\"\nval_txt = \"/kaggle/working/val.txt\"\ntest_txt = \"/kaggle/working/test.txt\"\n#batch_size = 32\n#num_workers = 4\n#num_epochs = 10\n#lr = 1e-3\n\nlabel_map = build_label_map([train_txt, val_txt, test_txt])\nnum_classes = len(label_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:43:27.304300Z","iopub.execute_input":"2025-06-10T03:43:27.304603Z","iopub.status.idle":"2025-06-10T03:43:27.309847Z","shell.execute_reply.started":"2025-06-10T03:43:27.304582Z","shell.execute_reply":"2025-06-10T03:43:27.309148Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\n#from models.vision import VisionClassifier\n\ndef load_vision_encoder(ckpt_path, num_classes=1000):\n    vision_encoder = VisionClassifier(num_classes=num_classes)\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    vision_encoder.load_state_dict(ckpt.get('model_state_dict', ckpt), strict=False)\n    return vision_encoder\n\nvision_encoder = load_vision_encoder('/kaggle/working/pretrained_hand_rgb.pth', num_classes=num_classes)\n# Để chỉ dùng features:\nvision_encoder_features = vision_encoder.features  # (B, 1280, 4, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:45:09.717392Z","iopub.execute_input":"2025-06-10T03:45:09.718089Z","iopub.status.idle":"2025-06-10T03:45:10.189884Z","shell.execute_reply.started":"2025-06-10T03:45:09.718065Z","shell.execute_reply":"2025-06-10T03:45:10.189251Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n100%|██████████| 20.5M/20.5M [00:00<00:00, 137MB/s]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"#from models.skeleton import SpatialPoseEncoder\n\ndef load_stgcn(ckpt_path, in_channels, num_joints, A, hid_dim=128, out_dim=256):\n    model = SpatialPoseEncoder(\n        in_channels=in_channels,\n        num_joints=num_joints,\n        num_classes=1,  # Không quan trọng nếu chỉ lấy feature\n        A = A,\n        hid_dim=hid_dim,\n        out_dim=out_dim\n    )\n    ckpt = torch.load(ckpt_path, map_location='cpu')\n    model.load_state_dict(ckpt.get('model_state_dict', ckpt), strict=False)\n    return model\n\nstgcn_body = load_stgcn('/kaggle/working/spatial_body_best.pth', in_channels=3, num_joints=25, A = get_body_spatial_graph())\nstgcn_left = load_stgcn('/kaggle/working/spatial_left_best.pth', in_channels=3, num_joints=21, A = get_left_hand_spatial_graph())\nstgcn_right = load_stgcn('/kaggle/working/spatial_right_best.pth', in_channels=3, num_joints=21, A = get_right_hand_spatial_graph())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:45:13.789495Z","iopub.execute_input":"2025-06-10T03:45:13.789909Z","iopub.status.idle":"2025-06-10T03:45:13.835327Z","shell.execute_reply.started":"2025-06-10T03:45:13.789872Z","shell.execute_reply":"2025-06-10T03:45:13.834541Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"dummy_input = torch.randn(2, 64, 25, 3)\nout = stgcn_body(dummy_input)\n\nfor i, o in enumerate(out):\n    print(f\"out[{i}].shape:\", o.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:45:16.967497Z","iopub.execute_input":"2025-06-10T03:45:16.967794Z","iopub.status.idle":"2025-06-10T03:45:17.047876Z","shell.execute_reply.started":"2025-06-10T03:45:16.967774Z","shell.execute_reply":"2025-06-10T03:45:17.047071Z"}},"outputs":[{"name":"stdout","text":"out[0].shape: torch.Size([2, 1])\nout[1].shape: torch.Size([2, 256, 25])\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"d_model = 256\nmax_seq = 64  # hoặc giá trị max_seq bạn sử dụng\n\ntemporal_encoder_body = TemporalEncoder(d_model=d_model, pool='mean')\ntemporal_encoder_hand = TemporalEncoder(d_model=d_model, pool='mean')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:38.242435Z","iopub.execute_input":"2025-06-10T04:58:38.243102Z","iopub.status.idle":"2025-06-10T04:58:38.267566Z","shell.execute_reply.started":"2025-06-10T04:58:38.243076Z","shell.execute_reply":"2025-06-10T04:58:38.266961Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"pgf_module = PGFModule(d_model=256, n_heads=8, n_points=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T03:45:20.574230Z","iopub.execute_input":"2025-06-10T03:45:20.574513Z","iopub.status.idle":"2025-06-10T03:45:20.580974Z","shell.execute_reply.started":"2025-06-10T03:45:20.574492Z","shell.execute_reply":"2025-06-10T03:45:20.580459Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"model = ClassifySign(\n    vision_encoder = vision_encoder_features,\n    pgf_module = pgf_module,\n    stgcn_body=stgcn_body,\n    stgcn_left=stgcn_left,\n    stgcn_right=stgcn_right,\n    temporal_encoder_body=temporal_encoder_body,\n    temporal_encoder_hand=temporal_encoder_hand,\n    num_classes=num_classes,  # Sửa cho đúng dataset của bạn\n    # Có thể cần các tham số khác như vision_encoder, pgf_module, left_idx, right_idx,... nếu repo yêu cầu\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:41.700150Z","iopub.execute_input":"2025-06-10T04:58:41.700613Z","iopub.status.idle":"2025-06-10T04:58:41.708083Z","shell.execute_reply.started":"2025-06-10T04:58:41.700591Z","shell.execute_reply":"2025-06-10T04:58:41.707413Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:43.126801Z","iopub.execute_input":"2025-06-10T04:58:43.127236Z","iopub.status.idle":"2025-06-10T04:58:43.130899Z","shell.execute_reply.started":"2025-06-10T04:58:43.127216Z","shell.execute_reply":"2025-06-10T04:58:43.130027Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs.\")\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:43.590121Z","iopub.execute_input":"2025-06-10T04:58:43.590831Z","iopub.status.idle":"2025-06-10T04:58:43.609072Z","shell.execute_reply.started":"2025-06-10T04:58:43.590800Z","shell.execute_reply":"2025-06-10T04:58:43.608141Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs.\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"set_seed(42)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n\nnum_epochs = 50\nbest_acc = 0.0\nearlystop_patience = 7\nearlystop_counter = 0\n\nfor epoch in range(num_epochs):\n    adjust_learning_rate(optimizer, epoch, lr=1e-4, step=10, decay=0.1)\n\n    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device=device)\n    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device=device)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n\n    # Lưu checkpoint tốt nhất\n    if val_acc > best_acc:\n        best_acc = val_acc\n        save_checkpoint(model, optimizer, epoch, \"/kaggle/working/best_model.pth\", best_acc)\n        print(\"Saved best model.\")\n        earlystop_counter = 0\n    else:\n        earlystop_counter += 1\n\n    # Early stopping nếu không cải thiện\n    if earlystop_counter >= earlystop_patience:\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\nprint(\"Train hoàn tất. Best val acc:\", best_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T04:58:45.383762Z","iopub.execute_input":"2025-06-10T04:58:45.384281Z","iopub.status.idle":"2025-06-10T04:58:47.822388Z","shell.execute_reply.started":"2025-06-10T04:58:45.384258Z","shell.execute_reply":"2025-06-10T04:58:47.821167Z"}},"outputs":[{"name":"stdout","text":"batch['keypoint'].shape: torch.Size([8, 64, 67, 3])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/598567053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2273272434.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         logits = model(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mkeypoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keypoint'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mrgb_imgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Nếu không dùng, truyền None hoặc bỏ field này khỏi model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> List[Any]:\n\u001b[0;32m--> 212\u001b[0;31m         return parallel_apply(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/3539276494.py\", line 62, in forward\n    _, h = self.stgcn_body(frame_kp)         # (B, C_body, Nb)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/1292438302.py\", line 10, in forward\n    B, T, N, C = x.shape\n    ^^^^^^^^^^\nValueError: not enough values to unpack (expected 4, got 3)\n"],"ename":"ValueError","evalue":"Caught ValueError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\", line 96, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/3539276494.py\", line 62, in forward\n    _, h = self.stgcn_body(frame_kp)         # (B, C_body, Nb)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/1292438302.py\", line 10, in forward\n    B, T, N, C = x.shape\n    ^^^^^^^^^^\nValueError: not enough values to unpack (expected 4, got 3)\n","output_type":"error"}],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T16:27:35.286641Z","iopub.execute_input":"2025-06-08T16:27:35.286935Z","iopub.status.idle":"2025-06-08T16:27:35.292220Z","shell.execute_reply.started":"2025-06-08T16:27:35.286915Z","shell.execute_reply":"2025-06-08T16:27:35.291476Z"}},"outputs":[],"execution_count":75}]}